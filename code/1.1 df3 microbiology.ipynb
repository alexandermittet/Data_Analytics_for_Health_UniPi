{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"Y:\\Studium\\3. Sem UniPI\\Data Analytics 4 digital Health\\Data\"\n",
    "\n",
    "DATASETS = {\n",
    "    \"heart_diagnoses_1\": \"heart_diagnoses_1.csv\",\n",
    "    \"laboratory_events_codes_2\": \"laboratory_events_codes_2.csv\",\n",
    "    \"microbiology_events_codes_3\": \"microbiology_events_codes_3.csv\",\n",
    "    \"procedure_code_4\": \"procedure_code_4.csv\",\n",
    "}\n",
    "name = \"microbiology_events_codes_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c414f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{DATA_DIR}/{DATASETS['laboratory_events_codes_2']}\", index_col=False)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a214bba",
   "metadata": {},
   "source": [
    "# INspections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28e781",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345f387",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3295c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[:10]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[10:]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4898f",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90151aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2c293",
   "metadata": {},
   "source": [
    "# Data Understanding and Preprocessing, cleaning of DF1 - heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show duplicated rows\n",
    "df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c08a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col == 'charttime' or col == 'hadm_id' or col == 'subject_id':\n",
    "        continue\n",
    "    print(f\"{col}: {df[col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560cfd8",
   "metadata": {},
   "source": [
    "## Check for wrong NaNs / non typical entries in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896cbdf",
   "metadata": {},
   "source": [
    "#### Find wrong NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FIND NON-NUMERICAL ENTRIES IN NUMERICAL COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify columns that should be numerical\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nColumns already numeric: {len(numeric_cols)}\")\n",
    "print(numeric_cols)\n",
    "\n",
    "# Check object/string columns that might contain numerical data\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nObject/String columns to check: {len(object_cols)}\")\n",
    "print(object_cols)\n",
    "\n",
    "# For each object column, try to find non-numerical entries\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING FOR NON-NUMERICAL ENTRIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "non_numerical_summary = {}\n",
    "\n",
    "for col in object_cols:\n",
    "    non_numerical_entries = []\n",
    "    \n",
    "    for idx, value in df[col].items():\n",
    "        if pd.isna(value):  # Skip NaN/None\n",
    "            continue\n",
    "        \n",
    "        # Try to convert to float\n",
    "        try:\n",
    "            float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            non_numerical_entries.append({\n",
    "                'index': idx,\n",
    "                'value': value,\n",
    "                'type': type(value).__name__\n",
    "            })\n",
    "    \n",
    "    # Store summary\n",
    "    if non_numerical_entries:\n",
    "        non_numerical_summary[col] = non_numerical_entries\n",
    "        \n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Column: '{col}' | Non-numerical entries: {len(non_numerical_entries)}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        # Get unique non-numerical values\n",
    "        unique_values = list(set([e['value'] for e in non_numerical_entries]))\n",
    "        print(f\"Unique non-numerical values ({len(unique_values)}):\")\n",
    "        for val in sorted(unique_values)[:20]:  # Show first 20\n",
    "            count = sum(1 for e in non_numerical_entries if e['value'] == val)\n",
    "            print(f\"  • '{val}' — appears {count} times\")\n",
    "        \n",
    "        if len(unique_values) > 20:\n",
    "            print(f\"  ... and {len(unique_values) - 20} more\")\n",
    "        \n",
    "        # Show sample rows\n",
    "        print(f\"\\nSample rows with non-numerical entries:\")\n",
    "        for entry in non_numerical_entries[:5]:\n",
    "            print(f\"  Index {entry['index']}: {entry['value']!r} ({entry['type']})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nColumns with non-numerical entries: {len(non_numerical_summary)}\")\n",
    "\n",
    "for col, entries in non_numerical_summary.items():\n",
    "    total_rows = len(df)\n",
    "    non_num_count = len(entries)\n",
    "    pct = (non_num_count / total_rows) * 100\n",
    "    unique_count = len(set([e['value'] for e in entries]))\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Non-numerical rows: {non_num_count} ({pct:.2f}%)\")\n",
    "    print(f\"  Unique non-numerical values: {unique_count}\")\n",
    "    print(f\"  Numerical rows: {total_rows - non_num_count}\")\n",
    "\n",
    "# Optional: Create a detailed report\n",
    "if non_numerical_summary:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED REPORT - ALL NON-NUMERICAL ENTRIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for col in non_numerical_summary:\n",
    "        print(f\"\\n{col}:\")\n",
    "        entries_df = pd.DataFrame(non_numerical_summary[col])\n",
    "        # Count occurrences\n",
    "        value_counts = entries_df['value'].value_counts()\n",
    "        print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612aa4de",
   "metadata": {},
   "source": [
    "- valueuom: has '' , remove; Pos/Neg == +/-; U ??\n",
    "- value has wrong entries inspect and extrat if possible to valuenum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d8135",
   "metadata": {},
   "source": [
    "### Handle Value wrong nans, then extract missing from value into new column valuenum_merged if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chec k only value column: show all unique non-numerical entries in 'value' column\n",
    "# non_numerical_values = []\n",
    "# for idx, value in df['value'].items():\n",
    "#     if pd.isna(value):  # Skip NaN/None\n",
    "#         continue\n",
    "    \n",
    "#     # Try to convert to float\n",
    "#     try:\n",
    "#         float(value)\n",
    "#     except (ValueError, TypeError):\n",
    "#         non_numerical_values.append({\n",
    "#             'index': idx,\n",
    "#             'value': value,\n",
    "#             'type': type(value).__name__\n",
    "#         })\n",
    "# print(f\"\\n{'─'*80}\")\n",
    "# print(f\"Column: 'value' | Non-numerical entries: {len(non_numerical_values)}\")\n",
    "# print(f\"{'─'*80}\")\n",
    "# # Get unique non-numerical values\n",
    "# unique_values = list(set([e['value'] for e in non_numerical_values]))\n",
    "# print(f\"Unique non-numerical values ({len(unique_values)}):\")\n",
    "# for val in sorted(unique_values):\n",
    "#     count = sum(1 for e in non_numerical_values if e['value'] == val)\n",
    "#     print(f\"  • '{val}' — appears {count} times\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f56c14",
   "metadata": {},
   "source": [
    "FINDINGS\n",
    "\n",
    "=> in col value, We want to convert '___' and 'NONE' 'ERROR' to np.nan!\n",
    "\n",
    "=> then we create a new col value_extracted (float64) out of col value where:\n",
    "- we can calculate as float complete / like 20/0 but only if there is anumber before and after the /! => complete\n",
    "- we can take the middle point of complete ranges like '80-160'\n",
    "- we can calculate a float value of comparisons with < > by sub/add 0.1 to the number, eg. '>1.050' => 1.150 or '<1' => 0.9\n",
    "- the rest is to nuemic error coerce put to NaN.\n",
    "\n",
    "=> then, we fill np.nan entries in col valuenum with values from valUe_extracted if they are not nan and tell me the amount of filled rows and show examples beffore and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first sanity check if needed\n",
    "# check are there rows where valuenum has nan but value has a value?a\n",
    "len(df[(df['valuenum'].isna()) & (df['value'].notna())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 1: CONVERT PLACEHOLDER STRINGS TO NaN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define placeholder patterns\n",
    "placeholders = ['___', 'NONE', 'ERROR']\n",
    "\n",
    "print(f\"\\nPlaceholders to convert: {placeholders}\")\n",
    "print(f\"Before: {df['value'].isna().sum()} NaN values\")\n",
    "\n",
    "# Convert placeholders to NaN (case-insensitive)\n",
    "for placeholder in placeholders:\n",
    "    mask = df['value'].astype(str).str.lower() == placeholder.lower()\n",
    "    count = mask.sum()\n",
    "    df.loc[mask, 'value'] = np.nan\n",
    "    print(f\"  Converted '{placeholder}': {count} rows\")\n",
    "\n",
    "print(f\"After: {df['value'].isna().sum()} NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 2: EXTRACT NUMERIC VALUES FROM 'value' COLUMN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_numeric_from_value(x):\n",
    "    \"\"\"\n",
    "    Extract numeric values from various string formats:\n",
    "    - Simple numbers: '123' => 123\n",
    "    - Decimals: '123.45' => 123.45\n",
    "    - Divisions: '20/10' => 2.0\n",
    "    - Ranges: '80-160' => 120 (midpoint)\n",
    "    - Comparisons: '>1.050' => 1.150, '<1' => 0.9\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    \n",
    "    x_str = str(x).strip()\n",
    "    \n",
    "    # Try direct float conversion\n",
    "    try:\n",
    "        return float(x_str)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Handle divisions (e.g., '20/10')\n",
    "    if '/' in x_str:\n",
    "        parts = x_str.split('/')\n",
    "        try:\n",
    "            if len(parts) == 2 and parts[0].strip() and parts[1].strip():\n",
    "                num1 = float(parts[0].strip())\n",
    "                num2 = float(parts[1].strip())\n",
    "                if num2 != 0:  # Avoid division by zero\n",
    "                    return num1 / num2\n",
    "        except (ValueError, ZeroDivisionError):\n",
    "            pass\n",
    "    \n",
    "    # Handle ranges (e.g., '80-160')\n",
    "    if '-' in x_str and not x_str.startswith('-'):\n",
    "        parts = x_str.split('-')\n",
    "        try:\n",
    "            if len(parts) == 2 and parts[0].strip() and parts[1].strip():\n",
    "                num1 = float(parts[0].strip())\n",
    "                num2 = float(parts[1].strip())\n",
    "                return (num1 + num2) / 2  # Midpoint\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Handle comparisons (e.g., '>1.050' => 1.150, '<1' => 0.9)\n",
    "    comparison_match = re.match(r'^([<>]=?)(\\d*\\.?\\d+)$', x_str.strip())\n",
    "    if comparison_match:\n",
    "        operator = comparison_match.group(1)\n",
    "        try:\n",
    "            num = float(comparison_match.group(2))\n",
    "            if operator == '>':\n",
    "                return num + 0.1\n",
    "            elif operator == '>=':\n",
    "                return num\n",
    "            elif operator == '<':\n",
    "                return num - 0.1\n",
    "            elif operator == '<=':\n",
    "                return num\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If nothing worked, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply extraction\n",
    "df['value_extracted'] = df['value'].apply(extract_numeric_from_value)\n",
    "\n",
    "print(f\"\\nExtraction complete!\")\n",
    "print(f\"Non-null values extracted: {df['value_extracted'].notna().sum():,}\")\n",
    "print(f\"Failed extractions (NaN): {df['value_extracted'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"STEP 3: SHOW EXAMPLES OF EXTRACTED VALUES (Complex Formats Only)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Find rows with successful extractions\n",
    "# extracted_rows = df[df['value_extracted'].notna() & df['value'].notna()].copy()\n",
    "\n",
    "# print(f\"\\nTotal rows with extracted values: {len(extracted_rows):,}\\n\")\n",
    "\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# # COMPARISONS: >, <, >=, <=\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# print(\"=\"*80)\n",
    "# print(\"EXAMPLES: COMPARISONS (>, <, >=, <=)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# comparison_pattern = r'^([<>]=?)(\\d*\\.?\\d+)$'\n",
    "# comparison_examples = []\n",
    "\n",
    "# for idx, row in extracted_rows.iterrows():\n",
    "#     original = str(row['value']).strip()\n",
    "#     if re.match(comparison_pattern, original):\n",
    "#         comparison_examples.append({\n",
    "#             'original': original,\n",
    "#             'extracted': row['value_extracted']\n",
    "#         })\n",
    "\n",
    "# if comparison_examples:\n",
    "#     # Show first 15 unique examples\n",
    "#     unique_comparisons = []\n",
    "#     seen = set()\n",
    "#     for ex in comparison_examples:\n",
    "#         if ex['original'] not in seen:\n",
    "#             unique_comparisons.append(ex)\n",
    "#             seen.add(ex['original'])\n",
    "#             if len(unique_comparisons) >= 15:\n",
    "#                 break\n",
    "    \n",
    "#     print(f\"\\nFound {len(comparison_examples)} comparison values\\n\")\n",
    "#     print(f\"{'Original':<20} {'Extracted':<15} {'Logic':<40}\")\n",
    "#     print(\"─\" * 75)\n",
    "    \n",
    "#     for ex in unique_comparisons:\n",
    "#         original = ex['original']\n",
    "#         extracted = ex['extracted']\n",
    "        \n",
    "#         # Explain the logic\n",
    "#         if original.startswith('>'):\n",
    "#             if original.startswith('>='):\n",
    "#                 logic = f\"'{original}' → value as-is\"\n",
    "#             else:\n",
    "#                 num = float(original[1:])\n",
    "#                 logic = f\"'{original}' → {num} + 0.1 = {extracted}\"\n",
    "#         elif original.startswith('<'):\n",
    "#             if original.startswith('<='):\n",
    "#                 logic = f\"'{original}' → value as-is\"\n",
    "#             else:\n",
    "#                 num = float(original[1:])\n",
    "#                 logic = f\"'{original}' → {num} - 0.1 = {extracted}\"\n",
    "        \n",
    "#         print(f\"{original:<20} {extracted:<15.3f} {logic:<40}\")\n",
    "# else:\n",
    "#     print(\"\\nNo comparison values found\")\n",
    "\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# # RANGES: 80-160, 0-2, etc.\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"EXAMPLES: RANGES (e.g., 80-160, 0-2)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# range_pattern = r'^(\\d+\\.?\\d*)-(\\d+\\.?\\d*)$'\n",
    "# range_examples = []\n",
    "\n",
    "# for idx, row in extracted_rows.iterrows():\n",
    "#     original = str(row['value']).strip()\n",
    "#     if re.match(range_pattern, original) and not original.startswith('-'):\n",
    "#         range_examples.append({\n",
    "#             'original': original,\n",
    "#             'extracted': row['value_extracted']\n",
    "#         })\n",
    "\n",
    "# if range_examples:\n",
    "#     # Show first 15 unique examples\n",
    "#     unique_ranges = []\n",
    "#     seen = set()\n",
    "#     for ex in range_examples:\n",
    "#         if ex['original'] not in seen:\n",
    "#             unique_ranges.append(ex)\n",
    "#             seen.add(ex['original'])\n",
    "#             if len(unique_ranges) >= 15:\n",
    "#                 break\n",
    "    \n",
    "#     print(f\"\\nFound {len(range_examples)} range values\\n\")\n",
    "#     print(f\"{'Original':<20} {'Extracted':<15} {'Logic (Midpoint)':<40}\")\n",
    "#     print(\"─\" * 75)\n",
    "    \n",
    "#     for ex in unique_ranges:\n",
    "#         original = ex['original']\n",
    "#         extracted = ex['extracted']\n",
    "        \n",
    "#         # Calculate and show logic\n",
    "#         parts = original.split('-')\n",
    "#         num1 = float(parts[0])\n",
    "#         num2 = float(parts[1])\n",
    "#         midpoint = (num1 + num2) / 2\n",
    "        \n",
    "#         logic = f\"({num1} + {num2}) / 2 = {midpoint}\"\n",
    "        \n",
    "#         print(f\"{original:<20} {extracted:<15.3f} {logic:<40}\")\n",
    "# else:\n",
    "#     print(\"\\nNo range values found\")\n",
    "\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# # DIVISIONS: 20/10, 15/3, etc.\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"EXAMPLES: DIVISIONS (e.g., 20/10, 15/3)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# division_pattern = r'^(\\d+\\.?\\d*)/(\\d+\\.?\\d*)$'\n",
    "# division_examples = []\n",
    "\n",
    "# for idx, row in extracted_rows.iterrows():\n",
    "#     original = str(row['value']).strip()\n",
    "#     if re.match(division_pattern, original):\n",
    "#         division_examples.append({\n",
    "#             'original': original,\n",
    "#             'extracted': row['value_extracted']\n",
    "#         })\n",
    "\n",
    "# if division_examples:\n",
    "#     # Show first 15 unique examples\n",
    "#     unique_divisions = []\n",
    "#     seen = set()\n",
    "#     for ex in division_examples:\n",
    "#         if ex['original'] not in seen:\n",
    "#             unique_divisions.append(ex)\n",
    "#             seen.add(ex['original'])\n",
    "#             if len(unique_divisions) >= 15:\n",
    "#                 break\n",
    "    \n",
    "#     print(f\"\\nFound {len(division_examples)} division values\\n\")\n",
    "#     print(f\"{'Original':<20} {'Extracted':<15} {'Logic':<40}\")\n",
    "#     print(\"─\" * 75)\n",
    "    \n",
    "#     for ex in unique_divisions:\n",
    "#         original = ex['original']\n",
    "#         extracted = ex['extracted']\n",
    "        \n",
    "#         # Calculate and show logic\n",
    "#         parts = original.split('/')\n",
    "#         num1 = float(parts[0])\n",
    "#         num2 = float(parts[1])\n",
    "#         result = num1 / num2 if num2 != 0 else float('nan')\n",
    "        \n",
    "#         logic = f\"{num1} / {num2} = {result}\"\n",
    "        \n",
    "#         print(f\"{original:<20} {extracted:<15.3f} {logic:<40}\")\n",
    "# else:\n",
    "#     print(\"\\nNo division values found\")\n",
    "\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# # SUMMARY STATISTICS\n",
    "# # ────────────────────────────────────────────────────────────────────────────\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"EXTRACTION SUMMARY\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# print(f\"\\nTotal extracted values: {len(extracted_rows):,}\")\n",
    "# print(f\"  • Comparisons:  {len(comparison_examples):,} ({len(comparison_examples)/len(extracted_rows)*100:.1f}%)\")\n",
    "# print(f\"  • Ranges:       {len(range_examples):,} ({len(range_examples)/len(extracted_rows)*100:.1f}%)\")\n",
    "# print(f\"  • Divisions:    {len(division_examples):,} ({len(division_examples)/len(extracted_rows)*100:.1f}%)\")\n",
    "# print(f\"  • Other:        {len(extracted_rows) - len(comparison_examples) - len(range_examples) - len(division_examples):,}\")\n",
    "\n",
    "# print(f\"\\n{'─'*80}\")\n",
    "# print(\"STATISTICS OF EXTRACTED VALUES\")\n",
    "# print(f\"{'─'*80}\")\n",
    "# print(df['value_extracted'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 4: MERGE valuenum + value_extracted\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store counts before merge\n",
    "valuenum_before = df['valuenum'].notna().sum()\n",
    "value_extracted_only = (df['valuenum'].isna() & df['value_extracted'].notna()).sum()\n",
    "\n",
    "print(f\"\\nBefore merge:\")\n",
    "print(f\"  valuenum (non-null):           {valuenum_before:,}\")\n",
    "print(f\"  value_extracted (non-null):    {df['value_extracted'].notna().sum():,}\")\n",
    "print(f\"  Can be filled from extraction:  {value_extracted_only:,}\")\n",
    "\n",
    "# Merge: prefer valuenum, fallback to value_extracted\n",
    "df['valuenum_merged'] = df['valuenum'].fillna(df['value_extracted'])\n",
    "\n",
    "valuenum_after = df['valuenum_merged'].notna().sum()\n",
    "newly_filled = valuenum_after - valuenum_before\n",
    "\n",
    "print(f\"\\nAfter merge:\")\n",
    "print(f\"  valuenum_merged (non-null):   {valuenum_after:,}\")\n",
    "print(f\"  Newly filled from extraction:   {newly_filled:,} (+{(newly_filled/len(df)*100):.2f}%)\")\n",
    "print(f\"  Total improvement:              {valuenum_after - df['valuenum'].isna().sum():,} rows\")\n",
    "\n",
    "print(f\"\\nData type: {df['valuenum_merged'].dtype}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(df['valuenum_merged'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c190b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"STEP 5: SHOW BEFORE/AFTER EXAMPLES\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Find rows that were newly filled\n",
    "# newly_filled_rows = df[(df['valuenum'].isna()) & (df['value_extracted'].notna())].copy()\n",
    "\n",
    "# print(f\"\\nTotal newly filled rows: {len(newly_filled_rows):,}\\n\")\n",
    "# print(\"BEFORE → AFTER Examples:\\n\")\n",
    "# print(f\"{'Original value':<20} {'Extracted':<15} {'valuenum (before)':<20} {'valuenum_merged (after)':<20}\")\n",
    "# print(\"─\" * 75)\n",
    "\n",
    "# for i, (idx, row) in enumerate(newly_filled_rows.head(20).iterrows()):\n",
    "#     original = str(row['value'])[:19]\n",
    "#     extracted = f\"{row['value_extracted']:.3f}\" if pd.notna(row['value_extracted']) else \"NaN\"\n",
    "#     before = \"NaN\"\n",
    "#     after = f\"{row['valuenum_merged']:.3f}\"\n",
    "    \n",
    "#     print(f\"{original:<20} {extracted:<15} {before:<20} {after:<20}\")\n",
    "\n",
    "# print(f\"\\n{'─'*75}\")\n",
    "# print(f\"\\nCoverage improvement:\")\n",
    "# print(f\"  Before: {(df['valuenum'].notna().sum() / len(df) * 100):.2f}% coverage\")\n",
    "# print(f\"  After:  {(df['valuenum_merged'].notna().sum() / len(df) * 100):.2f}% coverage\")\n",
    "# print(f\"  Gain:   {(newly_filled / len(df) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a28aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2122f9",
   "metadata": {},
   "source": [
    " ## Convert datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['charttime',]  \n",
    "\n",
    "for col in cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"{col}: parsed {df[col].notna().sum()} values, {df[col].isna().sum()} NaT\")\n",
    "\n",
    "display(df[[c for c in df.columns if c in cols]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871646e0",
   "metadata": {},
   "source": [
    "## inspect qc_flag == FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check qc_flag == 'FAIL' \n",
    "df[\"qc_flag\"].value_counts(), 19570/ len(df), 78124 / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282bfde",
   "metadata": {},
   "source": [
    "2% fail, 8% warn.\n",
    "\n",
    "IDEA: small percentage => set valuenum_merged to np.nan those rows bc qualtiy control failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle QC flags\n",
    "qc_fail_mask = df['qc_flag'] == 'FAIL'\n",
    "qc_warn_mask = df['qc_flag'] == 'WARN'\n",
    "\n",
    "# Drop failed measurements from valuenum_merged\n",
    "before_non_null = df['valuenum_merged'].notna().sum()\n",
    "df.loc[qc_fail_mask, 'valuenum_merged'] = np.nan\n",
    "after_non_null = df['valuenum_merged'].notna().sum()\n",
    "\n",
    "print(f\"set {before_non_null - after_non_null:,} FAIL measurements from valuenum_merged to nan.\")\n",
    "print(f\"Coverage drop: {(before_non_null - after_non_null) / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Binary QC features for downstream aggregation/clustering\n",
    "df['is_qc_fail'] = qc_fail_mask.astype(int)\n",
    "df['is_qc_warn'] = qc_warn_mask.astype(int)\n",
    "df['is_qc_ok'] = (~qc_fail_mask & ~qc_warn_mask).astype(int)\n",
    "\n",
    "print(df[['qc_flag', 'is_qc_ok', 'is_qc_warn', 'is_qc_fail']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b75dde",
   "metadata": {},
   "source": [
    "## Little intermed inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39680cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c7a9e",
   "metadata": {},
   "source": [
    "## check if flag indidactor (abnormal) is correect (valuenum_merged is within the ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nan / none null vals in flag to 'normal'\n",
    "df['flag'] = df['flag'].fillna('normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc16e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'flag' correctly indicates abnormal values based on reference ranges\n",
    "print(\"=\"*80)\n",
    "print(\"FLAG CORRECTNESS VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create working columns\n",
    "df['_val'] = pd.to_numeric(df['valuenum_merged'], errors='coerce')\n",
    "df['_ref_low'] = pd.to_numeric(df['ref_range_lower'], errors='coerce')\n",
    "df['_ref_high'] = pd.to_numeric(df['ref_range_upper'], errors='coerce')\n",
    "\n",
    "# Compute expected flag based on reference ranges\n",
    "def compute_expected_flag(row):\n",
    "    val = row['_val']\n",
    "    low = row['_ref_low']\n",
    "    high = row['_ref_high']\n",
    "    \n",
    "    if pd.isna(val):\n",
    "        return np.nan  # Can't determine without a value\n",
    "    if pd.isna(low) and pd.isna(high):\n",
    "        return np.nan  # No reference range available\n",
    "    \n",
    "    # Check if outside bounds\n",
    "    if pd.notna(low) and pd.notna(high):\n",
    "        return 'abnormal' if (val < low or val > high) else 'normal'\n",
    "    elif pd.notna(low):\n",
    "        return 'abnormal' if val < low else 'normal'\n",
    "    elif pd.notna(high):\n",
    "        return 'abnormal' if val > high else 'normal'\n",
    "    return np.nan\n",
    "\n",
    "df['_expected_flag'] = df.apply(compute_expected_flag, axis=1)\n",
    "\n",
    "# Compare with actual flag\n",
    "checkable = df[df['_expected_flag'].notna()].copy()\n",
    "checkable['_match'] = checkable['flag'] == checkable['_expected_flag']\n",
    "\n",
    "# Identify mismatches\n",
    "mismatches = checkable[~checkable['_match']]\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nTotal rows: {len(df):,}\")\n",
    "print(f\"Rows with computable expected flag: {len(checkable):,}\")\n",
    "print(f\"Matching flags: {checkable['_match'].sum():,} ({checkable['_match'].mean()*100:.2f}%)\")\n",
    "print(f\"Mismatched flags: {len(mismatches):,} ({len(mismatches)/len(checkable)*100:.2f}%)\")\n",
    "\n",
    "# Show mismatch breakdown\n",
    "if len(mismatches) > 0:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"MISMATCH BREAKDOWN:\")\n",
    "    print(\"-\"*80)\n",
    "    mismatch_types = mismatches.groupby(['flag', '_expected_flag']).size().reset_index(name='count')\n",
    "    print(mismatch_types.to_string(index=False))\n",
    "    \n",
    "    # Check how many mismatches have same valuenum as valuenum_merged\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"MISMATCH SOURCE ANALYSIS:\")\n",
    "    print(\"-\"*80)\n",
    "    mismatches['_same_valuenum'] = mismatches['valuenum'] == mismatches['valuenum_merged']\n",
    "    same_count = mismatches['_same_valuenum'].sum()\n",
    "    diff_count = len(mismatches) - same_count\n",
    "    \n",
    "    print(f\"Mismatches where valuenum == valuenum_merged: {same_count:,} ({same_count/len(mismatches)*100:.2f}%)\")\n",
    "    print(f\"Mismatches where valuenum != valuenum_merged: {diff_count:,} ({diff_count/len(mismatches)*100:.2f}%)\")\n",
    "    print(f\"  → Original data issue: {same_count/len(mismatches)*100:.1f}%\")\n",
    "    print(f\"  → Possibly from value extraction: {diff_count/len(mismatches)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SAMPLE MISMATCHES (first 20):\")\n",
    "    print(\"-\"*80)\n",
    "    display(mismatches[['hadm_id', 'label', 'value', 'valuenum', 'valuenum_merged', '_ref_low', '_ref_high', 'ref_range', 'qc_flag',\n",
    "                        'flag', '_expected_flag', '_same_valuenum']].head(20))\n",
    "\n",
    "# Cleanup helper columns\n",
    "df.drop(columns=['_val', '_ref_low', '_ref_high', '_expected_flag'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4e61c",
   "metadata": {},
   "source": [
    "fix those where  valuenum == valuenum_merged, put np.nan where !="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further fix: For rows with comparison operators (> or <), apply expected flag\n",
    "# Keep ranges (containing '-') as NaN\n",
    "\n",
    "# Recompute masks and expected flag\n",
    "df['_val'] = pd.to_numeric(df['valuenum_merged'], errors='coerce')\n",
    "df['_ref_low'] = pd.to_numeric(df['ref_range_lower'], errors='coerce')\n",
    "df['_ref_high'] = pd.to_numeric(df['ref_range_upper'], errors='coerce')\n",
    "\n",
    "def compute_expected_flag(row):\n",
    "    val, low, high = row['_val'], row['_ref_low'], row['_ref_high']\n",
    "    if pd.isna(val) or (pd.isna(low) and pd.isna(high)):\n",
    "        return np.nan\n",
    "    if pd.notna(low) and pd.notna(high):\n",
    "        return 'abnormal' if (val < low or val > high) else 'normal'\n",
    "    elif pd.notna(low):\n",
    "        return 'abnormal' if val < low else 'normal'\n",
    "    elif pd.notna(high):\n",
    "        return 'abnormal' if val > high else 'normal'\n",
    "    return np.nan\n",
    "\n",
    "df['_expected_flag'] = df.apply(compute_expected_flag, axis=1)\n",
    "\n",
    "# Identify mismatches\n",
    "mismatch_mask = (df['_expected_flag'].notna()) & (df['flag'] != df['_expected_flag'])\n",
    "\n",
    "# Check if valuenum == valuenum_merged for mismatches\n",
    "same_value_mask = df['valuenum'] == df['valuenum_merged']\n",
    "\n",
    "# Identify comparison values (> or <) vs ranges (-)\n",
    "has_comparison = df['value'].astype(str).str.contains(r'^[<>]', regex=True, na=False)\n",
    "has_range = df['value'].astype(str).str.contains(r'^\\d+\\.?\\d*-\\d+\\.?\\d*$', regex=True, na=False)\n",
    "\n",
    "# Set to NaN where valuenum != valuenum_merged (extraction issue)\n",
    "nan_mask = mismatch_mask & ~same_value_mask\n",
    "df.loc[nan_mask, 'flag_corrected'] = np.nan\n",
    "\n",
    "# Fix comparisons: apply expected flag\n",
    "comparison_fix_mask = nan_mask & has_comparison\n",
    "df.loc[comparison_fix_mask, 'flag_corrected'] = df.loc[comparison_fix_mask, '_expected_flag']\n",
    "\n",
    "print(f\"Rows with comparisons (> or <) fixed: {comparison_fix_mask.sum():,}\")\n",
    "print(f\"Rows with ranges (-) remaining NaN: {(nan_mask & has_range).sum():,}\")\n",
    "\n",
    "# Cleanup\n",
    "df.drop(columns=['_val', '_ref_low', '_ref_high', '_expected_flag'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54639887",
   "metadata": {},
   "source": [
    "## to nan: valueuom: has '' , remove; Pos/Neg == +/-; U ??\n",
    "keep U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e00be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle empty strings and whitespace-only values in valueuom\n",
    "empty_mask = (df['valueuom'] == '') | (df['valueuom'].astype(str).str.strip() == '')\n",
    "print(f\"  Empty strings '' or whitespace-only: {empty_mask.sum():,} → set to NaN\")\n",
    "df.loc[empty_mask, 'valueuom'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74645292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['valueuom'] = df['valueuom'].replace({'Pos/Neg': '+/-'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79a6c5",
   "metadata": {},
   "source": [
    "## Attempt unit normailaztion valueom => valuenum_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da87344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: (label_lowercase, from_unit) : (factor, to_unit)\n",
    "conversion_map = {\n",
    "    # Basic Metabolic Panel\n",
    "    ('glucose', 'mg/dL'): (0.0555, 'mmol/L'),\n",
    "    ('creatinine', 'mg/dL'): (88.4, 'µmol/L'),\n",
    "    ('urea nitrogen', 'mg/dL'): (0.357, 'mmol/L'),\n",
    "    ('bicarbonate', 'mEq/L'): (1.0, 'mmol/L'),\n",
    "    ('anion gap', 'mEq/L'): (1.0, 'mmol/L'),\n",
    "    ('calcium, total', 'mg/dL'): (0.25, 'mmol/L'),\n",
    "    ('magnesium', 'mg/dL'): (0.411, 'mmol/L'),\n",
    "    ('phosphate', 'mg/dL'): (0.323, 'mmol/L'),\n",
    "    ('potassium', 'mEq/L'): (1.0, 'mmol/L'),\n",
    "    ('sodium', 'mEq/L'): (1.0, 'mmol/L'),\n",
    "    ('triglycerides', 'mg/dL'): (0.0113, 'mmol/L'),\n",
    "    ('cholesterol, total', 'mg/dL'): (0.0259, 'mmol/L'),\n",
    "    ('cholesterol, ldl, calculated', 'mg/dL'): (0.0259, 'mmol/L'),\n",
    "    ('cholesterol, hdl', 'mg/dL'): (0.0259, 'mmol/L'),\n",
    "\n",
    "    # Hematology\n",
    "    ('hemoglobin', 'g/dL'): (10.0, 'g/L'),\n",
    "    ('hematocrit', '%'): (1.0, '%'),\n",
    "    ('red blood cells', 'M/uL'): (1e6, '/uL'),\n",
    "    ('white blood cells', 'K/uL'): (1000, '/uL'),\n",
    "    ('platelet count', 'K/uL'): (1000, '/uL'),\n",
    "    ('mch', 'pg'): (1.0, 'pg'),\n",
    "    ('mchc', 'g/dL'): (10.0, 'g/L'),\n",
    "    ('mcv', 'fL'): (1.0, 'fL'),\n",
    "    ('rdw', '%'): (1.0, '%'),\n",
    "\n",
    "    # Coagulation / Blood Gas\n",
    "    ('ptt', 'sec'): (1.0, 'sec'),\n",
    "    ('pt', 'sec'): (1.0, 'sec'),\n",
    "    ('inr(pt)', 'ratio'): (1.0, 'ratio'),\n",
    "    ('ph', ''): (1.0, ''),\n",
    "    ('pco2', 'mm Hg'): (0.133, 'kPa'),\n",
    "    ('po2', 'mm Hg'): (0.133, 'kPa'),\n",
    "    ('base excess', 'mmol/L'): (1.0, 'mmol/L'),\n",
    "    ('calculated total co2', 'mmol/L'): (1.0, 'mmol/L'),\n",
    "\n",
    "    # Enzymes\n",
    "    ('alanine aminotransferase (alt)', 'U/L'): (1.0, 'U/L'),\n",
    "    ('asparate aminotransferase (ast)', 'U/L'): (1.0, 'U/L'),\n",
    "    ('alkaline phosphatase', 'U/L'): (1.0, 'U/L'),\n",
    "    ('creatine kinase (ck)', 'U/L'): (1.0, 'U/L'),\n",
    "    ('creatine kinase, mb isoenzyme', 'U/L'): (1.0, 'U/L'),\n",
    "\n",
    "    # Bilirubin\n",
    "    ('bilirubin, total', 'mg/dL'): (17.1, 'µmol/L'),\n",
    "    ('bilirubin', 'mg/dL'): (17.1, 'µmol/L'),\n",
    "\n",
    "    # Protein / Albumin\n",
    "    ('albumin', 'g/dL'): (10.0, 'g/L'),\n",
    "    ('protein, total', 'g/dL'): (10.0, 'g/L'),\n",
    "\n",
    "    # Lactate / Calcium\n",
    "    ('lactate', 'mmol/L'): (1.0, 'mmol/L'),\n",
    "    ('free calcium', 'mg/dL'): (0.25, 'mmol/L'),\n",
    "\n",
    "    # Blood gases\n",
    "    ('oxygen saturation', '%'): (1.0, '%'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b688eef",
   "metadata": {},
   "source": [
    "These include qualitative tests, ratios, percentages, indices, specimen types, or already standardized units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58350244",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_convertible_labels = [\n",
    "    'Creatine Kinase (CK)', 'Creatine Kinase, MB Isoenzyme', 'Troponin T', 'INR(PT)',\n",
    "    'PT', 'PTT', '% Hemoglobin A1c', 'eAG', 'Anion Gap', 'Cholesterol Ratio (Total/HDL)',\n",
    "    'Specimen Type', 'EDTA Hold', 'Red Top Hold', 'Intubated', 'Ventilator', 'PEEP',\n",
    "    'Ventilation Rate', 'Required O2', 'H', 'L', 'I', 'Uhold', 'Other', 'Problem Specimen',\n",
    "    'Voided Specimen', 'Urine Specimen Type', 'Blue Top Hold', 'Green Top Hold (plasma)',\n",
    "    'Gray Top Hold (plasma)', 'Light Green Top Hold', 'Red Top Hold', 'Plasma', 'NRBC',\n",
    "    'WBC Clumps', 'Other Cells', 'Other Cell', 'NonSquamous Epithelial Cell',\n",
    "    'Non-squamous Epithelial Cells', 'Urobilinogen', 'Urine Appearance', 'Urine Color',\n",
    "    'Urine Mucous', 'Bacteria', 'Blood', 'Epithelial Cells', 'Hyaline Casts', 'Ketone',\n",
    "    'Leukocytes', 'Nitrite', 'Yeast', 'Amorphous Crystals', 'Voided Specimen',\n",
    "    'Macrocytes', 'Microcytes', 'Schistocytes', 'Teardrop Cells', 'Echinocytes',\n",
    "    'Spherocytes', 'Ovalocytes', 'Bite Cells', 'Target Cells', 'Elliptocytes', 'Pencil Cells',\n",
    "    'Sickle Cells', 'Triple Phosphate Crystals', 'Waxy Casts', 'Granular Casts',\n",
    "    'Cellular Cast', 'RBC Casts', 'Protein Electrophoresis', 'Immunofixation', 'CK-MB Index',\n",
    "    'Free Kappa', 'Free Lambda', 'Free Kappa/Free Lambda Ratio', 'Immature Granulocytes',\n",
    "    'CD markers', 'Anti-Nuclear Antibody', 'Hepatitis B Surface Antibody', 'Hepatitis C Virus Antibody',\n",
    "    'HIV Screen', 'Influenza A by PCR', 'Influenza B by PCR', 'G6PD, Qualitative',\n",
    "    'Calculated Thyroxine (T4) Index', 'Uptake Ratio', 'Calculated TBG', 'EtOH',\n",
    "    'Specimen-related holds', 'Ventilation/oxygen settings'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd57694",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (analyte, from_unit), (factor, to_unit) in conversion_map.items():\n",
    "    mask = (df['label'].str.lower() == analyte) & (df['valueuom'] == from_unit)\n",
    "    df.loc[mask, 'valuenum_merged'] *= factor\n",
    "    df.loc[mask, 'valueuom'] = to_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe8c4a",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sum of all missing values per column\n",
    "for col in df.columns:\n",
    "    missing_count = df[col].isna().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column '{col}': {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d589a52",
   "metadata": {},
   "source": [
    "## Do scatterpltos / distr / etc to check for dataqualtiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad8b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd09bf1",
   "metadata": {},
   "source": [
    "### Do correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152419f4",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{DATA_DIR}/{DATASETS[name].replace('.csv', '_cleaned.csv')}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a37114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already cleaned to skip first steps\n",
    "#df = pd.read_csv(f\"{DATA_DIR}/heart_diagnoses_1_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9cf62",
   "metadata": {},
   "source": [
    "## add subject id from df1, df3, and df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378c899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2de0cb",
   "metadata": {},
   "source": [
    "## Create features and slim version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201280fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim[df_slim.columns[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim[df_slim.columns[10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25385824",
   "metadata": {},
   "source": [
    "### Create features\n",
    "- var for exams\n",
    "- var for charttime?\n",
    "- var for dod (is_dead?), anchor_year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49df480",
   "metadata": {},
   "source": [
    "### Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CORRELATION MATRIX ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = df_slim.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns found: {numeric_cols}\")\n",
    "print(f\"Total numeric columns: {len(numeric_cols)}\\n\")\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df_slim[numeric_cols].corr()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CORRELATION MATRIX\")\n",
    "    print(\"=\"*80)\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Plot 1: Full correlation heatmap\n",
    "    ax1 = axes[0]\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=ax1)\n",
    "    ax1.set_title('Full Correlation Matrix Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # Plot 2: Mask for upper triangle (cleaner view)\n",
    "    ax2 = axes[1]\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                mask=mask,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=ax2)\n",
    "    ax2.set_title('Lower Triangle Correlation Matrix (Unique Pairs)', fontsize=14, fontweight='bold')\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax2.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract strong correlations (> 0.5 or < -0.5, excluding diagonal)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STRONG CORRELATIONS (|r| > 0.5, excluding self-correlations)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.5:\n",
    "                strong_corr.append({\n",
    "                    'Variable 1': correlation_matrix.columns[i],\n",
    "                    'Variable 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value,\n",
    "                    'Strength': 'Strong Positive' if corr_value > 0 else 'Strong Negative'\n",
    "                })\n",
    "    \n",
    "    if strong_corr:\n",
    "        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)\n",
    "        print(f\"\\nFound {len(strong_corr)} strong correlations:\\n\")\n",
    "        print(strong_corr_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo correlations with |r| > 0.5 found\")\n",
    "    \n",
    "    # Moderate correlations (0.3 to 0.5)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODERATE CORRELATIONS (0.3 < |r| ≤ 0.5)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    moderate_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if 0.3 < abs(corr_value) <= 0.5:\n",
    "                moderate_corr.append({\n",
    "                    'Variable 1': correlation_matrix.columns[i],\n",
    "                    'Variable 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value,\n",
    "                    'Strength': 'Moderate Positive' if corr_value > 0 else 'Moderate Negative'\n",
    "                })\n",
    "    \n",
    "    if moderate_corr:\n",
    "        moderate_corr_df = pd.DataFrame(moderate_corr).sort_values('Correlation', key=abs, ascending=False)\n",
    "        print(f\"\\nFound {len(moderate_corr)} moderate correlations:\\n\")\n",
    "        print(moderate_corr_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo moderate correlations found (0.3 < |r| ≤ 0.5)\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get correlation values (excluding diagonal)\n",
    "    corr_values = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_values.append(correlation_matrix.iloc[i, j])\n",
    "    \n",
    "    corr_values = np.array(corr_values)\n",
    "    print(f\"\\nMean correlation: {corr_values.mean():.3f}\")\n",
    "    print(f\"Median correlation: {np.median(corr_values):.3f}\")\n",
    "    print(f\"Std Dev: {corr_values.std():.3f}\")\n",
    "    print(f\"Min: {corr_values.min():.3f}\")\n",
    "    print(f\"Max: {corr_values.max():.3f}\")\n",
    "    \n",
    "    # Distribution of correlations\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.hist(corr_values, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax.axvline(corr_values.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {corr_values.mean():.3f}')\n",
    "    ax.axvline(np.median(corr_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(corr_values):.3f}')\n",
    "    ax.set_xlabel('Correlation Coefficient', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Distribution of Correlation Coefficients', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Not enough numeric columns for correlation analysis\")\n",
    "    print(f\"Found only {len(numeric_cols)} numeric column(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12962fd4",
   "metadata": {},
   "source": [
    "### Dropunused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02125343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.columns, len(df_slim.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP: 'note_id', 'note_type', 'note_seq', 'subject_id_dx', 'storetime', 'HPI', 'physical_exam', 'chief_complaint', 'invasions', 'X-ray', 'CT', 'Ultrasound', 'CATH', 'MRI', 'reports', 'subject_id_dx',  'anchor_year', 'dod', 'has_x-ray', 'has_ct', 'has_ultrasound', 'has_cath', 'has_ecg', 'has_mri', 'age_group'\n",
    "\n",
    "# keep 'subject_id', 'hadm_id' , icd_code, 'ECG' (for later task 3), charttime\n",
    "df_slim.drop(columns=[ 'note_id', 'note_type', 'note_seq', 'subject_id_dx', 'storetime', 'HPI', 'physical_exam', 'chief_complaint', 'invasions', 'X-ray', 'CT', 'Ultrasound', 'CATH', 'MRI', 'reports', 'subject_id_dx',  'anchor_year', 'dod', 'has_x-ray', 'has_ct', 'has_ultrasound', 'has_cath', 'has_ecg', 'has_mri', 'age_group'], inplace=True)\n",
    "df_slim.columns, len(df_slim.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02161030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim[['exam_type', 'exam_type_label']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23eae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim['has_cardiac_exam'].value_counts(), df_slim['has_radiology_exam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad427d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim['exam_type_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96570cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim['is_dead'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67181e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.drop(columns=['long_title', 'has_radiology_exam' ], inplace=True)\n",
    "df_slim.columns, len(df_slim.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406495ef",
   "metadata": {},
   "source": [
    "## Save SLim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d353275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.to_csv(f\"{DATA_DIR}/heart_diagnoses_1_cleaned_slim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebcaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4reg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
