{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics for Health - Task 1.1.2: Merge Datasets\n",
    "\n",
    "## Overview\n",
    "This notebook merges the four healthcare datasets using two strategies:\n",
    "1. **Option A**: Merge on `subject_id` only (patient-level)\n",
    "2. **Option B**: Merge on `(subject_id, hadm_id)` pair (admission-level)\n",
    "\n",
    "## Objectives\n",
    "- Clean problematic hadm_ids (those with multiple subject_ids)\n",
    "- Add subject_id to datasets that only have hadm_id\n",
    "- Aggregate datasets appropriately\n",
    "- Merge all datasets using both strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Data path: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up file paths\n",
    "notebook_dir = Path.cwd().resolve()\n",
    "data_path = (notebook_dir / '..' / 'Data').resolve()\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Heart Diagnoses: 4,864 rows \u00d7 25 columns\n",
      "Loaded Laboratory Events: 978,503 rows \u00d7 14 columns\n",
      "Loaded Microbiology Events: 15,587 rows \u00d7 14 columns\n",
      "Loaded Procedure Codes: 14,497 rows \u00d7 6 columns\n",
      "\n",
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load all four datasets\n",
    "df1 = pd.read_csv(data_path / 'heart_diagnoses_1.csv')  # Heart Diagnoses\n",
    "df2 = pd.read_csv(data_path / 'laboratory_events_codes_2.csv')  # Laboratory Events\n",
    "df3 = pd.read_csv(data_path / 'microbiology_events_codes_3.csv')  # Microbiology Events\n",
    "df4 = pd.read_csv(data_path / 'procedure_code_4.csv')  # Procedure Codes\n",
    "\n",
    "print(f\"Loaded Heart Diagnoses: {df1.shape[0]:,} rows \u00d7 {df1.shape[1]} columns\")\n",
    "print(f\"Loaded Laboratory Events: {df2.shape[0]:,} rows \u00d7 {df2.shape[1]} columns\")\n",
    "print(f\"Loaded Microbiology Events: {df3.shape[0]:,} rows \u00d7 {df3.shape[1]} columns\")\n",
    "print(f\"Loaded Procedure Codes: {df4.shape[0]:,} rows \u00d7 {df4.shape[1]} columns\")\n",
    "print(\"\\nAll datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for Problematic hadm_ids\n",
    "\n",
    "Some hadm_ids map to multiple subject_ids, which violates data integrity. We'll identify and handle these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Checking for problematic hadm_ids (multiple subject_ids per hadm_id)\n",
      "================================================================================\n",
      "\n",
      "Heart Diagnoses:\n",
      "  Total unique hadm_ids: 4,761\n",
      "  Problematic hadm_ids (multiple subject_ids): 101\n",
      "  Percentage: 2.12%\n",
      "    hadm_id 20200492: 2 subject_ids -> [19781816, 19998560]\n",
      "    hadm_id 20222315: 2 subject_ids -> [19032473, 19998539]\n",
      "    hadm_id 20343031: 2 subject_ids -> [17922874, 19998599]\n",
      "    hadm_id 20624985: 2 subject_ids -> [12483604, 19998509]\n",
      "    hadm_id 20706765: 2 subject_ids -> [12407830, 19998533]\n",
      "\n",
      "Microbiology Events:\n",
      "  Total unique hadm_ids: 2,454\n",
      "  Problematic hadm_ids (multiple subject_ids): 256\n",
      "  Percentage: 10.43%\n",
      "    hadm_id 20007905.0: 3 subject_ids -> [13709807, 19997460, 19997485]\n",
      "    hadm_id 20095782.0: 2 subject_ids -> [17443221, 19997450]\n",
      "    hadm_id 20097155.0: 2 subject_ids -> [18048134, 19997631]\n",
      "    hadm_id 20113266.0: 2 subject_ids -> [13933090, 19997491]\n",
      "    hadm_id 20205373.0: 2 subject_ids -> [17844820, 19997544]\n",
      "\n",
      "Procedure Codes:\n",
      "  Total unique hadm_ids: 3,459\n",
      "  Problematic hadm_ids (multiple subject_ids): 0\n",
      "  Percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to clean datasets: remove hadm_ids with multiple subject_ids\n",
    "def clean_df(df):\n",
    "    \"\"\"Remove rows where hadm_id maps to multiple subject_ids\"\"\"\n",
    "    if 'hadm_id' in df.columns and 'subject_id' in df.columns:\n",
    "        # Count unique subject_ids per hadm_id\n",
    "        counts = df.groupby('hadm_id')['subject_id'].nunique()\n",
    "        # Keep only hadm_ids with exactly one subject_id\n",
    "        valid_hadm = counts[counts == 1].index\n",
    "        return df[df['hadm_id'].isin(valid_hadm)].copy()\n",
    "    return df.copy()\n",
    "\n",
    "# Check for problematic hadm_ids\n",
    "print(\"=\"*80)\n",
    "print(\"Checking for problematic hadm_ids (multiple subject_ids per hadm_id)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, df in [(\"Heart Diagnoses\", df1), (\"Microbiology Events\", df3), (\"Procedure Codes\", df4)]:\n",
    "    if 'hadm_id' in df.columns and 'subject_id' in df.columns:\n",
    "        total_hadm = df['hadm_id'].nunique()\n",
    "        problematic = df.groupby('hadm_id')['subject_id'].nunique()\n",
    "        problematic_count = (problematic > 1).sum()\n",
    "        pct = (problematic_count / total_hadm * 100) if total_hadm > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Total unique hadm_ids: {total_hadm:,}\")\n",
    "        print(f\"  Problematic hadm_ids (multiple subject_ids): {problematic_count}\")\n",
    "        print(f\"  Percentage: {pct:.2f}%\")\n",
    "        \n",
    "        if problematic_count > 0:\n",
    "            examples = problematic[problematic > 1].head(5)\n",
    "            for hadm, count in examples.items():\n",
    "                subject_ids = df[df['hadm_id'] == hadm]['subject_id'].unique()\n",
    "                print(f\"    hadm_id {hadm}: {count} subject_ids -> {list(subject_ids)}\")\n",
    "\n",
    "# Clean all datasets\n",
    "df1_clean = clean_df(df1)\n",
    "df2_clean = df2.copy()  # df2 doesn't have subject_id yet\n",
    "df3_clean = clean_df(df3)\n",
    "df4_clean = clean_df(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Merge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def merge_option_a_subject_id(df1, df2, df3, df4):\n",
    "    \"\"\"\n",
    "    Option A: Merge on subject_id only (patient-level aggregation)\n",
    "    - Start with df1 (Heart Diagnoses) as base\n",
    "    - Add subject_id to df2 (Labs) using reference from df1/df3\n",
    "    - Aggregate all datasets by subject_id\n",
    "    - Merge all on subject_id\n",
    "    \"\"\"\n",
    "    # Create reference table for hadm_id -> subject_id mapping\n",
    "    ref_table = pd.concat([\n",
    "        df1_clean[['hadm_id', 'subject_id']].drop_duplicates(),\n",
    "        df3_clean[['hadm_id', 'subject_id']].drop_duplicates()\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    # Add subject_id to df2 (Labs) - it only has hadm_id\n",
    "    df2_with_subject = df2_clean.merge(ref_table, on='hadm_id', how='left')\n",
    "    print(f\"df2 (Labs) after adding subject_id: {df2_with_subject['subject_id'].notna().sum():,} / {len(df2_with_subject):,} rows have subject_id\")\n",
    "    \n",
    "    # Aggregate df2 by subject_id (numeric columns: mean and count)\n",
    "    numeric_cols_df2 = df2_with_subject.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df2 = [c for c in numeric_cols_df2 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df2 = {}\n",
    "    for col in numeric_cols_df2:\n",
    "        agg_dict_df2[col] = ['mean', 'count']\n",
    "    \n",
    "    df2_agg = df2_with_subject.groupby('subject_id').agg(agg_dict_df2).reset_index()\n",
    "    df2_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df2 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df3 by subject_id\n",
    "    numeric_cols_df3 = df3_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df3 = [c for c in numeric_cols_df3 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df3 = {}\n",
    "    for col in numeric_cols_df3:\n",
    "        agg_dict_df3[col] = ['mean', 'count']\n",
    "    \n",
    "    df3_agg = df3_clean.groupby('subject_id').agg(agg_dict_df3).reset_index()\n",
    "    df3_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df3 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df4 by subject_id\n",
    "    numeric_cols_df4 = df4_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df4 = [c for c in numeric_cols_df4 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df4 = {}\n",
    "    for col in numeric_cols_df4:\n",
    "        agg_dict_df4[col] = ['mean', 'count']\n",
    "    \n",
    "    df4_agg = df4_clean.groupby('subject_id').agg(agg_dict_df4).reset_index()\n",
    "    df4_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df4 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Start with df1 (keep all columns, but aggregate if multiple rows per subject_id)\n",
    "    # For df1, we'll take the first row per subject_id (or could aggregate)\n",
    "    df1_base = df1_clean.groupby('subject_id').first().reset_index()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged = df1_base.merge(df2_agg, on='subject_id', how='outer')\n",
    "    merged = merged.merge(df3_agg, on='subject_id', how='outer')\n",
    "    merged = merged.merge(df4_agg, on='subject_id', how='outer')\n",
    "    \n",
    "    return merged, df2_agg, df3_agg, df4_agg\n",
    "\n",
    "\n",
    "def merge_option_b_subject_hadm_id(df1, df2, df3, df4):\n",
    "    \"\"\"\n",
    "    Option B: Merge on (subject_id, hadm_id) pair (admission-level)\n",
    "    - Start with df1 (Heart Diagnoses) as base\n",
    "    - Add subject_id to df2 (Labs) using reference from df1/df3\n",
    "    - Aggregate all datasets by (subject_id, hadm_id)\n",
    "    - Merge all on (subject_id, hadm_id)\n",
    "    \"\"\"\n",
    "    # Create reference table for hadm_id -> subject_id mapping\n",
    "    ref_table = pd.concat([\n",
    "        df1_clean[['hadm_id', 'subject_id']].drop_duplicates(),\n",
    "        df3_clean[['hadm_id', 'subject_id']].drop_duplicates()\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    # Add subject_id to df2 (Labs)\n",
    "    df2_with_subject = df2_clean.merge(ref_table, on='hadm_id', how='left')\n",
    "    print(f\"df2 (Labs) after adding subject_id: {df2_with_subject['subject_id'].notna().sum():,} / {len(df2_with_subject):,} rows have subject_id\")\n",
    "    \n",
    "    # Aggregate df2 by (subject_id, hadm_id)\n",
    "    numeric_cols_df2 = df2_with_subject.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df2 = [c for c in numeric_cols_df2 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df2 = {}\n",
    "    for col in numeric_cols_df2:\n",
    "        agg_dict_df2[col] = ['mean', 'count']\n",
    "    \n",
    "    df2_agg = df2_with_subject.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df2).reset_index()\n",
    "    df2_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df2 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df3 by (subject_id, hadm_id)\n",
    "    numeric_cols_df3 = df3_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df3 = [c for c in numeric_cols_df3 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df3 = {}\n",
    "    for col in numeric_cols_df3:\n",
    "        agg_dict_df3[col] = ['mean', 'count']\n",
    "    \n",
    "    df3_agg = df3_clean.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df3).reset_index()\n",
    "    df3_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df3 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df4 by (subject_id, hadm_id)\n",
    "    numeric_cols_df4 = df4_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df4 = [c for c in numeric_cols_df4 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df4 = {}\n",
    "    for col in numeric_cols_df4:\n",
    "        agg_dict_df4[col] = ['mean', 'count']\n",
    "    \n",
    "    df4_agg = df4_clean.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df4).reset_index()\n",
    "    df4_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df4 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Start with df1 - get unique (subject_id, hadm_id) pairs\n",
    "    df1_base = df1_clean[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged = df1_base.merge(df2_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    merged = merged.merge(df3_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    merged = merged.merge(df4_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    \n",
    "    return merged, df2_agg, df3_agg, df4_agg\n",
    "\n",
    "print(\"Merge functions defined successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Both Merge Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing both merge strategies...\n",
      "\n",
      "================================================================================\n",
      "OPTION A: Merging on subject_id only\n",
      "================================================================================\n",
      "df2 (Labs) after adding subject_id: 971,633 / 978,503 rows have subject_id\n",
      "Base (df1): 4,202 unique subjects\n",
      "df2 aggregated: 4,237 unique subjects\n",
      "df3 aggregated: 2,082 unique subjects\n",
      "df4 aggregated: 3,229 unique subjects\n",
      "\n",
      "Final merged dataset: 4,278 unique subjects\n",
      "Shape: 4,278 rows \u00d7 35 columns\n",
      "\n",
      "================================================================================\n",
      "OPTION B: Merging on (subject_id, hadm_id) pair\n",
      "================================================================================\n",
      "df2 (Labs) after adding subject_id: 971,633 / 978,503 rows have subject_id\n",
      "Base (df1): 4,660 unique (subject_id, hadm_id) pairs\n",
      "Total (hadm_id, subject_id) pairs from df1 and df3: 4,711\n",
      "df2 aggregated: 4,702 unique (subject_id, hadm_id) pairs\n",
      "df3 aggregated: 2,198 unique (subject_id, hadm_id) pairs\n",
      "df4 aggregated: 3,459 unique (subject_id, hadm_id) pairs\n",
      "\n",
      "Final merged dataset: 4,746 unique (subject_id, hadm_id) pairs\n",
      "Shape: 4,746 rows \u00d7 12 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Executing both merge strategies...\\n\")\n",
    "\n",
    "# Option A: Merge on subject_id only\n",
    "print(\"=\"*80)\n",
    "print(\"OPTION A: Merging on subject_id only\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "merged_option_a, df2_agg_a, df3_agg_a, df4_agg_a = merge_option_a_subject_id(df1_clean, df2_clean, df3_clean, df4_clean)\n",
    "\n",
    "print(f\"Base (df1): {df1_clean['subject_id'].nunique():,} unique subjects\")\n",
    "print(f\"df2 aggregated: {df2_agg_a['subject_id'].nunique():,} unique subjects\")\n",
    "print(f\"df3 aggregated: {df3_agg_a['subject_id'].nunique():,} unique subjects\")\n",
    "print(f\"df4 aggregated: {df4_agg_a['subject_id'].nunique():,} unique subjects\")\n",
    "print(f\"\\nFinal merged dataset: {merged_option_a['subject_id'].nunique():,} unique subjects\")\n",
    "print(f\"Shape: {merged_option_a.shape[0]:,} rows \u00d7 {merged_option_a.shape[1]} columns\")\n",
    "\n",
    "# Option B: Merge on (subject_id, hadm_id) pair\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTION B: Merging on (subject_id, hadm_id) pair\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "merged_option_b, df2_agg_b, df3_agg_b, df4_agg_b = merge_option_b_subject_hadm_id(df1_clean, df2_clean, df3_clean, df4_clean)\n",
    "\n",
    "print(f\"Base (df1): {df1_clean[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "\n",
    "# Get total unique pairs from df1 and df3\n",
    "total_pairs = pd.concat([\n",
    "    df1_clean[['subject_id', 'hadm_id']].drop_duplicates(),\n",
    "    df3_clean[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "]).drop_duplicates().shape[0]\n",
    "print(f\"Total (hadm_id, subject_id) pairs from df1 and df3: {total_pairs:,}\")\n",
    "\n",
    "print(f\"df2 aggregated: {df2_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"df3 aggregated: {df3_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"df4 aggregated: {df4_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"\\nFinal merged dataset: {merged_option_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"Shape: {merged_option_b.shape[0]:,} rows \u00d7 {merged_option_b.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Option A (subject_id only):\n",
      "  - Rows: 4,278\n",
      "  - Columns: 35\n",
      "  - Unique subjects: 4,278\n",
      "\n",
      "Option B (subject_id + hadm_id):\n",
      "  - Rows: 4,746\n",
      "  - Columns: 12\n",
      "  - Unique (subject_id, hadm_id) pairs: 4,746\n",
      "  - Unique subjects: 4,278\n",
      "\n",
      "================================================================================\n",
      "Sample of Option A (first 5 rows):\n",
      "================================================================================\n",
      "   subject_id         note_id     hadm_id note_type  note_seq  \\\n",
      "0  10000980.0  10000980-DS-20  29654838.0        DS      20.0   \n",
      "1  10002013.0   10002013-DS-8  24760295.0        DS       8.0   \n",
      "2  10002155.0   10002155-DS-8  23822395.0        DS       8.0   \n",
      "3  10004457.0  10004457-DS-10  28723315.0        DS      10.0   \n",
      "4  10007058.0   10007058-DS-2  22954658.0        DS       2.0   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2188-01-06 03:00:00  2188-01-07 23:49:00   \n",
      "1  2160-07-13 03:00:00  2160-07-15 16:59:00   \n",
      "2  2129-08-19 03:00:00  2129-08-20 15:29:00   \n",
      "3  2141-08-14 03:00:00  2141-08-14 21:50:00   \n",
      "4  2167-11-12 03:00:00  2167-11-13 14:39:00   \n",
      "\n",
      "                                                 HPI  \\\n",
      "0  :\\n___ yo woman with h/o hypertension, hyperli...   \n",
      "1  :\\n___ w/ PMH of CAD s/p PCI x3, s/p off-pump ...   \n",
      "2  :\\n___ is a ___ yo female with a past medical ...   \n",
      "3  :\\nMr. ___ is a ___ with a hx of CAD (s/p DES ...   \n",
      "4  :\\nMr. ___ is a healthy ___ year-old male who ...   \n",
      "\n",
      "                                       physical_exam  \\\n",
      "0  Admission exam:\\nGENERAL- Oriented x3. Mood, a...   \n",
      "1  Admission:\\nVS- T 99.4 BP 157/88 HR 118 RR 24 ...   \n",
      "2  GENERAL: WDWN in NAD. Oriented x3. Mood, affec...   \n",
      "3  On Admission:\\nVS- 97.8 157/64 101 18 98% RA \\...   \n",
      "4  ADMISSION PHYSICAL EXAM:\\n====================...   \n",
      "\n",
      "                       chief_complaint  ... valuenum_mean valuenum_count  \\\n",
      "0           \\nShortness of breath\\n \\n  ...     50.427489          223.0   \n",
      "1                    \\nchest pain\\n \\n  ...     53.814800           50.0   \n",
      "2                \\nchest pressure\\n \\n  ...     43.598140          387.0   \n",
      "3  \\nAbnormal Stress Test, New AI\\n \\n  ...     62.915600           25.0   \n",
      "4                    \\nChest pain\\n \\n  ...     98.678028          142.0   \n",
      "\n",
      "  ref_range_lower_mean ref_range_lower_count ref_range_upper_mean  \\\n",
      "0            30.837273                 220.0            59.046727   \n",
      "1            30.610000                  50.0            52.862400   \n",
      "2            33.267801                 382.0            55.895340   \n",
      "3            31.069565                  23.0            71.017391   \n",
      "4            27.778014                 141.0            54.845177   \n",
      "\n",
      "  ref_range_upper_count dilution_value_mean dilution_value_count  \\\n",
      "0                 220.0                 NaN                  0.0   \n",
      "1                  50.0                 NaN                  NaN   \n",
      "2                 382.0                 NaN                  0.0   \n",
      "3                  23.0                 NaN                  NaN   \n",
      "4                 141.0                 NaN                  0.0   \n",
      "\n",
      "   seq_num_mean seq_num_count  \n",
      "0           4.0           7.0  \n",
      "1           1.5           2.0  \n",
      "2           4.5           8.0  \n",
      "3           NaN           NaN  \n",
      "4           1.5           2.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "================================================================================\n",
      "Sample of Option B (first 5 rows):\n",
      "================================================================================\n",
      "   subject_id     hadm_id  valuenum_mean  valuenum_count  \\\n",
      "0  10000980.0  26913865.0      50.305427           164.0   \n",
      "1  10000980.0  29654838.0      50.766780            59.0   \n",
      "2  10002013.0  24760295.0      53.814800            50.0   \n",
      "3  10002155.0  23822395.0      43.598140           387.0   \n",
      "4  10004457.0  28723315.0      62.915600            25.0   \n",
      "\n",
      "   ref_range_lower_mean  ref_range_lower_count  ref_range_upper_mean  \\\n",
      "0             31.772393                  163.0             58.486196   \n",
      "1             28.163158                   57.0             60.649649   \n",
      "2             30.610000                   50.0             52.862400   \n",
      "3             33.267801                  382.0             55.895340   \n",
      "4             31.069565                   23.0             71.017391   \n",
      "\n",
      "   ref_range_upper_count  dilution_value_mean  dilution_value_count  \\\n",
      "0                  163.0                  NaN                   0.0   \n",
      "1                   57.0                  NaN                   NaN   \n",
      "2                   50.0                  NaN                   NaN   \n",
      "3                  382.0                  NaN                   0.0   \n",
      "4                   23.0                  NaN                   NaN   \n",
      "\n",
      "   seq_num_mean  seq_num_count  \n",
      "0           4.0            7.0  \n",
      "1           NaN            NaN  \n",
      "2           1.5            2.0  \n",
      "3           4.5            8.0  \n",
      "4           NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"Option A (subject_id only):\")\n",
    "print(f\"  - Rows: {merged_option_a.shape[0]:,}\")\n",
    "print(f\"  - Columns: {merged_option_a.shape[1]}\")\n",
    "print(f\"  - Unique subjects: {merged_option_a['subject_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\nOption B (subject_id + hadm_id):\")\n",
    "print(f\"  - Rows: {merged_option_b.shape[0]:,}\")\n",
    "print(f\"  - Columns: {merged_option_b.shape[1]}\")\n",
    "print(f\"  - Unique (subject_id, hadm_id) pairs: {merged_option_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,}\")\n",
    "print(f\"  - Unique subjects: {merged_option_b['subject_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample of Option A (first 5 rows):\")\n",
    "print(\"=\"*80)\n",
    "print(merged_option_a.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample of Option B (first 5 rows):\")\n",
    "print(\"=\"*80)\n",
    "print(merged_option_b.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Merged Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Saved Option A to: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data/1.1.2_merged_dataset_option_a_subject_id.csv\n",
      "\u2713 Saved Option B to: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data/1.1.2_merged_dataset_option_b_subject_hadm_id.csv\n"
     ]
    }
   ],
   "source": [
    "# Save merged datasets with task prefix\n",
    "# Convert ID columns to integers before saving\n",
    "if 'subject_id' in merged_option_a.columns:\n",
    "    merged_option_a['subject_id'] = merged_option_a['subject_id'].astype('Int64')  # Nullable integer\n",
    "if 'subject_id' in merged_option_b.columns:\n",
    "    merged_option_b['subject_id'] = merged_option_b['subject_id'].astype('Int64')\n",
    "if 'hadm_id' in merged_option_b.columns:\n",
    "    merged_option_b['hadm_id'] = merged_option_b['hadm_id'].astype('Int64')\n",
    "\n",
    "option_a_file = data_path / '1.1.2_merged_dataset_option_a_subject_id.csv'\n",
    "option_b_file = data_path / '1.1.2_merged_dataset_option_b_subject_hadm_id.csv'\n",
    "\n",
    "merged_option_a.to_csv(option_a_file, index=False)\n",
    "print(f\"\u2713 Saved Option A to: {option_a_file}\")\n",
    "\n",
    "merged_option_b.to_csv(option_b_file, index=False)\n",
    "print(f\"\u2713 Saved Option B to: {option_b_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prop5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}