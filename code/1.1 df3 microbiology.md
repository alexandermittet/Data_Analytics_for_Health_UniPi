```python
%matplotlib inline
import math
import re
import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import seaborn as sns
import re

from collections import defaultdict
from scipy.stats import pearsonr
import pandas as pd
from IPython.display import display

```


```python
#%pip install matplotlib-venn
```


```python
DATA_DIR = r"Y:\Studium\3. Sem UniPI\Data Analytics 4 digital Health\Data"

DATASETS = {
    "heart_diagnoses_1": "heart_diagnoses_1.csv",
    "laboratory_events_codes_2": "laboratory_events_codes_2.csv",
    "microbiology_events_codes_3": "microbiology_events_codes_3.csv",
    "procedure_code_4": "procedure_code_4.csv",
}
name = "microbiology_events_codes_3"
```


```python
df = pd.read_csv(f"{DATA_DIR}/{DATASETS[name]}", index_col=False)

df.columns
```




    Index(['subject_id', 'hadm_id', 'chartdate', 'charttime', 'spec_type_desc',
           'test_name', 'org_name', 'ab_name', 'dilution_text',
           'dilution_comparison', 'dilution_value', 'interpretation',
           'technician_id', 'qc_flag'],
          dtype='object')




```python
df[['subject_id','hadm_id']].duplicated().any()
```




    np.True_



# INspections

## A


```python
df.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subject_id</th>
      <th>hadm_id</th>
      <th>chartdate</th>
      <th>charttime</th>
      <th>spec_type_desc</th>
      <th>test_name</th>
      <th>org_name</th>
      <th>ab_name</th>
      <th>dilution_text</th>
      <th>dilution_comparison</th>
      <th>dilution_value</th>
      <th>interpretation</th>
      <th>technician_id</th>
      <th>qc_flag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10000980</td>
      <td>26913865.0</td>
      <td>2189-06-27 00:00:00</td>
      <td>2189-06-27 10:52:00</td>
      <td>MRSA SCREEN</td>
      <td>MRSA SCREEN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_063</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-04 00:00:00</td>
      <td>2129-08-04 17:04:00</td>
      <td>MRSA SCREEN</td>
      <td>MRSA SCREEN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_095</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-05 00:00:00</td>
      <td>2129-08-05 15:54:00</td>
      <td>URINE</td>
      <td>Legionella Urinary Antigen</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_083</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-05 00:00:00</td>
      <td>2129-08-05 18:43:00</td>
      <td>SPUTUM</td>
      <td>GRAM STAIN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_095</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-05 00:00:00</td>
      <td>2129-08-05 18:43:00</td>
      <td>SPUTUM</td>
      <td>RESPIRATORY CULTURE</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_077</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-07 00:00:00</td>
      <td>2129-08-07 12:07:00</td>
      <td>SPUTUM</td>
      <td>ACID FAST SMEAR</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_068</td>
      <td>QC_WARN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-07 00:00:00</td>
      <td>2129-08-07 12:07:00</td>
      <td>SPUTUM</td>
      <td>ACID FAST CULTURE</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_084</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-08 00:00:00</td>
      <td>2129-08-08 12:22:00</td>
      <td>STOOL</td>
      <td>CLOSTRIDIUM DIFFICILE TOXIN A &amp; B TEST</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_013</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-09 00:00:00</td>
      <td>2129-08-09 20:03:00</td>
      <td>SPUTUM</td>
      <td>GRAM STAIN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_033</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10002155</td>
      <td>23822395.0</td>
      <td>2129-08-09 00:00:00</td>
      <td>2129-08-09 20:03:00</td>
      <td>SPUTUM</td>
      <td>RESPIRATORY CULTURE</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_023</td>
      <td>QC_WARN</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.tail(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subject_id</th>
      <th>hadm_id</th>
      <th>chartdate</th>
      <th>charttime</th>
      <th>spec_type_desc</th>
      <th>test_name</th>
      <th>org_name</th>
      <th>ab_name</th>
      <th>dilution_text</th>
      <th>dilution_comparison</th>
      <th>dilution_value</th>
      <th>interpretation</th>
      <th>technician_id</th>
      <th>qc_flag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>15577</th>
      <td>19997660</td>
      <td>20374585.0</td>
      <td>2175-10-12 00:00:00</td>
      <td>2175-10-12 05:45:00</td>
      <td>BLOOD CULTURE</td>
      <td>Aerobic Bottle Gram Stain</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_102</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15578</th>
      <td>19997661</td>
      <td>25968240.0</td>
      <td>2172-03-15 00:00:00</td>
      <td>2172-03-15 15:30:00</td>
      <td>FLUID,OTHER</td>
      <td>FUNGAL CULTURE</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_029</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15579</th>
      <td>19997662</td>
      <td>29338106.0</td>
      <td>2182-08-22 00:00:00</td>
      <td>2182-08-22 09:09:00</td>
      <td>URINE</td>
      <td>Legionella Urinary Antigen</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_082</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15580</th>
      <td>19997663</td>
      <td>20511836.0</td>
      <td>2158-06-06 00:00:00</td>
      <td>2158-06-06 19:30:00</td>
      <td>URINE</td>
      <td>URINE CULTURE</td>
      <td>ESCHERICHIA COLI</td>
      <td>GENTAMICIN</td>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.0</td>
      <td>S</td>
      <td>TECH_114</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15581</th>
      <td>19997664</td>
      <td>25289714.0</td>
      <td>2186-01-02 00:00:00</td>
      <td>2186-01-02 03:40:00</td>
      <td>MRSA SCREEN</td>
      <td>MRSA SCREEN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_049</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15582</th>
      <td>19997665</td>
      <td>26052266.0</td>
      <td>2173-11-09 00:00:00</td>
      <td>2173-11-09 18:09:00</td>
      <td>Staph aureus swab</td>
      <td>Staph aureus Screen</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_051</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15583</th>
      <td>19997666</td>
      <td>24256422.0</td>
      <td>2168-08-03 00:00:00</td>
      <td>2168-08-03 22:00:00</td>
      <td>BLOOD CULTURE</td>
      <td>Blood Culture, Routine</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_086</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15584</th>
      <td>19997667</td>
      <td>20372003.0</td>
      <td>2144-09-21 00:00:00</td>
      <td>2144-09-21 20:14:00</td>
      <td>URINE</td>
      <td>URINE CULTURE</td>
      <td>PSEUDOMONAS AERUGINOSA</td>
      <td>MEROPENEM</td>
      <td>8</td>
      <td>=</td>
      <td>8.0</td>
      <td>I</td>
      <td>TECH_115</td>
      <td>QC_FAIL</td>
    </tr>
    <tr>
      <th>15585</th>
      <td>19997668</td>
      <td>20329436.0</td>
      <td>2129-08-12 00:00:00</td>
      <td>2129-08-12 15:56:00</td>
      <td>CATHETER TIP-IV</td>
      <td>WOUND CULTURE</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_061</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>15586</th>
      <td>19997669</td>
      <td>28219199.0</td>
      <td>2160-07-03 00:00:00</td>
      <td>2160-07-03 15:39:00</td>
      <td>URINE</td>
      <td>URINE CULTURE</td>
      <td>KLEBSIELLA PNEUMONIAE</td>
      <td>TRIMETHOPRIM/SULFA</td>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.0</td>
      <td>S</td>
      <td>TECH_083</td>
      <td>QC_WARN</td>
    </tr>
  </tbody>
</table>
</div>



## B


```python
df["qc_flag"].value_counts(dropna=False) 
```




    qc_flag
    QC_OK      13235
    QC_WARN     1572
    QC_FAIL      779
    Name: count, dtype: int64



S = Sensitive / Susceptible
The organism is inhibited by standard drug levels → drug is expected to work.

R = Resistant
The organism is not inhibited even at high drug concentrations → drug will not work.

I = Intermediate (older meaning) / I = Increased exposure (new CLSI meaning)

Historically: Result is uncertain; success depends on achieving high drug levels.

Modern definitions: Drug can work if exposure is increased (higher dose, longer infusion, high concentrations at infection site).


```python
df["interpretation"].value_counts(dropna=False) 
```




    interpretation
    NaN    10766
    S       3807
    R        856
    I        157
    Name: count, dtype: int64




```python
df["dilution_comparison"].value_counts(dropna=False) 
```




    dilution_comparison
    NaN           10875
    <=             3233
    =               782
    =>              696
    Name: count, dtype: int64




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 15587 entries, 0 to 15586
    Data columns (total 14 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   subject_id           15587 non-null  int64  
     1   hadm_id              15587 non-null  float64
     2   chartdate            15587 non-null  object 
     3   charttime            15587 non-null  object 
     4   spec_type_desc       15587 non-null  object 
     5   test_name            15587 non-null  object 
     6   org_name             5391 non-null   object 
     7   ab_name              4820 non-null   object 
     8   dilution_text        4713 non-null   object 
     9   dilution_comparison  4711 non-null   object 
     10  dilution_value       4711 non-null   float64
     11  interpretation       4820 non-null   object 
     12  technician_id        15587 non-null  object 
     13  qc_flag              15587 non-null  object 
    dtypes: float64(2), int64(1), object(11)
    memory usage: 1.7+ MB
    

==> spec_type_desc == fluid from df2?


```python

```

# Data Understanding and Preprocessing, cleaning ofdf3 micro


```python
df.duplicated().sum()
```




    np.int64(1)




```python
#show duplicated rows
df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()).head(20)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subject_id</th>
      <th>hadm_id</th>
      <th>chartdate</th>
      <th>charttime</th>
      <th>spec_type_desc</th>
      <th>test_name</th>
      <th>org_name</th>
      <th>ab_name</th>
      <th>dilution_text</th>
      <th>dilution_comparison</th>
      <th>dilution_value</th>
      <th>interpretation</th>
      <th>technician_id</th>
      <th>qc_flag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2454</th>
      <td>11823798</td>
      <td>23491105.0</td>
      <td>2186-07-16 00:00:00</td>
      <td>2186-07-16 00:25:00</td>
      <td>BLOOD CULTURE</td>
      <td>Blood Culture, Routine</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_120</td>
      <td>QC_OK</td>
    </tr>
    <tr>
      <th>2455</th>
      <td>11823798</td>
      <td>23491105.0</td>
      <td>2186-07-16 00:00:00</td>
      <td>2186-07-16 00:25:00</td>
      <td>BLOOD CULTURE</td>
      <td>Blood Culture, Routine</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TECH_120</td>
      <td>QC_OK</td>
    </tr>
  </tbody>
</table>
</div>




```python
df = df.drop_duplicates()
```


```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Index: 15586 entries, 0 to 15586
    Data columns (total 14 columns):
     #   Column               Non-Null Count  Dtype  
    ---  ------               --------------  -----  
     0   subject_id           15586 non-null  int64  
     1   hadm_id              15586 non-null  float64
     2   chartdate            15586 non-null  object 
     3   charttime            15586 non-null  object 
     4   spec_type_desc       15586 non-null  object 
     5   test_name            15586 non-null  object 
     6   org_name             5391 non-null   object 
     7   ab_name              4820 non-null   object 
     8   dilution_text        4713 non-null   object 
     9   dilution_comparison  4711 non-null   object 
     10  dilution_value       4711 non-null   float64
     11  interpretation       4820 non-null   object 
     12  technician_id        15586 non-null  object 
     13  qc_flag              15586 non-null  object 
    dtypes: float64(2), int64(1), object(11)
    memory usage: 1.8+ MB
    


```python
for col in df.columns:
    if col == 'charttime' or col == 'hadm_id' or col == 'subject_id' or col == 'chartdate':
        continue
    print(f"{col}: {df[col].value_counts().tail(30)}")
```

    spec_type_desc: spec_type_desc
    CATHETER TIP-IV                                             113
    Blood (EBV)                                                 112
    BRONCHOALVEOLAR LAVAGE                                      107
    Rapid Respiratory Viral Screen & Culture                    102
    Influenza A/B by DFA                                         86
    Blood (CMV AB)                                               86
    BLOOD CULTURE ( MYCO/F LYTIC BOTTLE)                         53
    Mini-BAL                                                     49
    IMMUNOLOGY                                                   46
    BRONCHIAL WASHINGS                                           39
    PERITONEAL FLUID                                             36
    ABSCESS                                                      34
    DIALYSIS FLUID                                               34
    JOINT FLUID                                                  31
    Immunology (CMV)                                             27
    Blood (Toxo)                                                 24
    Blood (LYME)                                                 14
    CSF;SPINAL FLUID                                             10
    FOREIGN BODY                                                  8
    EAR                                                           6
    THROAT FOR STREP                                              6
    SKIN SCRAPINGS                                                5
    BRONCHIAL BRUSH                                               4
    THROAT CULTURE                                                3
    BILE                                                          3
    ASPIRATE                                                      2
    DIRECT ANTIGEN TEST FOR VARICELLA-ZOSTER VIRUS                2
    CRE Screen                                                    1
    STOOL (RECEIVED IN TRANSPORT SYSTEM)                          1
    Direct Antigen Test for Herpes Simplex Virus Types 1 & 2      1
    Name: count, dtype: int64
    test_name: test_name
    NEISSERIA GONORRHOEAE (GC), NUCLEIC ACID PROBE, WITH AMPLIFICATION    6
    Chlamydia trachomatis, Nucleic Acid Probe, with Amplification         6
    VIRAL CULTURE: R/O CYTOMEGALOVIRUS                                    5
    ASO Screen                                                            5
    CYTOMEGALOVIRUS EARLY ANTIGEN TEST (SHELL VIAL METHOD)                4
    RUBELLA IgG SEROLOGY                                                  4
    MTB Direct Amplification                                              4
    MONOSPOT                                                              4
    Respiratory Virus Identification                                      3
    RPR w/check for Prozone                                               3
    NOCARDIA CULTURE                                                      3
    VIRAL CULTURE: R/O HERPES SIMPLEX VIRUS                               2
    DIRECT ANTIGEN TEST FOR VARICELLA-ZOSTER VIRUS                        2
    Enterovirus Culture                                                   2
    Myco-F Bottle Gram Stain                                              2
    VARICELLA-ZOSTER CULTURE                                              2
    Direct Antigen Test for Herpes Simplex Virus Types 1 & 2              1
    Carbapenemase Resistant Enterobacteriaceae Screen                     1
    MICROSPORIDIA STAIN                                                   1
    CYCLOSPORA STAIN                                                      1
    BRUCELLA BLOOD CULTURE                                                1
    YEAST VAGINITIS CULTURE                                               1
    TREPONEMAL ANTIBODY TEST                                              1
    QUANTITATIVE RPR                                                      1
    FUNGAL CULTURE (HAIR/SKIN/NAILS)                                      1
    POTASSIUM HYDROXIDE PREPARATION (HAIR/SKIN/NAILS)                     1
    HCV GENOTYPE                                                          1
    GENITAL CULTURE                                                       1
    O&P MACROSCOPIC EXAM - WORM                                           1
    GRAM STAIN- R/O THRUSH                                                1
    Name: count, dtype: int64
    org_name: org_name
    CANDIDA TROPICALIS                                       3
    MYCOBACTERIUM AVIUM COMPLEX                              3
    HAEMOPHILUS INFLUENZAE, BETA-LACTAMASE POSITIVE          3
    GRAM POSITIVE RODS                                       3
    GRAM NEGATIVE ROD #2                                     2
    NEISSERIA MENINGITIDIS                                   2
    CLOSTRIDIUM SPECIES NOT C. PERFRINGENS OR C. SEPTICUM    2
    POSITIVE FOR INFLUENZA A VIRAL ANTIGEN                   2
    CANDIDA ALBICANS, PRESUMPTIVE IDENTIFICATION             2
    BETA STREPTOCOCCUS GROUP A                               2
    POSITIVE FOR PNEUMOCYSTIS JIROVECII (CARINII)            2
    RESPIRATORY SYNCYTIAL VIRUS (RSV)                        1
    ASPERGILLUS SPECIES                                      1
    PRESUMPTIVE PROPIONIBACTERIUM ACNES                      1
    GRAM NEGATIVE ROD #3                                     1
    PSEUDOMONAS SPECIES                                      1
    ASPERGILLUS FUMIGATUS                                    1
    HAEMOPHILUS SPECIES NOT INFLUENZAE                       1
    HAEMOPHILUS INFLUENZAE, BETA-LACTAMASE NEGATIVE          1
    CYTOMEGALOVIRUS                                          1
    TRICHOPHYTON SPECIES                                     1
    GRAM NEGATIVE ROD #4                                     1
    GEMELLA SPECIES                                          1
    MYCOBACTERIUM FORTUITUM                                  1
    GIARDIA CYSTS                                            1
    PENICILLIUM SPECIES                                      1
    LACTOBACILLUS SPECIES                                    1
    ASPERGILLUS SP. NOT FUMIGATUS, FLAVUS OR NIGER           1
    HERPES SIMPLEX VIRUS TYPE 1                              1
    STAPHYLOCOCCUS SPECIES                                   1
    Name: count, dtype: int64
    ab_name: ab_name
    GENTAMICIN              453
    TRIMETHOPRIM/SULFA      389
    TOBRAMYCIN              287
    CEFTAZIDIME             285
    CIPROFLOXACIN           284
    MEROPENEM               283
    CEFEPIME                282
    NITROFURANTOIN          261
    CEFTRIAXONE             248
    PIPERACILLIN/TAZO       223
    AMPICILLIN              206
    AMPICILLIN/SULBACTAM    201
    CEFAZOLIN               197
    TETRACYCLINE            186
    LEVOFLOXACIN            173
    OXACILLIN               171
    VANCOMYCIN              169
    ERYTHROMYCIN            153
    CLINDAMYCIN             146
    RIFAMPIN                 57
    PIPERACILLIN             41
    CEFUROXIME               36
    PENICILLIN G             35
    AMIKACIN                 27
    LINEZOLID                14
    IMIPENEM                  9
    DAPTOMYCIN                4
    Name: count, dtype: int64
    dilution_text: dilution_text
    <=0.5     371
    <=4       345
    <=2       186
    =>16      171
    <=16      169
    =>8       157
    4         148
    =>4       143
    8         133
    2         119
    =>32      118
    1          90
    16         78
    =>64       74
    0.5        56
    0.25       53
    <=0.12     48
    32         44
    64         36
    =>0.5      15
    128        13
    =>128      13
    256        10
    <=0.06      3
    =>2         3
    <=8         2
    =>512       2
    >256        2
    0.12        1
    3           1
    Name: count, dtype: int64
    dilution_comparison: dilution_comparison
    <=            3233
    =              782
    =>             696
    Name: count, dtype: int64
    dilution_value: dilution_value
    1.00      1544
    0.25       708
    4.00       636
    0.50       442
    16.00      418
    2.00       308
    8.00       292
    32.00      162
    64.00      110
    0.12        49
    128.00      26
    256.00      10
    0.06         3
    512.00       2
    3.00         1
    Name: count, dtype: int64
    interpretation: interpretation
    S    3807
    R     856
    I     157
    Name: count, dtype: int64
    technician_id: technician_id
    TECH_013    123
    TECH_032    123
    TECH_033    123
    TECH_110    122
    TECH_051    122
    TECH_092    121
    TECH_078    120
    TECH_100    120
    TECH_002    120
    TECH_045    120
    TECH_052    120
    TECH_104    119
    TECH_056    118
    TECH_080    118
    TECH_008    118
    TECH_066    117
    TECH_012    116
    TECH_015    116
    TECH_083    116
    TECH_070    115
    TECH_017    115
    TECH_061    115
    TECH_019    115
    TECH_044    115
    TECH_053    114
    TECH_018    113
    TECH_005    113
    TECH_058    110
    TECH_096    105
    TECH_120    104
    Name: count, dtype: int64
    qc_flag: qc_flag
    QC_OK      13235
    QC_WARN     1572
    QC_FAIL      779
    Name: count, dtype: int64
    

No wrong nans found

## Check for wrong NaNs / non typical entries in each column

#### Find wrong NaNs

- valueuom: has '' , remove; Pos/Neg == +/-; U ??
- value has wrong entries inspect and extrat if possible to valuenum


### Handle Value wrong nans, then extract missing from value into new column valuenum_merged if possible


```python
# # chec k only value column: show all unique non-numerical entries in 'value' column
# non_numerical_values = []
# for idx, value in df['value'].items():
#     if pd.isna(value):  # Skip NaN/None
#         continue
    
#     # Try to convert to float
#     try:
#         float(value)
#     except (ValueError, TypeError):
#         non_numerical_values.append({
#             'index': idx,
#             'value': value,
#             'type': type(value).__name__
#         })
# print(f"\n{'─'*80}")
# print(f"Column: 'value' | Non-numerical entries: {len(non_numerical_values)}")
# print(f"{'─'*80}")
# # Get unique non-numerical values
# unique_values = list(set([e['value'] for e in non_numerical_values]))
# print(f"Unique non-numerical values ({len(unique_values)}):")
# for val in sorted(unique_values):
#     count = sum(1 for e in non_numerical_values if e['value'] == val)
#     print(f"  • '{val}' — appears {count} times")   
    
```

FINDINGS

=> in col value, We want to convert '___' and 'NONE' 'ERROR' to np.nan!

=> then we create a new col value_extracted (float64) out of col value where:
- we can calculate as float complete / like 20/0 but only if there is anumber before and after the /! => complete
- we can take the middle point of complete ranges like '80-160'
- we can calculate a float value of comparisons with < > by sub/add 0.1 to the number, eg. '>1.050' => 1.150 or '<1' => 0.9
- the rest is to nuemic error coerce put to NaN.

=> then, we fill np.nan entries in col valuenum with values from valUe_extracted if they are not nan and tell me the amount of filled rows and show examples beffore and after

 ## Convert datetimes


```python
cols = ['charttime', 'chartdate']  

for col in cols:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')
        print(f"{col}: parsed {df[col].notna().sum()} values, {df[col].isna().sum()} NaT")

display(df[[c for c in df.columns if c in cols]])
```

    charttime: parsed 15586 values, 0 NaT
    chartdate: parsed 15586 values, 0 NaT
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chartdate</th>
      <th>charttime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2189-06-27</td>
      <td>2189-06-27 10:52:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2129-08-04</td>
      <td>2129-08-04 17:04:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2129-08-05</td>
      <td>2129-08-05 15:54:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2129-08-05</td>
      <td>2129-08-05 18:43:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2129-08-05</td>
      <td>2129-08-05 18:43:00</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>15582</th>
      <td>2173-11-09</td>
      <td>2173-11-09 18:09:00</td>
    </tr>
    <tr>
      <th>15583</th>
      <td>2168-08-03</td>
      <td>2168-08-03 22:00:00</td>
    </tr>
    <tr>
      <th>15584</th>
      <td>2144-09-21</td>
      <td>2144-09-21 20:14:00</td>
    </tr>
    <tr>
      <th>15585</th>
      <td>2129-08-12</td>
      <td>2129-08-12 15:56:00</td>
    </tr>
    <tr>
      <th>15586</th>
      <td>2160-07-03</td>
      <td>2160-07-03 15:39:00</td>
    </tr>
  </tbody>
</table>
<p>15586 rows × 2 columns</p>
</div>


## inspect qc_flag == FAIL


```python
# check qc_flag == 'FAIL' 
df["qc_flag"].value_counts()
```




    qc_flag
    QC_OK      13235
    QC_WARN     1572
    QC_FAIL      779
    Name: count, dtype: int64



inspect fail dataset


```python
df_test = df[df["qc_flag"] == 'QC_FAIL'].copy()
df_test.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Index: 779 entries, 42 to 15584
    Data columns (total 14 columns):
     #   Column               Non-Null Count  Dtype         
    ---  ------               --------------  -----         
     0   subject_id           779 non-null    int64         
     1   hadm_id              779 non-null    float64       
     2   chartdate            779 non-null    datetime64[ns]
     3   charttime            779 non-null    datetime64[ns]
     4   spec_type_desc       779 non-null    object        
     5   test_name            779 non-null    object        
     6   org_name             290 non-null    object        
     7   ab_name              259 non-null    object        
     8   dilution_text        255 non-null    object        
     9   dilution_comparison  255 non-null    object        
     10  dilution_value       255 non-null    float64       
     11  interpretation       259 non-null    object        
     12  technician_id        779 non-null    object        
     13  qc_flag              779 non-null    object        
    dtypes: datetime64[ns](2), float64(2), int64(1), object(9)
    memory usage: 91.3+ KB
    

2% fail, 8% warn.

IDEA: small percentage => set valuenum_merged to np.nan those rows bc qualtiy control failed


```python
# Handle QC flags
qc_fail_mask = df['qc_flag'] == 'QC_FAIL'
qc_warn_mask = df['qc_flag'] == 'QC_WARN'

# Drop failed measurements from dilution_value       
before_non_null = df['dilution_value'].notna().sum()
df.loc[qc_fail_mask, 'dilution_value'] = np.nan
df.loc[qc_fail_mask, 'dilution_comparison'] = np.nan
df.loc[qc_fail_mask, 'dilution_text'] = np.nan
after_non_null = df['dilution_value'].notna().sum()

print(f"set {before_non_null - after_non_null:,} FAIL measurements from dilution_value to nan.")
print(f"Coverage drop: {(before_non_null - after_non_null) / len(df) * 100:.2f}%")

# Binary QC features for downstream aggregation/clustering
df['is_qc_fail'] = qc_fail_mask.astype(int)
df['is_qc_warn'] = qc_warn_mask.astype(int)
df['is_qc_ok'] = (~qc_fail_mask & ~qc_warn_mask).astype(int)

print(df[['qc_flag', 'is_qc_ok', 'is_qc_warn', 'is_qc_fail']].head())
```

    set 255 FAIL measurements from dilution_value to nan.
    Coverage drop: 1.64%
      qc_flag  is_qc_ok  is_qc_warn  is_qc_fail
    0   QC_OK         1           0           0
    1   QC_OK         1           0           0
    2   QC_OK         1           0           0
    3   QC_OK         1           0           0
    4   QC_OK         1           0           0
    

## Check if dilution_text matches with diluation_value and dilution_comparison


```python
# check if 
df_test = df[df["dilution_comparison"].isna()].copy()
df_test["dilution_text"].value_counts(dropna=False)
```




    dilution_text
    NaN     11128
    >256        2
    Name: count, dtype: int64




```python
df_test[df["dilution_text"] == ">256"]
```

    C:\Users\dgars\AppData\Local\Temp\ipykernel_17012\2108123711.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
      df_test[df["dilution_text"] == ">256"]
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subject_id</th>
      <th>hadm_id</th>
      <th>chartdate</th>
      <th>charttime</th>
      <th>spec_type_desc</th>
      <th>test_name</th>
      <th>org_name</th>
      <th>ab_name</th>
      <th>dilution_text</th>
      <th>dilution_comparison</th>
      <th>dilution_value</th>
      <th>interpretation</th>
      <th>technician_id</th>
      <th>qc_flag</th>
      <th>is_qc_fail</th>
      <th>is_qc_warn</th>
      <th>is_qc_ok</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8607</th>
      <td>15762152</td>
      <td>21311947.0</td>
      <td>2134-02-07</td>
      <td>2134-02-07 22:30:00</td>
      <td>SWAB</td>
      <td>R/O VANCOMYCIN RESISTANT ENTEROCOCCUS</td>
      <td>ENTEROCOCCUS SP.</td>
      <td>VANCOMYCIN</td>
      <td>&gt;256</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>R</td>
      <td>TECH_062</td>
      <td>QC_OK</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13478</th>
      <td>18991843</td>
      <td>26003222.0</td>
      <td>2146-05-01</td>
      <td>2146-05-01 06:03:00</td>
      <td>SWAB</td>
      <td>R/O VANCOMYCIN RESISTANT ENTEROCOCCUS</td>
      <td>ENTEROCOCCUS SP.</td>
      <td>VANCOMYCIN</td>
      <td>&gt;256</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>R</td>
      <td>TECH_117</td>
      <td>QC_OK</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



fix those


```python
df.loc[df["dilution_text"] == ">256", "dilution_comparison"] = ">"
df.loc[df["dilution_text"] == ">256", "dilution_value"] = 256.0
```

#### in genreal (same findings)


```python
# in general:
print("="*80)
print("CHECK: dilution_text vs dilution_value + dilution_comparison")
print("="*80)

# Extract comparison operator and value from dilution_text
def parse_dilution_text(text):
    if pd.isna(text):
        return np.nan, np.nan
    text = str(text).strip()
    
    # Match patterns like ">256", "<=0.5", "=4", etc.
    match = re.match(r'^([<>=]+)?\s*(\d+\.?\d*)$', text)
    if match:
        comp = match.group(1) if match.group(1) else '='
        val = float(match.group(2))
        return comp, val
    return np.nan, np.nan

# Parse dilution_text
parsed = df['dilution_text'].apply(parse_dilution_text)
df['_parsed_comp'] = parsed.apply(lambda x: x[0])
df['_parsed_val'] = parsed.apply(lambda x: x[1])

# Compare with existing columns
df['_comp_match'] = df['_parsed_comp'] == df['dilution_comparison']
df['_val_match'] = df['_parsed_val'] == df['dilution_value']

# Summary
has_text = df['dilution_text'].notna()
print(f"\nRows with dilution_text: {has_text.sum():,}")

comp_match = (df['_comp_match'] == True).sum()
val_match = (df['_val_match'] == True).sum()
both_match = ((df['_comp_match'] == True) & (df['_val_match'] == True)).sum()

print(f"\nComparison matches: {comp_match:,}")
print(f"Value matches: {val_match:,}")
print(f"Both match: {both_match:,}")

# Show mismatches
mismatches = df[has_text & ((df['_comp_match'] == False) | (df['_val_match'] == False))]
print(f"\nMismatches: {len(mismatches):,}")

if len(mismatches) > 0:
    print("\nSample mismatches:")
    display(mismatches[['dilution_text', '_parsed_comp', '_parsed_val', 
                        'dilution_comparison', 'dilution_value']].head(20))

# Show rows where dilution_text exists but comparison/value is NaN
missing_parsed = df[has_text & (df['dilution_comparison'].isna() | df['dilution_value'].isna())]
print(f"\nRows with dilution_text but missing comparison/value: {len(missing_parsed):,}")
if len(missing_parsed) > 0:
    print("\nSample rows to potentially fill:")
    display(missing_parsed[['dilution_text', '_parsed_comp', '_parsed_val', 
                            'dilution_comparison', 'dilution_value']].head(20))

# Cleanup
df.drop(columns=['_parsed_comp', '_parsed_val', '_comp_match', '_val_match'], inplace=True)
```

    ================================================================================
    CHECK: dilution_text vs dilution_value + dilution_comparison
    ================================================================================
    
    Rows with dilution_text: 4,458
    
    Comparison matches: 2
    Value matches: 4,458
    Both match: 2
    
    Mismatches: 4,456
    
    Sample mismatches:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dilution_text</th>
      <th>_parsed_comp</th>
      <th>_parsed_val</th>
      <th>dilution_comparison</th>
      <th>dilution_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>125</th>
      <td>8</td>
      <td>=</td>
      <td>8.00</td>
      <td>=</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>126</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>127</th>
      <td>&lt;=16</td>
      <td>&lt;=</td>
      <td>16.00</td>
      <td>&lt;=</td>
      <td>16.00</td>
    </tr>
    <tr>
      <th>128</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>129</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>130</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>131</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>132</th>
      <td>&lt;=0.25</td>
      <td>&lt;=</td>
      <td>0.25</td>
      <td>&lt;=</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>133</th>
      <td>8</td>
      <td>=</td>
      <td>8.00</td>
      <td>=</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>134</th>
      <td>&lt;=1</td>
      <td>&lt;=</td>
      <td>1.00</td>
      <td>&lt;=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>135</th>
      <td>&lt;=0.25</td>
      <td>&lt;=</td>
      <td>0.25</td>
      <td>&lt;=</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>157</th>
      <td>=&gt;8</td>
      <td>=&gt;</td>
      <td>8.00</td>
      <td>=&gt;</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>158</th>
      <td>=&gt;8</td>
      <td>=&gt;</td>
      <td>8.00</td>
      <td>=&gt;</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>159</th>
      <td>&lt;=0.5</td>
      <td>&lt;=</td>
      <td>0.50</td>
      <td>&lt;=</td>
      <td>0.50</td>
    </tr>
    <tr>
      <th>160</th>
      <td>2</td>
      <td>=</td>
      <td>2.00</td>
      <td>=</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>161</th>
      <td>&lt;=0.5</td>
      <td>&lt;=</td>
      <td>0.50</td>
      <td>&lt;=</td>
      <td>0.50</td>
    </tr>
    <tr>
      <th>162</th>
      <td>1</td>
      <td>=</td>
      <td>1.00</td>
      <td>=</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>163</th>
      <td>=&gt;4</td>
      <td>=&gt;</td>
      <td>4.00</td>
      <td>=&gt;</td>
      <td>4.00</td>
    </tr>
    <tr>
      <th>164</th>
      <td>=&gt;8</td>
      <td>=&gt;</td>
      <td>8.00</td>
      <td>=&gt;</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>165</th>
      <td>&lt;=0.5</td>
      <td>&lt;=</td>
      <td>0.50</td>
      <td>&lt;=</td>
      <td>0.50</td>
    </tr>
  </tbody>
</table>
</div>


    
    Rows with dilution_text but missing comparison/value: 0
    

## Little intermed inspection 


```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Index: 15586 entries, 0 to 15586
    Data columns (total 17 columns):
     #   Column               Non-Null Count  Dtype         
    ---  ------               --------------  -----         
     0   subject_id           15586 non-null  int64         
     1   hadm_id              15586 non-null  float64       
     2   chartdate            15586 non-null  datetime64[ns]
     3   charttime            15586 non-null  datetime64[ns]
     4   spec_type_desc       15586 non-null  object        
     5   test_name            15586 non-null  object        
     6   org_name             5391 non-null   object        
     7   ab_name              4820 non-null   object        
     8   dilution_text        4458 non-null   object        
     9   dilution_comparison  4458 non-null   object        
     10  dilution_value       4458 non-null   float64       
     11  interpretation       4820 non-null   object        
     12  technician_id        15586 non-null  object        
     13  qc_flag              15586 non-null  object        
     14  is_qc_fail           15586 non-null  int64         
     15  is_qc_warn           15586 non-null  int64         
     16  is_qc_ok             15586 non-null  int64         
    dtypes: datetime64[ns](2), float64(2), int64(4), object(9)
    memory usage: 2.6+ MB
    


```python
df['hadm_id'] = df['hadm_id'].astype('int64')
```

## Handle missing values


```python
# print sum of all missing values per column
for col in df.columns:
    missing_count = df[col].isna().sum()
    if missing_count > 0:
        print(f"Column '{col}': {missing_count} missing values")
```

    Column 'org_name': 10195 missing values
    Column 'ab_name': 10766 missing values
    Column 'dilution_text': 11128 missing values
    Column 'dilution_comparison': 11128 missing values
    Column 'dilution_value': 11128 missing values
    Column 'interpretation': 10766 missing values
    


```python
#%pip install matplotlib-venn
```


```python
# overlap of those missing vals (maybe venn diagramm actually? )
from matplotlib_venn import venn3

# Create missing value masks
missing_org = df['org_name'].isna()
missing_ab = df['ab_name'].isna()
missing_dilution = df['dilution_text'].isna()  # dilution_comparison, dilution_value have same pattern

# Calculate overlaps
only_org = (missing_org & ~missing_ab & ~missing_dilution).sum()
only_ab = (~missing_org & missing_ab & ~missing_dilution).sum()
only_dilution = (~missing_org & ~missing_ab & missing_dilution).sum()
org_ab = (missing_org & missing_ab & ~missing_dilution).sum()
org_dilution = (missing_org & ~missing_ab & missing_dilution).sum()
ab_dilution = (~missing_org & missing_ab & missing_dilution).sum()
all_three = (missing_org & missing_ab & missing_dilution).sum()

# Venn diagram
fig, ax = plt.subplots(figsize=(10, 8))
venn = venn3(
    subsets=(only_org, only_ab, org_ab, only_dilution, org_dilution, ab_dilution, all_three),
    set_labels=('org_name\nmissing', 'ab_name\nmissing', 'dilution_*\nmissing'),
    ax=ax
)
ax.set_title('Overlap of Missing Values in Microbiology Data', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('missing_values_venn.png', dpi=300, bbox_inches='tight')
plt.show()

# Summary table
print("="*60)
print("MISSING VALUES OVERLAP SUMMARY")
print("="*60)
print(f"\nTotal rows: {len(df):,}")
print(f"\nIndividual missing counts:")
print(f"  org_name:    {missing_org.sum():,}")
print(f"  ab_name:     {missing_ab.sum():,}")
print(f"  dilution_*:  {missing_dilution.sum():,}")
print(f"\nOverlap breakdown:")
print(f"  Only org_name missing:              {only_org:,}")
print(f"  Only ab_name missing:               {only_ab:,}")
print(f"  Only dilution_* missing:            {only_dilution:,}")
print(f"  org_name + ab_name:                 {org_ab:,}")
print(f"  org_name + dilution_*:              {org_dilution:,}")
print(f"  ab_name + dilution_*:               {ab_dilution:,}")
print(f"  All three missing:                  {all_three:,}")
print(f"\nRows with ANY missing:              {(missing_org | missing_ab | missing_dilution).sum():,}")
print(f"Rows with NO missing (complete):    {(~missing_org & ~missing_ab & ~missing_dilution).sum():,}")
```


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_51_0.png)
    


    ============================================================
    MISSING VALUES OVERLAP SUMMARY
    ============================================================
    
    Total rows: 15,586
    
    Individual missing counts:
      org_name:    10,195
      ab_name:     10,766
      dilution_*:  11,128
    
    Overlap breakdown:
      Only org_name missing:              0
      Only ab_name missing:               0
      Only dilution_* missing:            362
      org_name + ab_name:                 0
      org_name + dilution_*:              0
      ab_name + dilution_*:               571
      All three missing:                  10,195
    
    Rows with ANY missing:              11,128
    Rows with NO missing (complete):    4,458
    

### Do correlation matrix

## Save


```python
df.to_csv(f"{DATA_DIR}/{DATASETS[name].replace('.csv', '_cleaned.csv')}", index=False)
```


```python
# load already cleaned to skip first steps
#df = pd.read_csv(f"{DATA_DIR}/{DATASETS[name].replace('.csv', '_cleaned.csv')}")
```


```python
cols = ['charttime', 'chartdate']  

for col in cols:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')
        print(f"{col}: parsed {df[col].notna().sum()} values, {df[col].isna().sum()} NaT")

display(df[[c for c in df.columns if c in cols]])
```

    charttime: parsed 15586 values, 0 NaT
    chartdate: parsed 15586 values, 0 NaT
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chartdate</th>
      <th>charttime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2189-06-27</td>
      <td>2189-06-27 10:52:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2129-08-04</td>
      <td>2129-08-04 17:04:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2129-08-05</td>
      <td>2129-08-05 15:54:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2129-08-05</td>
      <td>2129-08-05 18:43:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2129-08-05</td>
      <td>2129-08-05 18:43:00</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>15582</th>
      <td>2173-11-09</td>
      <td>2173-11-09 18:09:00</td>
    </tr>
    <tr>
      <th>15583</th>
      <td>2168-08-03</td>
      <td>2168-08-03 22:00:00</td>
    </tr>
    <tr>
      <th>15584</th>
      <td>2144-09-21</td>
      <td>2144-09-21 20:14:00</td>
    </tr>
    <tr>
      <th>15585</th>
      <td>2129-08-12</td>
      <td>2129-08-12 15:56:00</td>
    </tr>
    <tr>
      <th>15586</th>
      <td>2160-07-03</td>
      <td>2160-07-03 15:39:00</td>
    </tr>
  </tbody>
</table>
<p>15586 rows × 2 columns</p>
</div>



```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Index: 15586 entries, 0 to 15586
    Data columns (total 17 columns):
     #   Column               Non-Null Count  Dtype         
    ---  ------               --------------  -----         
     0   subject_id           15586 non-null  int64         
     1   hadm_id              15586 non-null  int64         
     2   chartdate            15586 non-null  datetime64[ns]
     3   charttime            15586 non-null  datetime64[ns]
     4   spec_type_desc       15586 non-null  object        
     5   test_name            15586 non-null  object        
     6   org_name             5391 non-null   object        
     7   ab_name              4820 non-null   object        
     8   dilution_text        4458 non-null   object        
     9   dilution_comparison  4458 non-null   object        
     10  dilution_value       4458 non-null   float64       
     11  interpretation       4820 non-null   object        
     12  technician_id        15586 non-null  object        
     13  qc_flag              15586 non-null  object        
     14  is_qc_fail           15586 non-null  int64         
     15  is_qc_warn           15586 non-null  int64         
     16  is_qc_ok             15586 non-null  int64         
    dtypes: datetime64[ns](2), float64(1), int64(5), object(9)
    memory usage: 2.6+ MB
    

# Create features and slim version


```python
df_slim = df.copy() 
```


```python
df_slim.columns
```




    Index(['subject_id', 'hadm_id', 'chartdate', 'charttime', 'spec_type_desc',
           'test_name', 'org_name', 'ab_name', 'dilution_text',
           'dilution_comparison', 'dilution_value', 'interpretation',
           'technician_id', 'qc_flag', 'is_qc_fail', 'is_qc_warn', 'is_qc_ok'],
          dtype='object')




```python
df_slim.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Index: 15586 entries, 0 to 15586
    Data columns (total 17 columns):
     #   Column               Non-Null Count  Dtype         
    ---  ------               --------------  -----         
     0   subject_id           15586 non-null  int64         
     1   hadm_id              15586 non-null  int64         
     2   chartdate            15586 non-null  datetime64[ns]
     3   charttime            15586 non-null  datetime64[ns]
     4   spec_type_desc       15586 non-null  object        
     5   test_name            15586 non-null  object        
     6   org_name             5391 non-null   object        
     7   ab_name              4820 non-null   object        
     8   dilution_text        4458 non-null   object        
     9   dilution_comparison  4458 non-null   object        
     10  dilution_value       4458 non-null   float64       
     11  interpretation       4820 non-null   object        
     12  technician_id        15586 non-null  object        
     13  qc_flag              15586 non-null  object        
     14  is_qc_fail           15586 non-null  int64         
     15  is_qc_warn           15586 non-null  int64         
     16  is_qc_ok             15586 non-null  int64         
    dtypes: datetime64[ns](2), float64(1), int64(5), object(9)
    memory usage: 2.6+ MB
    

## features


```python
df_micro = df_slim.copy()

gb = df_micro.groupby(['subject_id', 'hadm_id'])
```


```python
def first_non_null(series):
    """Get first non-null value, or None if all null"""
    non_null = series.dropna()
    return non_null.iloc[0] if len(non_null) > 0 else None
```


```python
# --- Core count ---
feat_total_micro = gb.size().rename('total_microbio_events')

# --- Diversity features ---
feat_spec_types = gb['spec_type_desc'].nunique().rename('unique_specimen_types')
feat_test_names = gb['test_name'].nunique().rename('unique_test_names')
feat_orgs = gb['org_name'].nunique().rename('unique_organisms')
feat_ab = gb['ab_name'].nunique().rename('unique_antibiotics')
feat_tech = gb['technician_id'].nunique().rename('unique_technicians')

# --- Susceptibility ---
feat_susc = gb['interpretation'].apply(lambda x: (x == 'S').sum()).rename('num_susceptible')
feat_res  = gb['interpretation'].apply(lambda x: (x == 'R').sum()).rename('num_resistant')
feat_int  = gb['interpretation'].apply(lambda x: (x == 'I').sum()).rename('num_intermediate')

feat_res_ratio = (feat_res / feat_total_micro).rename('resistant_ratio')

# --- QC flags ---
feat_qc_fail = gb['is_qc_fail'].sum().rename('micro_qc_fail')
feat_qc_warn = gb['is_qc_warn'].sum().rename('micro_qc_warn')
feat_qc_ok   = gb['is_qc_ok'].sum().rename('micro_qc_ok')

# --- Temporal ---
feat_time_span = (
    (gb['charttime'].max() - gb['charttime'].min()).dt.total_seconds() / 3600
).rename('micro_time_span_hours')

# --- Merge ---
micro_features = pd.concat([
    feat_total_micro,
    feat_spec_types,
    feat_test_names,
    feat_orgs,
    feat_ab,
    feat_tech,
    feat_susc,
    feat_res,
    feat_int,
    feat_res_ratio,
    feat_qc_fail,
    feat_qc_warn,
    feat_qc_ok,
    feat_time_span
], axis=1)

# presence flag
micro_features['has_micro'] = 1

micro_features = micro_features.reset_index()
```


```python
micro_features.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>subject_id</th>
      <th>hadm_id</th>
      <th>total_microbio_events</th>
      <th>unique_specimen_types</th>
      <th>unique_test_names</th>
      <th>unique_organisms</th>
      <th>unique_antibiotics</th>
      <th>unique_technicians</th>
      <th>num_susceptible</th>
      <th>num_resistant</th>
      <th>num_intermediate</th>
      <th>resistant_ratio</th>
      <th>micro_qc_fail</th>
      <th>micro_qc_warn</th>
      <th>micro_qc_ok</th>
      <th>micro_time_span_hours</th>
      <th>has_micro</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10000980</td>
      <td>26913865</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10002155</td>
      <td>23822395</td>
      <td>12</td>
      <td>4</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>11</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>2</td>
      <td>10</td>
      <td>215.966667</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10007058</td>
      <td>22954658</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>28.650000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10013569</td>
      <td>22891949</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10017531</td>
      <td>20668418</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
micro_features.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 2756 entries, 0 to 2755
    Data columns (total 17 columns):
     #   Column                 Non-Null Count  Dtype  
    ---  ------                 --------------  -----  
     0   subject_id             2756 non-null   int64  
     1   hadm_id                2756 non-null   int64  
     2   total_microbio_events  2756 non-null   int64  
     3   unique_specimen_types  2756 non-null   int64  
     4   unique_test_names      2756 non-null   int64  
     5   unique_organisms       2756 non-null   int64  
     6   unique_antibiotics     2756 non-null   int64  
     7   unique_technicians     2756 non-null   int64  
     8   num_susceptible        2756 non-null   int64  
     9   num_resistant          2756 non-null   int64  
     10  num_intermediate       2756 non-null   int64  
     11  resistant_ratio        2756 non-null   float64
     12  micro_qc_fail          2756 non-null   int64  
     13  micro_qc_warn          2756 non-null   int64  
     14  micro_qc_ok            2756 non-null   int64  
     15  micro_time_span_hours  2756 non-null   float64
     16  has_micro              2756 non-null   int64  
    dtypes: float64(2), int64(15)
    memory usage: 366.2 KB
    


```python
micro_features.to_csv(f"{DATA_DIR}/{DATASETS[name].replace('.csv', '_agg_features_large.csv')}", index=False)
```

reduced set

### Corr


```python
print("="*80)
print("CORRELATION MATRIX ANALYSIS")
print("="*80)

# Select numeric columns only
numeric_cols = micro_features.select_dtypes(include=[np.number]).columns.tolist()
print(f"\nNumeric columns found: {numeric_cols}")
print(f"Total numeric columns: {len(numeric_cols)}\n")

if len(numeric_cols) > 1:
    # Calculate correlation matrix
    correlation_matrix = micro_features[numeric_cols].corr()
    
    print("="*80)
    print("CORRELATION MATRIX")
    print("="*80)
    print(correlation_matrix.round(3))
    
    # Create visualization
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    
    # Plot 1: Full correlation heatmap
    ax1 = axes[0]
    sns.heatmap(correlation_matrix, 
                annot=True, 
                fmt='.2f', 
                cmap='coolwarm', 
                center=0,
                square=True,
                linewidths=0.5,
                cbar_kws={'label': 'Correlation Coefficient'},
                ax=ax1)
    ax1.set_title('Full Correlation Matrix Heatmap', fontsize=14, fontweight='bold')
    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
    plt.setp(ax1.get_yticklabels(), rotation=0)
    
    # Plot 2: Mask for upper triangle (cleaner view)
    ax2 = axes[1]
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, 
                annot=True, 
                fmt='.2f', 
                cmap='coolwarm', 
                center=0,
                square=True,
                linewidths=0.5,
                mask=mask,
                cbar_kws={'label': 'Correlation Coefficient'},
                ax=ax2)
    ax2.set_title('Lower Triangle Correlation Matrix (Unique Pairs)', fontsize=14, fontweight='bold')
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
    plt.setp(ax2.get_yticklabels(), rotation=0)
    
    plt.tight_layout()
    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Extract strong correlations (> 0.5 or < -0.5, excluding diagonal)
    print("\n" + "="*80)
    print("STRONG CORRELATIONS (|r| > 0.5, excluding self-correlations)")
    print("="*80)
    
    strong_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_value = correlation_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                strong_corr.append({
                    'Variable 1': correlation_matrix.columns[i],
                    'Variable 2': correlation_matrix.columns[j],
                    'Correlation': corr_value,
                    'Strength': 'Strong Positive' if corr_value > 0 else 'Strong Negative'
                })
    
    if strong_corr:
        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)
        print(f"\nFound {len(strong_corr)} strong correlations:\n")
        print(strong_corr_df.to_string(index=False))
    else:
        print("\nNo correlations with |r| > 0.7 found")
    
    # Moderate correlations (0.3 to 0.5)
    print("\n" + "="*80)
    print("MODERATE CORRELATIONS (0.3 < |r| ≤ 0.7)")
    print("="*80)
    
    moderate_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_value = correlation_matrix.iloc[i, j]
            if 0.3 < abs(corr_value) <= 0.7:
                moderate_corr.append({
                    'Variable 1': correlation_matrix.columns[i],
                    'Variable 2': correlation_matrix.columns[j],
                    'Correlation': corr_value,
                    'Strength': 'Moderate Positive' if corr_value > 0 else 'Moderate Negative'
                })
    
    if moderate_corr:
        moderate_corr_df = pd.DataFrame(moderate_corr).sort_values('Correlation', key=abs, ascending=False)
        print(f"\nFound {len(moderate_corr)} moderate correlations:\n")
        print(moderate_corr_df.to_string(index=False))
    else:
        print("\nNo moderate correlations found (0.3 < |r| ≤ 0.7)")
    
    # Summary statistics
    print("\n" + "="*80)
    print("SUMMARY STATISTICS")
    print("="*80)
    
    # Get correlation values (excluding diagonal)
    corr_values = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_values.append(correlation_matrix.iloc[i, j])
    
    corr_values = np.array(corr_values)
    print(f"\nMean correlation: {corr_values.mean():.3f}")
    print(f"Median correlation: {np.median(corr_values):.3f}")
    print(f"Std Dev: {corr_values.std():.3f}")
    print(f"Min: {corr_values.min():.3f}")
    print(f"Max: {corr_values.max():.3f}")
    
    # Distribution of correlations
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.hist(corr_values, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
    ax.axvline(corr_values.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {corr_values.mean():.3f}')
    ax.axvline(np.median(corr_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(corr_values):.3f}')
    ax.set_xlabel('Correlation Coefficient', fontsize=12, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax.set_title('Distribution of Correlation Coefficients', fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig('correlation_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
else:
    print("⚠️ Not enough numeric columns for correlation analysis")
    print(f"Found only {len(numeric_cols)} numeric column(s)")
```

    ================================================================================
    CORRELATION MATRIX ANALYSIS
    ================================================================================
    
    Numeric columns found: ['subject_id', 'hadm_id', 'total_microbio_events', 'unique_specimen_types', 'unique_test_names', 'unique_organisms', 'unique_antibiotics', 'unique_technicians', 'num_susceptible', 'num_resistant', 'num_intermediate', 'resistant_ratio', 'micro_qc_fail', 'micro_qc_warn', 'micro_qc_ok', 'micro_time_span_hours', 'has_micro']
    Total numeric columns: 17
    
    ================================================================================
    CORRELATION MATRIX
    ================================================================================
                           subject_id  hadm_id  total_microbio_events  \
    subject_id                  1.000   -0.011                 -0.086   
    hadm_id                    -0.011    1.000                 -0.001   
    total_microbio_events      -0.086   -0.001                  1.000   
    unique_specimen_types      -0.112   -0.003                  0.665   
    unique_test_names          -0.099    0.005                  0.670   
    unique_organisms           -0.010   -0.028                  0.665   
    unique_antibiotics         -0.052   -0.007                  0.643   
    unique_technicians         -0.093   -0.003                  0.991   
    num_susceptible            -0.055   -0.005                  0.760   
    num_resistant              -0.025    0.009                  0.508   
    num_intermediate           -0.007   -0.003                  0.374   
    resistant_ratio             0.045    0.003                  0.227   
    micro_qc_fail              -0.062   -0.019                  0.652   
    micro_qc_warn              -0.066    0.006                  0.767   
    micro_qc_ok                -0.085   -0.000                  0.993   
    micro_time_span_hours      -0.096    0.001                  0.654   
    has_micro                     NaN      NaN                    NaN   
    
                           unique_specimen_types  unique_test_names  \
    subject_id                            -0.112             -0.099   
    hadm_id                               -0.003              0.005   
    total_microbio_events                  0.665              0.670   
    unique_specimen_types                  1.000              0.891   
    unique_test_names                      0.891              1.000   
    unique_organisms                       0.331              0.299   
    unique_antibiotics                     0.171              0.125   
    unique_technicians                     0.685              0.693   
    num_susceptible                        0.224              0.179   
    num_resistant                          0.139              0.119   
    num_intermediate                       0.082              0.062   
    resistant_ratio                        0.008             -0.008   
    micro_qc_fail                          0.442              0.436   
    micro_qc_warn                          0.500              0.514   
    micro_qc_ok                            0.661              0.666   
    micro_time_span_hours                  0.632              0.568   
    has_micro                                NaN                NaN   
    
                           unique_organisms  unique_antibiotics  \
    subject_id                       -0.010              -0.052   
    hadm_id                          -0.028              -0.007   
    total_microbio_events             0.665               0.643   
    unique_specimen_types             0.331               0.171   
    unique_test_names                 0.299               0.125   
    unique_organisms                  1.000               0.672   
    unique_antibiotics                0.672               1.000   
    unique_technicians                0.667               0.666   
    num_susceptible                   0.667               0.876   
    num_resistant                     0.454               0.574   
    num_intermediate                  0.341               0.478   
    resistant_ratio                   0.342               0.419   
    micro_qc_fail                     0.438               0.419   
    micro_qc_warn                     0.500               0.494   
    micro_qc_ok                       0.661               0.638   
    micro_time_span_hours             0.407               0.253   
    has_micro                           NaN                 NaN   
    
                           unique_technicians  num_susceptible  num_resistant  \
    subject_id                         -0.093           -0.055         -0.025   
    hadm_id                            -0.003           -0.005          0.009   
    total_microbio_events               0.991            0.760          0.508   
    unique_specimen_types               0.685            0.224          0.139   
    unique_test_names                   0.693            0.179          0.119   
    unique_organisms                    0.667            0.667          0.454   
    unique_antibiotics                  0.666            0.876          0.574   
    unique_technicians                  1.000            0.744          0.511   
    num_susceptible                     0.744            1.000          0.444   
    num_resistant                       0.511            0.444          1.000   
    num_intermediate                    0.378            0.396          0.476   
    resistant_ratio                     0.239            0.243          0.619   
    micro_qc_fail                       0.651            0.514          0.339   
    micro_qc_warn                       0.766            0.575          0.407   
    micro_qc_ok                         0.983            0.754          0.501   
    micro_time_span_hours               0.656            0.334          0.251   
    has_micro                             NaN              NaN            NaN   
    
                           num_intermediate  resistant_ratio  micro_qc_fail  \
    subject_id                       -0.007            0.045         -0.062   
    hadm_id                          -0.003            0.003         -0.019   
    total_microbio_events             0.374            0.227          0.652   
    unique_specimen_types             0.082            0.008          0.442   
    unique_test_names                 0.062           -0.008          0.436   
    unique_organisms                  0.341            0.342          0.438   
    unique_antibiotics                0.478            0.419          0.419   
    unique_technicians                0.378            0.239          0.651   
    num_susceptible                   0.396            0.243          0.514   
    num_resistant                     0.476            0.619          0.339   
    num_intermediate                  1.000            0.274          0.264   
    resistant_ratio                   0.274            1.000          0.145   
    micro_qc_fail                     0.264            0.145          1.000   
    micro_qc_warn                     0.304            0.177          0.473   
    micro_qc_ok                       0.367            0.225          0.598   
    micro_time_span_hours             0.201            0.074          0.428   
    has_micro                           NaN              NaN            NaN   
    
                           micro_qc_warn  micro_qc_ok  micro_time_span_hours  \
    subject_id                    -0.066       -0.085                 -0.096   
    hadm_id                        0.006       -0.000                  0.001   
    total_microbio_events          0.767        0.993                  0.654   
    unique_specimen_types          0.500        0.661                  0.632   
    unique_test_names              0.514        0.666                  0.568   
    unique_organisms               0.500        0.661                  0.407   
    unique_antibiotics             0.494        0.638                  0.253   
    unique_technicians             0.766        0.983                  0.656   
    num_susceptible                0.575        0.754                  0.334   
    num_resistant                  0.407        0.501                  0.251   
    num_intermediate               0.304        0.367                  0.201   
    resistant_ratio                0.177        0.225                  0.074   
    micro_qc_fail                  0.473        0.598                  0.428   
    micro_qc_warn                  1.000        0.702                  0.475   
    micro_qc_ok                    0.702        1.000                  0.654   
    micro_time_span_hours          0.475        0.654                  1.000   
    has_micro                        NaN          NaN                    NaN   
    
                           has_micro  
    subject_id                   NaN  
    hadm_id                      NaN  
    total_microbio_events        NaN  
    unique_specimen_types        NaN  
    unique_test_names            NaN  
    unique_organisms             NaN  
    unique_antibiotics           NaN  
    unique_technicians           NaN  
    num_susceptible              NaN  
    num_resistant                NaN  
    num_intermediate             NaN  
    resistant_ratio              NaN  
    micro_qc_fail                NaN  
    micro_qc_warn                NaN  
    micro_qc_ok                  NaN  
    micro_time_span_hours        NaN  
    has_micro                    NaN  
    


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_71_1.png)
    


    
    ================================================================================
    STRONG CORRELATIONS (|r| > 0.5, excluding self-correlations)
    ================================================================================
    
    Found 11 strong correlations:
    
               Variable 1         Variable 2  Correlation        Strength
    total_microbio_events        micro_qc_ok     0.993120 Strong Positive
    total_microbio_events unique_technicians     0.990860 Strong Positive
       unique_technicians        micro_qc_ok     0.982787 Strong Positive
    unique_specimen_types  unique_test_names     0.890931 Strong Positive
       unique_antibiotics    num_susceptible     0.875749 Strong Positive
    total_microbio_events      micro_qc_warn     0.766980 Strong Positive
       unique_technicians      micro_qc_warn     0.765602 Strong Positive
    total_microbio_events    num_susceptible     0.760190 Strong Positive
          num_susceptible        micro_qc_ok     0.754477 Strong Positive
       unique_technicians    num_susceptible     0.744262 Strong Positive
            micro_qc_warn        micro_qc_ok     0.701914 Strong Positive
    
    ================================================================================
    MODERATE CORRELATIONS (0.3 < |r| ≤ 0.7)
    ================================================================================
    
    Found 57 moderate correlations:
    
               Variable 1            Variable 2  Correlation          Strength
        unique_test_names    unique_technicians     0.693374 Moderate Positive
    unique_specimen_types    unique_technicians     0.685212 Moderate Positive
         unique_organisms    unique_antibiotics     0.671795 Moderate Positive
    total_microbio_events     unique_test_names     0.670130 Moderate Positive
         unique_organisms       num_susceptible     0.667081 Moderate Positive
         unique_organisms    unique_technicians     0.666774 Moderate Positive
       unique_antibiotics    unique_technicians     0.666147 Moderate Positive
        unique_test_names           micro_qc_ok     0.665676 Moderate Positive
    total_microbio_events      unique_organisms     0.664818 Moderate Positive
    total_microbio_events unique_specimen_types     0.664577 Moderate Positive
         unique_organisms           micro_qc_ok     0.661287 Moderate Positive
    unique_specimen_types           micro_qc_ok     0.660657 Moderate Positive
       unique_technicians micro_time_span_hours     0.656065 Moderate Positive
    total_microbio_events micro_time_span_hours     0.654251 Moderate Positive
              micro_qc_ok micro_time_span_hours     0.653621 Moderate Positive
    total_microbio_events         micro_qc_fail     0.652276 Moderate Positive
       unique_technicians         micro_qc_fail     0.650617 Moderate Positive
    total_microbio_events    unique_antibiotics     0.642526 Moderate Positive
       unique_antibiotics           micro_qc_ok     0.637885 Moderate Positive
    unique_specimen_types micro_time_span_hours     0.632497 Moderate Positive
            num_resistant       resistant_ratio     0.619466 Moderate Positive
            micro_qc_fail           micro_qc_ok     0.598094 Moderate Positive
          num_susceptible         micro_qc_warn     0.574743 Moderate Positive
       unique_antibiotics         num_resistant     0.574001 Moderate Positive
        unique_test_names micro_time_span_hours     0.568092 Moderate Positive
          num_susceptible         micro_qc_fail     0.514431 Moderate Positive
        unique_test_names         micro_qc_warn     0.513816 Moderate Positive
       unique_technicians         num_resistant     0.510869 Moderate Positive
    total_microbio_events         num_resistant     0.508119 Moderate Positive
            num_resistant           micro_qc_ok     0.501390 Moderate Positive
         unique_organisms         micro_qc_warn     0.500237 Moderate Positive
    unique_specimen_types         micro_qc_warn     0.499930 Moderate Positive
       unique_antibiotics         micro_qc_warn     0.494362 Moderate Positive
       unique_antibiotics      num_intermediate     0.478422 Moderate Positive
            num_resistant      num_intermediate     0.476201 Moderate Positive
            micro_qc_warn micro_time_span_hours     0.475377 Moderate Positive
            micro_qc_fail         micro_qc_warn     0.472865 Moderate Positive
         unique_organisms         num_resistant     0.453723 Moderate Positive
          num_susceptible         num_resistant     0.444124 Moderate Positive
    unique_specimen_types         micro_qc_fail     0.442337 Moderate Positive
         unique_organisms         micro_qc_fail     0.438143 Moderate Positive
        unique_test_names         micro_qc_fail     0.435690 Moderate Positive
            micro_qc_fail micro_time_span_hours     0.428140 Moderate Positive
       unique_antibiotics       resistant_ratio     0.419353 Moderate Positive
       unique_antibiotics         micro_qc_fail     0.418935 Moderate Positive
         unique_organisms micro_time_span_hours     0.407368 Moderate Positive
            num_resistant         micro_qc_warn     0.406608 Moderate Positive
          num_susceptible      num_intermediate     0.396362 Moderate Positive
       unique_technicians      num_intermediate     0.378313 Moderate Positive
    total_microbio_events      num_intermediate     0.373937 Moderate Positive
         num_intermediate           micro_qc_ok     0.366916 Moderate Positive
         unique_organisms       resistant_ratio     0.342308 Moderate Positive
         unique_organisms      num_intermediate     0.340543 Moderate Positive
            num_resistant         micro_qc_fail     0.338638 Moderate Positive
          num_susceptible micro_time_span_hours     0.334270 Moderate Positive
    unique_specimen_types      unique_organisms     0.330739 Moderate Positive
         num_intermediate         micro_qc_warn     0.303523 Moderate Positive
    
    ================================================================================
    SUMMARY STATISTICS
    ================================================================================
    
    Mean correlation: nan
    Median correlation: nan
    Std Dev: nan
    Min: nan
    Max: nan
    


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_71_3.png)
    



```python
# Boxplots + Histograms for all numeric features
print("="*80)
print("BOXPLOTS & HISTOGRAMS FOR NUMERIC FEATURES")
print("="*80)

# Get numeric columns (exclude IDs)
numeric_cols = micro_features.select_dtypes(include=[np.number]).columns.tolist()
id_cols = ['subject_id', 'hadm_id']
numeric_cols = [col for col in numeric_cols if col not in id_cols]

print(f"\nAnalyzing {len(numeric_cols)} numeric features:\n{numeric_cols}\n")

# Create grid layout
n_cols = len(numeric_cols)
n_rows = n_cols
fig, axes = plt.subplots(n_rows, 2, figsize=(14, 4*n_rows))

# Ensure axes is 2D array
if n_rows == 1:
    axes = axes.reshape(1, -1)

for idx, col in enumerate(numeric_cols):
    data = micro_features[col].dropna()
    
    # Statistics
    stats_text = f"n={len(data):,} | μ={data.mean():.2f} | σ={data.std():.2f}\nmin={data.min():.2f} | Q2={data.median():.2f} | max={data.max():.2f}"
    
    # Boxplot (left)
    ax_box = axes[idx, 0]
    bp = ax_box.boxplot(data, vert=False, patch_artist=True, widths=0.6,
                         boxprops=dict(facecolor='lightblue', alpha=0.7),
                         medianprops=dict(color='red', linewidth=2),
                         flierprops=dict(marker='o', markerfacecolor='red', markersize=4, alpha=0.5))
    ax_box.set_xlabel('Value', fontsize=10, fontweight='bold')
    ax_box.set_title(f'{col.replace("_", " ").title()}', fontsize=11, fontweight='bold')
    ax_box.grid(axis='x', alpha=0.3, linestyle='--')
    ax_box.text(0.02, 0.98, stats_text, transform=ax_box.transAxes,
                fontsize=8, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    # Histogram (right)
    ax_hist = axes[idx, 1]
    n, bins, patches = ax_hist.hist(data, bins=30, edgecolor='black', 
                                     alpha=0.7, color='steelblue')
    ax_hist.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.2f}')
    ax_hist.axvline(data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {data.median():.2f}')
    ax_hist.set_xlabel('Value', fontsize=10, fontweight='bold')
    ax_hist.set_ylabel('Frequency', fontsize=10, fontweight='bold')
    ax_hist.set_title(f'Distribution of {col.replace("_", " ").title()}', fontsize=11, fontweight='bold')
    ax_hist.legend(fontsize=9)
    ax_hist.grid(axis='y', alpha=0.3, linestyle='--')

plt.tight_layout()
plt.savefig('micro_features_boxplots_histograms.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("SUMMARY STATISTICS TABLE")
print("="*80)

# Create summary table
summary_stats = []
for col in numeric_cols:
    data = micro_features[col].dropna()
    summary_stats.append({
        'Feature': col,
        'Count': len(data),
        'Mean': f"{data.mean():.2f}",
        'Std': f"{data.std():.2f}",
        'Min': f"{data.min():.2f}",
        'Q1': f"{data.quantile(0.25):.2f}",
        'Median': f"{data.median():.2f}",
        'Q3': f"{data.quantile(0.75):.2f}",
        'Max': f"{data.max():.2f}",
        'Skew': f"{data.skew():.2f}"
    })

summary_df = pd.DataFrame(summary_stats)
print(summary_df.to_string(index=False))

```

    ================================================================================
    BOXPLOTS & HISTOGRAMS FOR NUMERIC FEATURES
    ================================================================================
    
    Analyzing 15 numeric features:
    ['total_microbio_events', 'unique_specimen_types', 'unique_test_names', 'unique_organisms', 'unique_antibiotics', 'unique_technicians', 'num_susceptible', 'num_resistant', 'num_intermediate', 'resistant_ratio', 'micro_qc_fail', 'micro_qc_warn', 'micro_qc_ok', 'micro_time_span_hours', 'has_micro']
    
    


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_72_1.png)
    


    
    ================================================================================
    SUMMARY STATISTICS TABLE
    ================================================================================
                  Feature  Count  Mean   Std  Min   Q1 Median    Q3     Max Skew
    total_microbio_events   2756  5.66  8.39 1.00 1.00   2.50  7.00  122.00 4.49
    unique_specimen_types   2756  2.03  1.43 1.00 1.00   2.00  3.00   15.00 2.08
        unique_test_names   2756  2.70  2.63 1.00 1.00   2.00  3.00   23.00 2.49
         unique_organisms   2756  0.36  0.72 0.00 0.00   0.00  1.00    9.00 3.13
       unique_antibiotics   2756  1.43  3.74 0.00 0.00   0.00  0.00   23.00 2.71
       unique_technicians   2756  5.30  7.10 1.00 1.00   2.00  7.00   82.00 3.30
          num_susceptible   2756  1.38  4.25 0.00 0.00   0.00  0.00   68.00 5.17
            num_resistant   2756  0.31  1.48 0.00 0.00   0.00  0.00   25.00 8.32
         num_intermediate   2756  0.06  0.34 0.00 0.00   0.00  0.00    5.00 7.79
          resistant_ratio   2756  0.02  0.10 0.00 0.00   0.00  0.00    1.00 6.66
            micro_qc_fail   2756  0.28  0.67 0.00 0.00   0.00  0.00    7.00 3.44
            micro_qc_warn   2756  0.57  1.09 0.00 0.00   0.00  1.00   12.00 3.33
              micro_qc_ok   2756  4.80  7.17 0.00 1.00   2.00  6.00  107.00 4.54
    micro_time_span_hours   2756 48.49 95.44 0.00 0.00   2.08 56.34 1085.98 3.49
                has_micro   2756  1.00  0.00 1.00 1.00   1.00  1.00    1.00 0.00
    

### Dropunused


```python
micro_features.columns
```




    Index(['subject_id', 'hadm_id', 'total_microbio_events',
           'unique_specimen_types', 'unique_test_names', 'unique_organisms',
           'unique_antibiotics', 'unique_technicians', 'num_susceptible',
           'num_resistant', 'num_intermediate', 'resistant_ratio', 'micro_qc_fail',
           'micro_qc_warn', 'micro_qc_ok', 'micro_time_span_hours', 'has_micro'],
          dtype='object')




```python
micro_features_small6 = [
    'subject_id',
    'hadm_id',
    'has_micro',
    'total_microbio_events',   # overall microbiology activity (dominant signal)
    'unique_specimen_types',   # sampling diversity
    'unique_organisms',        # biological diversity
    'resistant_ratio',         # susceptibility profile summary
    'unique_antibiotics',      # captures treatment relevance independent of organisms
   # 'unique_test_names'        # captures test diversity (operational dimension)
]
features_to_keep = [col for col in micro_features_small6 if col in micro_features.columns]
feat_reduced = micro_features[features_to_keep].copy()
```

### Corr


```python
print("="*80)
print("CORRELATION MATRIX ANALYSIS")
print("="*80)

# Select numeric columns only
numeric_cols = feat_reduced.select_dtypes(include=[np.number]).columns.tolist()
print(f"\nNumeric columns found: {numeric_cols}")
print(f"Total numeric columns: {len(numeric_cols)}\n")  

if len(numeric_cols) > 1:
    # Calculate correlation matrix
    correlation_matrix = feat_reduced[numeric_cols].corr()
    
    print("="*80)
    print("CORRELATION MATRIX")
    print("="*80)
    print(correlation_matrix.round(3))
    
    # Create visualization
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    
    # Plot 1: Full correlation heatmap
    ax1 = axes[0]
    sns.heatmap(correlation_matrix, 
                annot=True, 
                fmt='.2f', 
                cmap='coolwarm', 
                center=0,
                square=True,
                linewidths=0.5,
                cbar_kws={'label': 'Correlation Coefficient'},
                ax=ax1)
    ax1.set_title('Full Correlation Matrix Heatmap', fontsize=14, fontweight='bold')
    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
    plt.setp(ax1.get_yticklabels(), rotation=0)
    
    # Plot 2: Mask for upper triangle (cleaner view)
    ax2 = axes[1]
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, 
                annot=True, 
                fmt='.2f', 
                cmap='coolwarm', 
                center=0,
                square=True,
                linewidths=0.5,
                mask=mask,
                cbar_kws={'label': 'Correlation Coefficient'},
                ax=ax2)
    ax2.set_title('Lower Triangle Correlation Matrix (Unique Pairs)', fontsize=14, fontweight='bold')
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
    plt.setp(ax2.get_yticklabels(), rotation=0)
    
    plt.tight_layout()
    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Extract strong correlations (> 0.5 or < -0.5, excluding diagonal)
    print("\n" + "="*80)
    print("STRONG CORRELATIONS (|r| > 0.5, excluding self-correlations)")
    print("="*80)
    
    strong_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_value = correlation_matrix.iloc[i, j]
            if abs(corr_value) > 0.5:
                strong_corr.append({
                    'Variable 1': correlation_matrix.columns[i],
                    'Variable 2': correlation_matrix.columns[j],
                    'Correlation': corr_value,
                    'Strength': 'Strong Positive' if corr_value > 0 else 'Strong Negative'
                })
    
    if strong_corr:
        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)
        print(f"\nFound {len(strong_corr)} strong correlations:\n")
        print(strong_corr_df.to_string(index=False))
    else:
        print("\nNo correlations with |r| > 0.5 found")
    
    # Moderate correlations (0.3 to 0.5)
    print("\n" + "="*80)
    print("MODERATE CORRELATIONS (0.3 < |r| ≤ 0.5)")
    print("="*80)
    
    moderate_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_value = correlation_matrix.iloc[i, j]
            if 0.3 < abs(corr_value) <= 0.5:
                moderate_corr.append({
                    'Variable 1': correlation_matrix.columns[i],
                    'Variable 2': correlation_matrix.columns[j],
                    'Correlation': corr_value,
                    'Strength': 'Moderate Positive' if corr_value > 0 else 'Moderate Negative'
                })
    
    if moderate_corr:
        moderate_corr_df = pd.DataFrame(moderate_corr).sort_values('Correlation', key=abs, ascending=False)
        print(f"\nFound {len(moderate_corr)} moderate correlations:\n")
        print(moderate_corr_df.to_string(index=False))
    else:
        print("\nNo moderate correlations found (0.3 < |r| ≤ 0.5)")
    
    # Summary statistics
    print("\n" + "="*80)
    print("SUMMARY STATISTICS")
    print("="*80)
    
    # Get correlation values (excluding diagonal)
    corr_values = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_values.append(correlation_matrix.iloc[i, j])
    
    corr_values = np.array(corr_values)
    print(f"\nMean correlation: {corr_values.mean():.3f}")
    print(f"Median correlation: {np.median(corr_values):.3f}")
    print(f"Std Dev: {corr_values.std():.3f}")
    print(f"Min: {corr_values.min():.3f}")
    print(f"Max: {corr_values.max():.3f}")
    
    # Distribution of correlations
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.hist(corr_values, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
    ax.axvline(corr_values.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {corr_values.mean():.3f}')
    ax.axvline(np.median(corr_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(corr_values):.3f}')
    ax.set_xlabel('Correlation Coefficient', fontsize=12, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax.set_title('Distribution of Correlation Coefficients', fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig('correlation_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
else:
    print("⚠️ Not enough numeric columns for correlation analysis")
    print(f"Found only {len(numeric_cols)} numeric column(s)")
```

    ================================================================================
    CORRELATION MATRIX ANALYSIS
    ================================================================================
    
    Numeric columns found: ['subject_id', 'hadm_id', 'has_micro', 'total_microbio_events', 'unique_specimen_types', 'unique_organisms', 'resistant_ratio', 'unique_antibiotics']
    Total numeric columns: 8
    
    ================================================================================
    CORRELATION MATRIX
    ================================================================================
                           subject_id  hadm_id  has_micro  total_microbio_events  \
    subject_id                  1.000   -0.011        NaN                 -0.086   
    hadm_id                    -0.011    1.000        NaN                 -0.001   
    has_micro                     NaN      NaN        NaN                    NaN   
    total_microbio_events      -0.086   -0.001        NaN                  1.000   
    unique_specimen_types      -0.112   -0.003        NaN                  0.665   
    unique_organisms           -0.010   -0.028        NaN                  0.665   
    resistant_ratio             0.045    0.003        NaN                  0.227   
    unique_antibiotics         -0.052   -0.007        NaN                  0.643   
    
                           unique_specimen_types  unique_organisms  \
    subject_id                            -0.112            -0.010   
    hadm_id                               -0.003            -0.028   
    has_micro                                NaN               NaN   
    total_microbio_events                  0.665             0.665   
    unique_specimen_types                  1.000             0.331   
    unique_organisms                       0.331             1.000   
    resistant_ratio                        0.008             0.342   
    unique_antibiotics                     0.171             0.672   
    
                           resistant_ratio  unique_antibiotics  
    subject_id                       0.045              -0.052  
    hadm_id                          0.003              -0.007  
    has_micro                          NaN                 NaN  
    total_microbio_events            0.227               0.643  
    unique_specimen_types            0.008               0.171  
    unique_organisms                 0.342               0.672  
    resistant_ratio                  1.000               0.419  
    unique_antibiotics               0.419               1.000  
    


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_77_1.png)
    


    
    ================================================================================
    STRONG CORRELATIONS (|r| > 0.5, excluding self-correlations)
    ================================================================================
    
    Found 4 strong correlations:
    
               Variable 1            Variable 2  Correlation        Strength
         unique_organisms    unique_antibiotics     0.671795 Strong Positive
    total_microbio_events      unique_organisms     0.664818 Strong Positive
    total_microbio_events unique_specimen_types     0.664577 Strong Positive
    total_microbio_events    unique_antibiotics     0.642526 Strong Positive
    
    ================================================================================
    MODERATE CORRELATIONS (0.3 < |r| ≤ 0.5)
    ================================================================================
    
    Found 3 moderate correlations:
    
               Variable 1         Variable 2  Correlation          Strength
          resistant_ratio unique_antibiotics     0.419353 Moderate Positive
         unique_organisms    resistant_ratio     0.342308 Moderate Positive
    unique_specimen_types   unique_organisms     0.330739 Moderate Positive
    
    ================================================================================
    SUMMARY STATISTICS
    ================================================================================
    
    Mean correlation: nan
    Median correlation: nan
    Std Dev: nan
    Min: nan
    Max: nan
    


    
![png](1.1%20df3%20microbiology_files/1.1%20df3%20microbiology_77_3.png)
    


## Save SLim


```python
feat_reduced.to_csv(f"{DATA_DIR}/{DATASETS[name].replace('.csv', '_agg_features.csv')}", index=False)
```


```python

```


```python

```
