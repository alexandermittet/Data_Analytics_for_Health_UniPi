# Data Analytics for Health - Task 2.1: K-means Clustering Analysis

## Overview
This notebook performs K-means clustering analysis on the prepared patient profile:
- 2.1.1: Identification of the best value of k
- 2.1.2: Characterization of the obtained clusters (centroid analysis and distribution comparison)
- 2.1.3: Evaluation of the clustering results

## Objectives
- Determine optimal number of clusters using elbow method, silhouette analysis, etc.
- Analyze cluster characteristics through centroids and variable distributions
- Evaluate clustering quality using appropriate metrics

---



```python
#%pip install umap-learn
```


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA

warnings.filterwarnings('ignore')
plt.style.use('default')
sns.set_palette("husl")

# Set up file paths
notebook_dir = Path.cwd().resolve()
data_path = (notebook_dir / '..' /  'Data').resolve()
plots_dir = (notebook_dir / '..' / 'plots').resolve()
plots_dir.mkdir(parents=True, exist_ok=True)

print("Libraries imported successfully")
print(f"Data path: {data_path}")
print(f"Plots directory: {plots_dir}")

# Quick listing
csv_files = sorted(data_path.glob('*.csv'))
for f in csv_files:
    print(f.name)
```

    Libraries imported successfully
    Data path: Y:\Studium\3. Sem UniPI\Data Analytics 4 digital Health\data_analytics_4_health_unipi\Data
    Plots directory: Y:\Studium\3. Sem UniPI\Data Analytics 4 digital Health\data_analytics_4_health_unipi\plots
    1.1.2_merged_dataset_option_a_subject_id.csv
    1.1.2_merged_dataset_option_b_subject_hadm_id.csv
    1.2.1_final_patient_profile.csv
    1.2.1_prepared_patient_profile.csv
    1.2.2_prepared_patient_profile.csv
    1.2.3_final_patient_profile.csv
    1.2_admission_time_features.csv
    1.2_subject_time_features.csv
    2.1_kmeans_clustered_data.csv
    2.2_dbscan_clustered_data.csv
    2.3_hierarchical_clustered_data.csv
    3.1_time_series_features.csv
    3.1_time_series_paa_summary.csv
    heart_diagnoses_1.csv
    microbiology_events_codes_3.csv
    patient_profile_broad.csv
    patient_profile_broad_clean.csv
    patient_profile_broad_clean_classification.csv
    patient_profile_broad_clean_clustering.csv
    patient_profile_clustering_biological.csv
    patient_profile_final_large.csv
    patient_profile_large_uncorr.csv
    patient_profile_master_large.csv
    patient_profile_small.csv
    procedure_code_4.csv
    

## 1. Load Prepared Patient Profile



```python
# Load prepared patient profile 
df = pd.read_csv(data_path / 'patient_profile_broad_clean_clustering.csv')

print(f"Loaded patient profile: {df.shape[0]:,} subjects × {df.shape[1]} features")
print(f"\nColumns: {df.columns.tolist()}")

# Get numeric features (excluding subject_id and categorical variables)
numeric_features = [col for col in df.columns if col != 'subject_id' and col != 'hadm_id' and df[col].dtype in [np.float64, np.int64]]
print(f"\nNumeric features for clustering: {len(numeric_features)}")
print(f"Features: {numeric_features}")
```

    Loaded patient profile: 5,166 subjects × 13 features
    
    Columns: ['subject_id', 'hadm_id', 'abnormal_ratio', 'qc_fail_ratio', 'fluid_diversity', 'procedure_span_days_missing', 'gender_F', 'micro_resistance_score', 'metabolic_stress_index', 'oxygenation_dysfunction_index', 'inflammation_liver_stress_index', 'hematologic_stability_score', 'renal_failure_index']
    
    Numeric features for clustering: 11
    Features: ['abnormal_ratio', 'qc_fail_ratio', 'fluid_diversity', 'procedure_span_days_missing', 'gender_F', 'micro_resistance_score', 'metabolic_stress_index', 'oxygenation_dysfunction_index', 'inflammation_liver_stress_index', 'hematologic_stability_score', 'renal_failure_index']
    


```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 5166 entries, 0 to 5165
    Data columns (total 13 columns):
     #   Column                           Non-Null Count  Dtype  
    ---  ------                           --------------  -----  
     0   subject_id                       5166 non-null   int64  
     1   hadm_id                          5166 non-null   int64  
     2   abnormal_ratio                   5166 non-null   float64
     3   qc_fail_ratio                    5166 non-null   float64
     4   fluid_diversity                  5166 non-null   float64
     5   procedure_span_days_missing      5166 non-null   int64  
     6   gender_F                         5166 non-null   int64  
     7   micro_resistance_score           5166 non-null   float64
     8   metabolic_stress_index           5166 non-null   float64
     9   oxygenation_dysfunction_index    5166 non-null   float64
     10  inflammation_liver_stress_index  5166 non-null   float64
     11  hematologic_stability_score      5166 non-null   float64
     12  renal_failure_index              5166 non-null   float64
    dtypes: float64(9), int64(4)
    memory usage: 524.8 KB
    


```python
df.describe().T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>subject_id</th>
      <td>5166.0</td>
      <td>1.539306e+07</td>
      <td>3.073751e+06</td>
      <td>1.000098e+07</td>
      <td>1.267767e+07</td>
      <td>1.539807e+07</td>
      <td>1.812905e+07</td>
      <td>1.999860e+07</td>
    </tr>
    <tr>
      <th>hadm_id</th>
      <td>5166.0</td>
      <td>2.501832e+07</td>
      <td>2.871364e+06</td>
      <td>2.000446e+07</td>
      <td>2.261127e+07</td>
      <td>2.502269e+07</td>
      <td>2.746995e+07</td>
      <td>2.999967e+07</td>
    </tr>
    <tr>
      <th>abnormal_ratio</th>
      <td>5166.0</td>
      <td>8.124055e-05</td>
      <td>7.761210e-04</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>1.980198e-02</td>
    </tr>
    <tr>
      <th>qc_fail_ratio</th>
      <td>5166.0</td>
      <td>1.894092e-02</td>
      <td>1.493210e-02</td>
      <td>0.000000e+00</td>
      <td>9.433962e-03</td>
      <td>1.812142e-02</td>
      <td>2.648131e-02</td>
      <td>2.857143e-01</td>
    </tr>
    <tr>
      <th>fluid_diversity</th>
      <td>5166.0</td>
      <td>8.535274e-01</td>
      <td>3.143714e-01</td>
      <td>0.000000e+00</td>
      <td>6.931472e-01</td>
      <td>6.931472e-01</td>
      <td>1.098612e+00</td>
      <td>1.791759e+00</td>
    </tr>
    <tr>
      <th>procedure_span_days_missing</th>
      <td>5166.0</td>
      <td>3.304297e-01</td>
      <td>4.704134e-01</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>gender_F</th>
      <td>5166.0</td>
      <td>3.904375e-01</td>
      <td>4.878956e-01</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>micro_resistance_score</th>
      <td>5166.0</td>
      <td>1.371272e-02</td>
      <td>7.233400e-02</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>7.412763e-01</td>
    </tr>
    <tr>
      <th>metabolic_stress_index</th>
      <td>5166.0</td>
      <td>5.366816e-01</td>
      <td>2.800145e+00</td>
      <td>-3.656627e+00</td>
      <td>-6.544193e-01</td>
      <td>0.000000e+00</td>
      <td>1.069571e+00</td>
      <td>1.019930e+02</td>
    </tr>
    <tr>
      <th>oxygenation_dysfunction_index</th>
      <td>5166.0</td>
      <td>-4.467932e-02</td>
      <td>1.320977e+00</td>
      <td>-4.405457e+01</td>
      <td>-3.108434e-01</td>
      <td>0.000000e+00</td>
      <td>7.038894e-01</td>
      <td>5.850936e+00</td>
    </tr>
    <tr>
      <th>inflammation_liver_stress_index</th>
      <td>5166.0</td>
      <td>9.187544e-03</td>
      <td>1.591256e-01</td>
      <td>-5.371486e-01</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>2.420683e+00</td>
    </tr>
    <tr>
      <th>hematologic_stability_score</th>
      <td>5166.0</td>
      <td>-2.867563e-01</td>
      <td>1.324859e+00</td>
      <td>-5.638587e+00</td>
      <td>-1.258152e+00</td>
      <td>-1.195652e-01</td>
      <td>6.526268e-01</td>
      <td>4.131341e+00</td>
    </tr>
    <tr>
      <th>renal_failure_index</th>
      <td>5166.0</td>
      <td>5.617604e-01</td>
      <td>2.323787e+00</td>
      <td>-5.242988e+01</td>
      <td>-2.500000e-01</td>
      <td>0.000000e+00</td>
      <td>6.250000e-01</td>
      <td>2.125000e+01</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Get numeric features (excluding subject_id and categorical variables)
numeric_features = [col for col in df.columns if col != 'subject_id' and col != 'hadm_id' and df[col].dtype in [np.float64, np.int64]]
print(f"\nNumeric features for clustering: {len(numeric_features)}")
print(f"Features: {numeric_features}")
```

    
    Numeric features for clustering: 11
    Features: ['abnormal_ratio', 'qc_fail_ratio', 'fluid_diversity', 'procedure_span_days_missing', 'gender_F', 'micro_resistance_score', 'metabolic_stress_index', 'oxygenation_dysfunction_index', 'inflammation_liver_stress_index', 'hematologic_stability_score', 'renal_failure_index']
    


```python
X = df[numeric_features].copy()
X.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 5166 entries, 0 to 5165
    Data columns (total 11 columns):
     #   Column                           Non-Null Count  Dtype  
    ---  ------                           --------------  -----  
     0   abnormal_ratio                   5166 non-null   float64
     1   qc_fail_ratio                    5166 non-null   float64
     2   fluid_diversity                  5166 non-null   float64
     3   procedure_span_days_missing      5166 non-null   int64  
     4   gender_F                         5166 non-null   int64  
     5   micro_resistance_score           5166 non-null   float64
     6   metabolic_stress_index           5166 non-null   float64
     7   oxygenation_dysfunction_index    5166 non-null   float64
     8   inflammation_liver_stress_index  5166 non-null   float64
     9   hematologic_stability_score      5166 non-null   float64
     10  renal_failure_index              5166 non-null   float64
    dtypes: float64(9), int64(2)
    memory usage: 444.1 KB
    


```python
# Create composite unique record id (subject_id + hadm_id) for mapping back results
df['record_id'] = df['subject_id'].astype(str) + '_' + df['hadm_id'].astype(str)
record_ids = df['record_id'].copy()

# Handle any remaining NaN values: NOTE is_Dead, abnormal_rat, qc_fail_ratio have some NaNs
print(f"\nNaN values before handling: {X.isna().sum().sum()}")
X = X.fillna(X.mean())  # Fill with mean or use appropriate strategy
print(f"NaN values after handling: {X.isna().sum().sum()}")

# Standardize features
#scaler = StandardScaler()  
scaler = RobustScaler()  
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=numeric_features, index=X.index)

print(f"\nData prepared for clustering: {X_scaled_df.shape}")
```

    
    NaN values before handling: 0
    NaN values after handling: 0
    
    Data prepared for clustering: (5166, 11)
    

## 2.1.1 Identification of the Best Value of k



```python
# Test different values of k
k_range = range(2, 11)  # Test k from 2 to 10
inertias = []
silhouette_scores = []
davies_bouldin_scores = []
calinski_harabasz_scores = []

print("Testing different values of k...")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, labels))
    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels))
    calinski_harabasz_scores.append(calinski_harabasz_score(X_scaled, labels))
    
    print(f"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}, "
          f"DB={davies_bouldin_scores[-1]:.3f}, CH={calinski_harabasz_scores[-1]:.2f}")

# Store results
k_evaluation = pd.DataFrame({
    'k': list(k_range),
    'inertia': inertias,
    'silhouette': silhouette_scores,
    'davies_bouldin': davies_bouldin_scores,
    'calinski_harabasz': calinski_harabasz_scores
})

```

    Testing different values of k...
    k=2: Inertia=50578.77, Silhouette=0.635, DB=0.839, CH=2073.61
    k=3: Inertia=45471.87, Silhouette=0.258, DB=1.465, CH=1442.95
    k=4: Inertia=38011.57, Silhouette=0.631, DB=0.681, CH=1488.25
    k=5: Inertia=33105.19, Silhouette=0.257, DB=1.121, CH=1472.59
    k=6: Inertia=29844.75, Silhouette=0.264, DB=1.005, CH=1419.27
    k=7: Inertia=26999.76, Silhouette=0.198, DB=1.114, CH=1397.69
    k=8: Inertia=25717.95, Silhouette=0.131, DB=1.333, CH=1294.21
    k=9: Inertia=23297.02, Silhouette=0.171, DB=1.178, CH=1316.86
    k=10: Inertia=21509.09, Silhouette=0.171, DB=1.107, CH=1315.26
    

### Understanding Silhouette Plots

**What is a Silhouette Plot?**
A silhouette plot visualizes how well each data point fits into its assigned cluster. The silhouette coefficient ranges from **-1 to +1**:
- **+1**: Point is well-matched to its cluster and poorly matched to neighboring clusters (ideal)
- **0**: Point is on the boundary between two clusters (ambiguous)
- **-1**: Point is poorly matched to its cluster and better matched to a neighboring cluster (misclassified)

**What to Look For:**

1. **Average Silhouette Score (Red Dashed Line)**
   - Higher is better (closer to +1)
   - Values > 0.5 indicate good clustering structure
   - Values 0.3-0.5 indicate fair clustering
   - Values < 0.3 suggest poor clustering

2. **Cluster Width and Shape**
   - **Wide bars**: Most points in the cluster have high silhouette scores (good cohesion)
   - **Narrow bars**: Points have varying silhouette scores (less cohesive)
   - **Uniform width**: All clusters are similarly well-defined

3. **Cluster Heights**
   - Height represents the number of points in each cluster
   - Similar heights indicate balanced cluster sizes
   - Very different heights may indicate imbalanced clusters

4. **Negative Values**
   - Points with negative silhouette scores are closer to other clusters than their own
   - Many negative values suggest poor cluster separation
   - Ideally, all bars should be to the right of 0

5. **Comparing Different k Values**
   - Choose the k with the highest average silhouette score
   - Prefer k values where all clusters have mostly positive scores
   - Avoid k values with many negative scores or very low averages



```python
# Visualize elbow method and evaluation metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Elbow method
axes[0, 0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0, 0].set_xlabel('Number of clusters (k)', fontsize=12)
axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)
axes[0, 0].set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# Silhouette score
axes[0, 1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
axes[0, 1].set_xlabel('Number of clusters (k)', fontsize=12)
axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)
axes[0, 1].set_title('Silhouette Score vs k', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)
best_k_sil = k_range[np.argmax(silhouette_scores)]
axes[0, 1].axvline(x=best_k_sil, color='r', linestyle='--', label=f'Best k={best_k_sil}')
axes[0, 1].legend()

# Davies-Bouldin score (lower is better)
axes[1, 0].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)
axes[1, 0].set_xlabel('Number of clusters (k)', fontsize=12)
axes[1, 0].set_ylabel('Davies-Bouldin Score', fontsize=12)
axes[1, 0].set_title('Davies-Bouldin Score vs k (lower is better)', fontsize=14, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)
best_k_db = k_range[np.argmin(davies_bouldin_scores)]
axes[1, 0].axvline(x=best_k_db, color='g', linestyle='--', label=f'Best k={best_k_db}')
axes[1, 0].legend()

# Calinski-Harabasz score (higher is better)
axes[1, 1].plot(k_range, calinski_harabasz_scores, 'mo-', linewidth=2, markersize=8)
axes[1, 1].set_xlabel('Number of clusters (k)', fontsize=12)
axes[1, 1].set_ylabel('Calinski-Harabasz Score', fontsize=12)
axes[1, 1].set_title('Calinski-Harabasz Score vs k (higher is better)', fontsize=14, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)
best_k_ch = k_range[np.argmax(calinski_harabasz_scores)]
axes[1, 1].axvline(x=best_k_ch, color='b', linestyle='--', label=f'Best k={best_k_ch}')
axes[1, 1].legend()

plt.tight_layout()
plt.savefig(plots_dir / '2.1.1_k_selection_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nBest k based on Silhouette Score: {best_k_sil}")
print(f"Best k based on Davies-Bouldin Score: {best_k_db}")
print(f"Best k based on Calinski-Harabasz Score: {best_k_ch}")

```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_13_0.png)
    


    
    Best k based on Silhouette Score: 2
    Best k based on Davies-Bouldin Score: 4
    Best k based on Calinski-Harabasz Score: 2
    


```python
# We choose k = 4
```


```python
# Silhouette analysis for selected k values (fixed to handle single-axis and typing issues)
selected_k_values = [best_k_sil, 3, 4]
if len(set(selected_k_values)) < len(selected_k_values):
    selected_k_values = list(set(selected_k_values))

# Defensive: ensure we have at least one k
if len(selected_k_values) == 0:
    selected_k_values = [best_k_sil]

fig, axes = plt.subplots(1, len(selected_k_values), figsize=(6 * len(selected_k_values), 5))
# Normalize axes to a list so axes[idx] always works
if isinstance(axes, np.ndarray):
    axes_list = axes.flatten().tolist()
else:
    axes_list = [axes]

for idx, k in enumerate(selected_k_values):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    labels = np.asarray(labels)  # ensure numpy array

    silhouette_avg = float(silhouette_score(X_scaled, labels))
    sample_silhouette_values = np.asarray(silhouette_samples(X_scaled, labels))

    y_lower = 10
    n_samples = X_scaled.shape[0]
    cmap = plt.cm.get_cmap('nipy_spectral')  # robust colormap access

    ax = axes_list[idx]
    for i in range(k):
        mask = (labels == i)
        ith_cluster_silhouette_values = sample_silhouette_values[mask]
        if ith_cluster_silhouette_values.size == 0:
            continue

        ith_sorted = np.sort(ith_cluster_silhouette_values)
        size_cluster_i = ith_sorted.size
        y_upper = y_lower + size_cluster_i

        color = cmap(float(i) / k)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_sorted,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax.axvline(x=silhouette_avg, color="red", linestyle="--",
               label=f'Avg Silhouette: {silhouette_avg:.3f}')
    ax.set_xlabel('Silhouette Coefficient Values', fontsize=11)
    ax.set_ylabel('Cluster Label', fontsize=11)
    ax.set_title(f'Silhouette Plot for k={k}', fontsize=12, fontweight='bold')
    ax.set_ylim([0, n_samples + (k + 1) * 10])
    ax.legend()

#plt.tight_layout()
plt.savefig(plots_dir / '2.1.1_silhouette_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_15_0.png)
    


The silhouette coefficient ranges from **-1 to +1**:
- **+1**: Point is well-matched to its cluster and poorly matched to neighboring clusters (ideal)
- **0**: Point is on the boundary between two clusters (ambiguous)
- **-1**: Point is poorly matched to its cluster and better matched to a neighboring cluster (misclassified)

**What to Look For:**

1. **Average Silhouette Score (Red Dashed Line)**
   - Higher is better (closer to +1)
   - Values > 0.5 indicate good clustering structure
   - Values 0.3-0.5 indicate fair clustering
   - Values < 0.3 suggest poor clustering

2. **Cluster Width and Shape**
   - **Wide bars**: Most points in the cluster have high silhouette scores (good cohesion)
   - **Narrow bars**: Points have varying silhouette scores (less cohesive)
   - **Uniform width**: All clusters are similarly well-defined

3. **Cluster Heights**
   - Height represents the number of points in each cluster
   - Similar heights indicate balanced cluster sizes
   - Very different heights may indicate imbalanced clusters

4. **Negative Values**
   - Points with negative silhouette scores are closer to other clusters than their own
   - Many negative values suggest poor cluster separation
   - Ideally, all bars should be to the right of 0

5. **Comparing Different k Values**
   - Choose the k with the highest average silhouette score
   - Prefer k values where all clusters have mostly positive scores
   - Avoid k values with many negative scores or very low averages


## 2.1.2 Characterization of the Obtained Clusters

### Select optimal k and perform final clustering



```python
selected_k_values
```




    [2, 3, 6]




```python
optimal_k = best_k_db

# TODO
# Manually override to try different Ks
optimal_k = 2
# TODO

print(f"Selected optimal k: {optimal_k}")
print(f"Note: Chosen for patient profiling purposes (more nuanced patient stereotypes)")
print(f"  - Metrics-based best k (silhouette): {best_k_sil}")
print(f"  - Metrics-based best k (Davies-Bouldin): {best_k_db}")
print(f"  - Metrics-based best k (Calinski-Harabasz): {best_k_ch}")

# Perform final clustering
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans_final.fit_predict(X_scaled)

# Add cluster labels to original dataframe
df_clustered = df.copy()
df_clustered['cluster'] = cluster_labels

print(f"\nCluster distribution:")
print(df_clustered['cluster'].value_counts().sort_index())

# Save clustered data
df_clustered.to_csv(data_path / '2.1_kmeans_clustered_data.csv', index=False)
print(f"\nClustered data saved to: {data_path / '2.1_kmeans_clustered_data.csv'}")

```

    Selected optimal k: 2
    Note: Chosen for patient profiling purposes (more nuanced patient stereotypes)
      - Metrics-based best k (silhouette): 2
      - Metrics-based best k (Davies-Bouldin): 4
      - Metrics-based best k (Calinski-Harabasz): 2
    
    Cluster distribution:
    cluster
    0     347
    1    4819
    Name: count, dtype: int64
    
    Clustered data saved to: Y:\Studium\3. Sem UniPI\Data Analytics 4 digital Health\data_analytics_4_health_unipi\Data\2.1_kmeans_clustered_data.csv
    

### How to Interpret the Cluster Centroids Comparison Plot

**What are Centroids?**
- Centroids represent the "average" or "typical" patient profile for each cluster
- Each centroid value shows the mean feature value for all patients in that cluster
- Values are in the **original scale** (not standardized), so they're interpretable in real-world units

**Reading the Plot:**

1. **X-axis (Features)**: Each bar position represents a different feature (e.g., n_lab_events, age, max_glucose)

2. **Y-axis (Feature Values)**: The height of each bar shows the centroid value for that feature
   - Higher bars = cluster has higher average values for that feature
   - Lower bars = cluster has lower average values for that feature
   - Bars near zero = cluster has values close to the overall dataset mean

3. **Colors (Clusters)**: Each color represents a different cluster
   - Compare how different clusters differ across features
   - Look for patterns: which clusters are similar? Which are distinct?

**What to Look For:**

1. **Distinctive Features per Cluster**
   - Find features where one cluster's bar is much higher/lower than others
   - These features help characterize and distinguish that cluster
   - Example: If Cluster 6 has a very high bar for "max_glucose", it represents patients with many diagnoses

2. **Cluster Similarities**
   - Clusters with similar bar patterns across features are similar patient types
   - Clusters with very different patterns represent distinct patient profiles

3. **Feature Patterns**
   - Look for features where clusters show clear separation (good for distinguishing clusters)
   - Features where all clusters are similar indicate less discriminatory power

4. **Clinical Interpretation**
   - Translate centroid values into patient characteristics:
     - High max_glucose → diabetic/metabolic issues
     - High n_lab_events → more intensive monitoring
     - High abnormal_ratio → more abnormal test results
     - High age → older patient population

**Example Interpretation:**
- If Cluster 4 has high bars for "n_lab_events", "n_micro_exam", and "abnormal_ratio", it might represent: *"Patients with extensive testing and many abnormal results - possibly complex cases requiring intensive monitoring"*
- If Cluster 0 has low bars across most features, it might represent: *"Relatively healthy patients with minimal testing and normal results"*


### Analysis of Cluster Centroids



```python
# Get centroids (in scaled space) and transform back to original scale
centroids_scaled = kmeans_final.cluster_centers_
centroids_original = scaler.inverse_transform(centroids_scaled)

# Create dataframe with centroids
centroids_df = pd.DataFrame(centroids_original, columns=numeric_features)
centroids_df.index.name = 'cluster'
centroids_df.index = [f'Cluster {i}' for i in range(optimal_k)]

print("Cluster Centroids (original scale):")
print(centroids_df.round(2))

# Visualize centroids
fig, ax = plt.subplots(figsize=(14, 8))
centroids_df.T.plot(kind='bar', ax=ax, width=0.8)
ax.set_xlabel('Features', fontsize=12)
ax.set_ylabel('Feature Value (original scale, log)', fontsize=12)
ax.set_yscale('log')
ax.set_title('Cluster Centroids Comparison (Log Scale)', fontsize=14, fontweight='bold')
ax.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
ax.tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.savefig(plots_dir / '2.1.2_centroids_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

```

    Cluster Centroids (original scale):
               abnormal_ratio  qc_fail_ratio  fluid_diversity  \
    Cluster 0             0.0           0.02             0.95   
    Cluster 1             0.0           0.02             0.85   
    
               procedure_span_days_missing  gender_F  micro_resistance_score  \
    Cluster 0                         0.12      0.38                    0.03   
    Cluster 1                         0.35      0.39                    0.01   
    
               metabolic_stress_index  oxygenation_dysfunction_index  \
    Cluster 0                    4.10                          -0.26   
    Cluster 1                    0.28                          -0.03   
    
               inflammation_liver_stress_index  hematologic_stability_score  \
    Cluster 0                             0.03                        -1.12   
    Cluster 1                             0.01                        -0.23   
    
               renal_failure_index  
    Cluster 0                 6.75  
    Cluster 1                 0.12  
    


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_22_1.png)
    


### Comparison of Variable Distributions: Within Clusters vs Whole Dataset



```python
# Compare distributions for key features
key_features = numeric_features

n_features = len(key_features)
n_cols = 3
n_rows = (n_features + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
axes = axes.flatten() if n_features > 1 else [axes]

for idx, feature in enumerate(key_features):
    ax = axes[idx]
    
    # Plot distribution for each cluster
    for cluster_id in range(optimal_k):
        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id][feature]
        ax.hist(cluster_data, alpha=0.5, label=f'Cluster {cluster_id}', bins=30, density=True)
    
    # Plot overall distribution
    ax.hist(df_clustered[feature], alpha=0.3, color='black', 
           label='Overall', bins=30, density=True, linestyle='--', linewidth=2)
    
    ax.set_xlabel(feature, fontsize=11)
    ax.set_ylabel('Density', fontsize=11)
    ax.set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

# Hide unused subplots
for idx in range(n_features, len(axes)):
    axes[idx].axis('off')

plt.tight_layout()
plt.savefig(plots_dir / '2.1.2_feature_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_24_0.png)
    



```python
# Statistical comparison: mean values per cluster vs overall mean
comparison_stats = []

for feature in numeric_features:
    overall_mean = df_clustered[feature].mean()
    overall_std = df_clustered[feature].std()
    
    for cluster_id in range(optimal_k):
        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id][feature]
        cluster_mean = cluster_data.mean()
        cluster_std = cluster_data.std()
        
        # Z-score: how many standard deviations away from overall mean
        z_score = (cluster_mean - overall_mean) / overall_std if overall_std > 0 else 0
        
        comparison_stats.append({
            'feature': feature,
            'cluster': cluster_id,
            'cluster_mean': cluster_mean,
            'cluster_std': cluster_std,
            'overall_mean': overall_mean,
            'overall_std': overall_std,
            'difference': cluster_mean - overall_mean,
            'z_score': z_score
        })

comparison_df = pd.DataFrame(comparison_stats)

# Create heatmap of z-scores
pivot_z = comparison_df.pivot(index='feature', columns='cluster', values='z_score')

plt.figure(figsize=(10, max(8, len(numeric_features)*0.5)))
sns.heatmap(pivot_z, annot=True, fmt='.2f', cmap='RdBu_r', center=0,
           cbar_kws={'label': 'Z-score (standard deviations from overall mean)'})
plt.title('Feature Characteristics by Cluster (Z-scores)\nPositive = above average, Negative = below average', 
         fontsize=14, fontweight='bold')
plt.xlabel('Cluster', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.tight_layout()
plt.savefig(plots_dir / '2.1.2_cluster_characteristics_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nCluster Characteristics Summary:")
print(comparison_df.groupby('cluster')[['feature', 'z_score']].apply(
    lambda x: x.nlargest(3, 'z_score')[['feature', 'z_score']].to_dict('records')
))

```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_25_0.png)
    


    
    Cluster Characteristics Summary:
    cluster
    0    [{'feature': 'renal_failure_index', 'z_score':...
    1    [{'feature': 'hematologic_stability_score', 'z...
    dtype: object
    

### 2D Visualization using PCA



```python
# Reduce to 2D using PCA for visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# Create visualization
fig, ax = plt.subplots(figsize=(12, 8))

scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, 
                    cmap='viridis', alpha=0.6, s=50, edgecolors='k', linewidth=0.5)

# Plot centroids in PCA space
centroids_pca = pca.transform(kmeans_final.cluster_centers_)
ax.scatter(centroids_pca[:, 0], centroids_pca[:, 1], 
          c='red', marker='X', s=300, edgecolors='black', linewidth=2, 
          label='Centroids', zorder=10)

ax.set_xlabel(f'First Principal Component (explained variance: {pca.explained_variance_ratio_[0]:.2%})', 
             fontsize=12)
ax.set_ylabel(f'Second Principal Component (explained variance: {pca.explained_variance_ratio_[1]:.2%})', 
             fontsize=12)
ax.set_title(f'K-means Clustering Results (k={optimal_k}) - PCA Visualization', 
            fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax, label='Cluster')
plt.tight_layout()
plt.savefig(plots_dir / '2.1.2_pca_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"PCA explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total explained variance: {pca.explained_variance_ratio_.sum():.2%}")

```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_27_0.png)
    


    PCA explained variance ratio: [0.53163852 0.1914993 ]
    Total explained variance: 72.31%
    

### UMAP vs PCA


```python
import umap
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Initialize UMAP
# n_neighbors: low (5-20) focuses on local detail; high (50+) focuses on global structure
# min_dist: controls how tightly points are packed together
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# 2. Re-run your PCA for the comparison
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# 3. Create Side-by-Side Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Plot PCA
# Plot centroids in PCA space
centroids_pca = pca.transform(kmeans_final.cluster_centers_)
ax1.scatter(centroids_pca[:, 0], centroids_pca[:, 1], 
          c='red', marker='X', s=200, edgecolors='black', linewidth=2, 
          label='Centroids', zorder=10)

scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, 
                      cmap='viridis', alpha=0.6, s=30, edgecolors='k', linewidth=0.2)
ax1.set_title('PCA Projection (Linear)', fontsize=15, fontweight='bold')
ax1.set_xlabel('PC1')
ax1.set_ylabel('PC2')

# Plot UMAP
# plot centroids in UMAP space
centroids_umap = reducer.transform(kmeans_final.cluster_centers_)
ax2.scatter(centroids_umap[:, 0], centroids_umap[:, 1], 
          c='red', marker='X', s=200, edgecolors='black', linewidth=2, 
          label='Centroids', zorder=10)

scatter2 = ax2.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, 
                      cmap='viridis', alpha=0.6, s=30, edgecolors='k', linewidth=0.2)
ax2.set_title('UMAP Projection (Non-Linear)', fontsize=15, fontweight='bold')
ax2.set_xlabel('UMAP 1')
ax2.set_ylabel('UMAP 2')

# Add a shared colorbar
cbar = fig.colorbar(scatter2, ax=[ax1, ax2], location='right', shrink=0.6)
cbar.set_label('Cluster ID', fontsize=12)

plt.suptitle(f'Dimensionality Reduction Comparison (k={optimal_k})', fontsize=18, fontweight='bold', y=1.02)
plt.savefig(plots_dir / '2.1.2_pca_vs_umap_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_29_0.png)
    


### Clinical outcomes


```python
# Create a quick summary table of clinical outcomes
outcome_summary = df_clustered.groupby('cluster').agg({
    'metabolic_stress_index': 'mean',
    'hematologic_stability_score': 'mean',
    'renal_failure_index': 'mean',
    'abnormal_ratio': 'mean'
}).round(3)

print("Clinical Outcomes by Cluster:")
print(outcome_summary)
```

    Clinical Outcomes by Cluster:
             metabolic_stress_index  hematologic_stability_score  \
    cluster                                                        
    0                         4.102                       -1.125   
    1                         0.280                       -0.226   
    
             renal_failure_index  abnormal_ratio  
    cluster                                       
    0                      6.748             0.0  
    1                      0.116             0.0  
    

### Describe per cluster


```python
# # This creates a summary table of every feature's mean, min, and max for each cluster
# cluster_summary_table = df_clustered.groupby('cluster').describe().transpose()

# means_only = df_clustered.groupby('cluster').mean()
# print(means_only)
```


```python
print("=" * 80)
print("CLUSTER CHARACTERIZATION WITH AGE")
print("=" * 80)

#load df1 for age analysis of clusters
OG_DATA_DIR = (notebook_dir / '..' /  '..' /  'Data').resolve()
path = f"{OG_DATA_DIR}/heart_diagnoses_1_agg_features.csv"
age_df = pd.read_csv(Path(path))

# Merge age data from age_df back to clustered data
# age_df should have columns: subject_id, hadm_id, age (and possibly others)
print(f"\nAge dataframe shape: {age_df.shape}")
print(f"Age dataframe columns: {age_df.columns.tolist()}")

# Select relevant columns from age_df (subject_id, hadm_id, age)
age_cols = [col for col in ['subject_id', 'hadm_id', 'age'] if col in age_df.columns]
age_subset = age_df[age_cols].copy()

# Merge on subject_id and hadm_id
df_with_age = df_clustered.merge(
    age_subset,
    on=['subject_id', 'hadm_id'],
    how='left'
)

print(f"\nMerged data shape: {df_with_age.shape}")
print(f"Rows with age data: {df_with_age['age'].notna().sum()} / {len(df_with_age)}")

# Cluster characterization by age
print("\n" + "=" * 80)
print("AGE STATISTICS BY CLUSTER")
print("=" * 80)

age_cluster_stats = df_with_age.groupby('cluster')['age'].agg([
    ('count', 'count'),
    ('mean', 'mean'),
    ('median', 'median'),
    ('std', 'std'),
    ('min', 'min'),
    ('max', 'max'),
    ('pct_missing', lambda x: (x.isna().sum() / len(x) * 100))
]).round(2)

print(age_cluster_stats)

# Visualize age distribution by cluster
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Box plot
ax = axes[0, 0]
df_with_age.boxplot(column='age', by='cluster', ax=ax)
ax.set_xlabel('Cluster', fontsize=11)
ax.set_ylabel('Age (years)', fontsize=11)
ax.set_title('Age Distribution by Cluster (Box Plot)', fontsize=12, fontweight='bold')
plt.sca(ax)
plt.xticks(rotation=0)

# 2. Violin plot
ax = axes[0, 1]
sns.violinplot(data=df_with_age, x='cluster', y='age', ax=ax, palette='Set2')
ax.set_xlabel('Cluster', fontsize=11)
ax.set_ylabel('Age (years)', fontsize=11)
ax.set_title('Age Distribution by Cluster (Violin Plot)', fontsize=12, fontweight='bold')

# 3. Histogram with overlaid distributions
ax = axes[1, 0]
for cluster_id in sorted(df_with_age['cluster'].unique()):
    if cluster_id is not None:
        cluster_age = df_with_age[df_with_age['cluster'] == cluster_id]['age'].dropna()
        ax.hist(cluster_age, alpha=0.5, label=f'Cluster {cluster_id}', bins=20)
ax.set_xlabel('Age (years)', fontsize=11)
ax.set_ylabel('Frequency', fontsize=11)
ax.set_title('Age Distribution by Cluster (Histogram)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# 4. Statistics table
ax = axes[1, 1]
ax.axis('off')
table_data = age_cluster_stats.values
table_cols = age_cluster_stats.columns.tolist()
table_rows = [f'Cluster {i}' for i in age_cluster_stats.index]

table = ax.table(
    cellText=table_data,
    colLabels=table_cols,
    rowLabels=table_rows,
    cellLoc='center',
    loc='center',
    bbox=[0, 0, 1, 1]
)
table.auto_set_font_size(False)
table.set_fontsize(9)
table.scale(1, 2)
ax.set_title('Age Statistics by Cluster', fontsize=12, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig(plots_dir / '2.1.2_age_characterization_by_cluster.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nAge characterization plot saved!")
```

    ================================================================================
    CLUSTER CHARACTERIZATION WITH AGE
    ================================================================================
    
    Age dataframe shape: (4864, 18)
    Age dataframe columns: ['subject_id', 'hadm_id', 'gender', 'age', 'is_dead', 'charttime', 'icd_code', 'icd_cat', 'imaging_variety', 'doc_complexity_index', 'cardiac_comorbidity_score', 'has_heart', 'has_hf', 'has_arr', 'has_ami', 'has_arrest', 'has_valvular', 'has_inflammatory']
    
    Merged data shape: (5166, 16)
    Rows with age data: 1363 / 5166
    
    ================================================================================
    AGE STATISTICS BY CLUSTER
    ================================================================================
             count   mean  median    std   min   max  pct_missing
    cluster                                                      
    0           73  69.85    70.0  13.27  39.0  93.0        78.96
    1         1290  68.93    70.0  15.07  18.0  95.0        73.23
    


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_34_1.png)
    


    
    Age characterization plot saved!
    

## 2.1.3 Evaluation of the Clustering Results



```python
# Compute evaluation metrics
silhouette_avg = silhouette_score(X_scaled, cluster_labels)
davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)
calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)
inertia = kmeans_final.inertia_

print("="*80)
print("K-MEANS CLUSTERING EVALUATION METRICS")
print("="*80)
print(f"Number of clusters (k): {optimal_k}")
print(f"\nSilhouette Score: {silhouette_avg:.4f}")
print("  (Range: -1 to 1, higher is better)")
print(f"\nDavies-Bouldin Score: {davies_bouldin:.4f}")
print("  (Range: 0 to ∞, lower is better)")
print(f"\nCalinski-Harabasz Score: {calinski_harabasz:.4f}")
print("  (Range: 0 to ∞, higher is better)")
print(f"\nInertia (Within-cluster sum of squares): {inertia:.2f}")
print("  (Lower is better, but decreases with more clusters)")

# Create summary dataframe
evaluation_summary = pd.DataFrame({
    'Metric': ['Silhouette Score', 'Davies-Bouldin Score', 
               'Calinski-Harabasz Score', 'Inertia'],
    'Value': [silhouette_avg, davies_bouldin, calinski_harabasz, inertia],
    'Interpretation': [
        f"{'Good' if silhouette_avg > 0.5 else 'Fair' if silhouette_avg > 0.3 else 'Poor'} clustering structure",
        f"{'Good' if davies_bouldin < 1 else 'Fair' if davies_bouldin < 2 else 'Poor'} cluster separation",
        f"Higher values indicate better defined clusters",
        f"Within-cluster variance"
    ]
})

print("\n" + "="*80)
print("EVALUATION SUMMARY")
print("="*80)
print(evaluation_summary.to_string(index=False))

```

    ================================================================================
    K-MEANS CLUSTERING EVALUATION METRICS
    ================================================================================
    Number of clusters (k): 2
    
    Silhouette Score: 0.6352
      (Range: -1 to 1, higher is better)
    
    Davies-Bouldin Score: 0.8393
      (Range: 0 to ∞, lower is better)
    
    Calinski-Harabasz Score: 2073.6114
      (Range: 0 to ∞, higher is better)
    
    Inertia (Within-cluster sum of squares): 50578.77
      (Lower is better, but decreases with more clusters)
    
    ================================================================================
    EVALUATION SUMMARY
    ================================================================================
                     Metric        Value                                 Interpretation
           Silhouette Score     0.635203                      Good clustering structure
       Davies-Bouldin Score     0.839285                        Good cluster separation
    Calinski-Harabasz Score  2073.611437 Higher values indicate better defined clusters
                    Inertia 50578.767745                        Within-cluster variance
    


```python
# Cluster size analysis
cluster_sizes = df_clustered['cluster'].value_counts().sort_index()
cluster_proportions = cluster_sizes / len(df_clustered)

print("\n" + "="*80)
print("CLUSTER SIZE ANALYSIS")
print("="*80)
for cluster_id in range(optimal_k):
    size = cluster_sizes[cluster_id]
    prop = cluster_proportions[cluster_id]
    print(f"Cluster {cluster_id}: {size:,} subjects ({prop:.1%})")

# Visualize cluster sizes
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Bar plot
cluster_sizes.plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black')
ax1.set_xlabel('Cluster', fontsize=12)
ax1.set_ylabel('Number of Subjects', fontsize=12)
ax1.set_title('Cluster Sizes', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# Pie chart
ax2.pie(cluster_sizes, labels=[f'Cluster {i}' for i in range(optimal_k)], 
       autopct='%1.1f%%', startangle=90, colors=plt.cm.Set3.colors)
ax2.set_title('Cluster Distribution', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(plots_dir / '2.1.3_cluster_sizes.png', dpi=300, bbox_inches='tight')
plt.show()

```

    
    ================================================================================
    CLUSTER SIZE ANALYSIS
    ================================================================================
    Cluster 0: 347 subjects (6.7%)
    Cluster 1: 4,819 subjects (93.3%)
    


    
![png](2.1_kmeans_clustering_files/2.1_kmeans_clustering_37_1.png)
    



```python

```


```python

```


```python

```


```python

```


```python

```
