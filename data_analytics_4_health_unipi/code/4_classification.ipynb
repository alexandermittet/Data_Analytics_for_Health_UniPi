{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Classification Analysis\n",
    "\n",
    "## Objective\n",
    "Create binary classification models to distinguish between ischemic (Class 1) and non-ischemic (Class 0) cardiovascular conditions based on patient profiles.\n",
    "\n",
    "### Label Definition\n",
    "- **Class 1 (Ischemic)**: ICD codes I20, I21, I22, I24, I25\n",
    "- **Class 0 (Non-ischemic)**: All other cardiovascular codes\n",
    "- If a patient has multiple diagnoses and at least one is ischemic, label as Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = \"/Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data\"\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load heart diagnoses dataset\n",
    "heart_diag = pd.read_csv(f\"{DATA_DIR}/heart_diagnoses_1.csv\")\n",
    "print(f\"Heart Diagnoses shape: {heart_diag.shape}\")\n",
    "print(f\"\\nColumns: {heart_diag.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "heart_diag[['subject_id', 'hadm_id', 'icd_code']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patient profile\n",
    "patient_profile = pd.read_csv(f\"{DATA_DIR}/patient_profile_small.csv\")\n",
    "print(f\"Patient Profile shape: {patient_profile.shape}\")\n",
    "print(f\"\\nColumns: {patient_profile.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "patient_profile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Binary Labels\n",
    "\n",
    "Following the reference implementation from `labels_binary_classification.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all cardiovascular ICD codes (as per reference script)\n",
    "icds = {\n",
    "    \"I20\", \"I21\", \"I22\", \"I24\", \"I25\",\n",
    "    \"I30\", \"I31\", \"I33\",\n",
    "    \"I34\", \"I35\", \"I36\",\n",
    "    \"I40\", \"I42\",\n",
    "    \"I44\", \"I45\", \"I46\", \"I47\", \"I48\", \"I49\",\n",
    "    \"I50\"\n",
    "}\n",
    "\n",
    "# Define ischemic codes (Class 1)\n",
    "class1 = {\"I20\", \"I21\", \"I22\", \"I24\", \"I25\"}\n",
    "\n",
    "# Function to compute ischemic label (matching reference script)\n",
    "def compute_ischemic_label(code_set: set) -> int:\n",
    "    \"\"\"Check if any code in the set is ischemic using set intersection\"\"\"\n",
    "    ischemic = len(code_set & class1) > 0\n",
    "    return 1 if ischemic else 0\n",
    "\n",
    "# Clean and process ICD codes (matching reference script)\n",
    "heart_diag[\"subject_id\"] = heart_diag[\"subject_id\"].astype(str).str.strip()\n",
    "heart_diag[\"icd_code\"] = (\n",
    "    heart_diag[\"icd_code\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .replace({\"\": np.nan, \"NAN\": np.nan})\n",
    ")\n",
    "\n",
    "# Filter to only valid cardiovascular codes (matching reference script)\n",
    "diag_valid = heart_diag[heart_diag[\"icd_code\"].isin(icds)].copy()\n",
    "if diag_valid.empty:\n",
    "    raise ValueError(\"Problems with the format of the codes\")\n",
    "\n",
    "print(f\"Total valid cardiovascular diagnoses: {len(diag_valid)}\")\n",
    "print(f\"Unique ICD codes: {diag_valid['icd_code'].nunique()}\")\n",
    "print(f\"\\nICD code distribution:\")\n",
    "print(diag_valid['icd_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by subject_id and get unique ICD codes (matching reference script)\n",
    "subject_codes = (\n",
    "    diag_valid.groupby(\"subject_id\")[\"icd_code\"].unique().reset_index(name=\"icd_codes_list\")\n",
    ")\n",
    "subject_codes[\"icd_codes_set\"] = subject_codes[\"icd_codes_list\"].apply(set)\n",
    "\n",
    "# Create binary labels\n",
    "subject_codes[\"label_ischemic\"] = subject_codes[\"icd_codes_set\"].apply(compute_ischemic_label)\n",
    "\n",
    "# Display label distribution\n",
    "print(\"Label distribution:\")\n",
    "print(subject_codes[\"label_ischemic\"].value_counts())\n",
    "print(f\"\\nClass 0 (Non-ischemic): {(subject_codes['label_ischemic'] == 0).sum()}\")\n",
    "print(f\"Class 1 (Ischemic): {(subject_codes['label_ischemic'] == 1).sum()}\")\n",
    "print(f\"\\nClass balance: {(subject_codes['label_ischemic'] == 1).sum() / len(subject_codes):.2%}\")\n",
    "\n",
    "# Show sample\n",
    "subject_codes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Labels with Patient Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels dataframe (keep only subject_id and label)\n",
    "labels_df = subject_codes[[\"subject_id\", \"label_ischemic\"]].copy()\n",
    "labels_df[\"subject_id\"] = labels_df[\"subject_id\"].astype(str)\n",
    "\n",
    "# Convert patient_profile subject_id to string for merging\n",
    "patient_profile[\"subject_id\"] = patient_profile[\"subject_id\"].astype(str)\n",
    "\n",
    "# Merge patient profile with labels\n",
    "# Since patient_profile has multiple rows per subject_id (different hadm_id), \n",
    "# we'll aggregate or use the first admission per subject\n",
    "df_classification = patient_profile.merge(\n",
    "    labels_df, \n",
    "    on=\"subject_id\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"After merging: {df_classification.shape}\")\n",
    "print(f\"Unique subjects: {df_classification['subject_id'].nunique()}\")\n",
    "print(f\"\\nLabel distribution in merged dataset:\")\n",
    "print(df_classification[\"label_ischemic\"].value_counts())\n",
    "\n",
    "# Check for missing labels\n",
    "print(f\"\\nSubjects with labels: {df_classification['label_ischemic'].notna().sum()}\")\n",
    "print(f\"Subjects without labels: {df_classification['label_ischemic'].isna().sum()}\")\n",
    "\n",
    "df_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle multiple admissions per subject\n",
    "# Option 1: Use the first admission per subject (or we could aggregate)\n",
    "# For now, let's use one row per subject (first admission)\n",
    "df_classification_unique = df_classification.groupby(\"subject_id\").first().reset_index()\n",
    "\n",
    "print(f\"After deduplication: {df_classification_unique.shape}\")\n",
    "print(f\"Unique subjects: {df_classification_unique['subject_id'].nunique()}\")\n",
    "print(f\"\\nFinal label distribution:\")\n",
    "print(df_classification_unique[\"label_ischemic\"].value_counts())\n",
    "print(f\"\\nClass balance: {(df_classification_unique['label_ischemic'] == 1).sum() / len(df_classification_unique):.2%}\")\n",
    "\n",
    "df_classification_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# Drop non-feature columns\n",
    "X = df_classification_unique.drop(columns=[\"subject_id\", \"hadm_id\", \"label_ischemic\"])\n",
    "y = df_classification_unique[\"label_ischemic\"]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {X.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(X.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(X.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Check categorical feature values\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} unique values: {X[col].unique()}\")\n",
    "    print(f\"{col} value counts:\\n{X[col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Encode gender (binary: M/F)\n",
    "if 'gender' in categorical_cols:\n",
    "    le_gender = LabelEncoder()\n",
    "    X_processed['gender_encoded'] = le_gender.fit_transform(X_processed['gender'].fillna('Unknown'))\n",
    "    X_processed = X_processed.drop(columns=['gender'])\n",
    "    print(f\"Gender encoding: {dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_)))}\")\n",
    "\n",
    "# Handle missing values in numerical columns\n",
    "# Use median imputation for numerical features\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_processed[numerical_cols] = imputer.fit_transform(X_processed[numerical_cols])\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Shape: {X_processed.shape}\")\n",
    "print(f\"Missing values: {X_processed.isnull().sum().sum()}\")\n",
    "print(f\"\\nFeature columns: {X_processed.columns.tolist()}\")\n",
    "\n",
    "X_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for highly correlated features\n",
    "correlation_matrix = X_processed.corr()\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:  # Threshold for high correlation\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (|correlation| > 0.9):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found (threshold: 0.9)\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features (keep one from each pair)\n",
    "# We'll keep the first feature and remove the second\n",
    "features_to_remove = set()\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    if feat2 not in features_to_remove:\n",
    "        features_to_remove.add(feat2)\n",
    "        print(f\"Removing {feat2} (highly correlated with {feat1}: {corr:.3f})\")\n",
    "\n",
    "if features_to_remove:\n",
    "    X_processed = X_processed.drop(columns=list(features_to_remove))\n",
    "    print(f\"\\nAfter removing correlated features: {X_processed.shape}\")\n",
    "else:\n",
    "    print(\"No features removed due to correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split and Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets (stratified to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Check class imbalance\n",
    "class_counts_train = y_train.value_counts()\n",
    "imbalance_ratio = class_counts_train[0] / class_counts_train[1]\n",
    "print(f\"\\nClass imbalance ratio (Class 0 / Class 1): {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for logistic regression and distance-based methods)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features standardized successfully!\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Classification Models\n",
    "\n",
    "We will train and compare the following models:\n",
    "1. **Logistic Regression** - Linear model, interpretable\n",
    "2. **Decision Tree** - Non-linear, interpretable\n",
    "3. **Random Forest** - Ensemble of trees, robust\n",
    "4. **Gradient Boosting** - Advanced ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight='balanced', max_depth=10),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100, max_depth=10),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "# Train models and store results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Use scaled features for logistic regression, original for tree-based models\n",
    "    if name == \"Logistic Regression\":\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        y_train_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
    "        \"precision\": precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_train, y_train_pred),\n",
    "        \"f1\": f1_score(y_train, y_train_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_train, y_train_proba)\n",
    "    }\n",
    "    \n",
    "    test_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
    "        \"precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_test_pred),\n",
    "        \"f1\": f1_score(y_test, y_test_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_test_proba)\n",
    "    }\n",
    "    \n",
    "    results[name] = {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_test_proba\": y_test_proba,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"y_train_proba\": y_train_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        \"Model\": name,\n",
    "        \"Train Accuracy\": result[\"train\"][\"accuracy\"],\n",
    "        \"Test Accuracy\": result[\"test\"][\"accuracy\"],\n",
    "        \"Train Balanced Accuracy\": result[\"train\"][\"balanced_accuracy\"],\n",
    "        \"Test Balanced Accuracy\": result[\"test\"][\"balanced_accuracy\"],\n",
    "        \"Train Precision\": result[\"train\"][\"precision\"],\n",
    "        \"Test Precision\": result[\"test\"][\"precision\"],\n",
    "        \"Train Recall\": result[\"train\"][\"recall\"],\n",
    "        \"Test Recall\": result[\"test\"][\"recall\"],\n",
    "        \"Train F1\": result[\"train\"][\"f1\"],\n",
    "        \"Test F1\": result[\"test\"][\"f1\"],\n",
    "        \"Train ROC-AUC\": result[\"train\"][\"roc_auc\"],\n",
    "        \"Test ROC-AUC\": result[\"test\"][\"roc_auc\"]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"Model Comparison - All Metrics\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison of key metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = [\n",
    "    (\"Test Accuracy\", \"accuracy\"),\n",
    "    (\"Test Balanced Accuracy\", \"balanced_accuracy\"),\n",
    "    (\"Test Precision\", \"precision\"),\n",
    "    (\"Test Recall\", \"recall\"),\n",
    "    (\"Test F1-Score\", \"f1\"),\n",
    "    (\"Test ROC-AUC\", \"roc_auc\")\n",
    "]\n",
    "\n",
    "for idx, (title, metric_key) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    values = [results[name][\"test\"][metric_key] for name in models.keys()]\n",
    "    bars = ax.bar(models.keys(), values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Score\", fontsize=10)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, result[\"y_test_pred\"])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Non-ischemic', 'Ischemic'],\n",
    "                yticklabels=['Non-ischemic', 'Ischemic'])\n",
    "    ax.set_title(f\"{name}\\nConfusion Matrix\", fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(\"True Label\", fontsize=10)\n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result[\"y_test_proba\"])\n",
    "    auc = result[\"test\"][\"roc_auc\"]\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Classification Report: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(classification_report(y_test, result[\"y_test_pred\"], \n",
    "                                target_names=['Non-ischemic', 'Ischemic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, name in enumerate(tree_models):\n",
    "    if name in trained_models:\n",
    "        model = trained_models[name]\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = X_train.columns\n",
    "            \n",
    "            # Sort by importance\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            top_n = min(15, len(feature_names))  # Show top 15 features\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.barh(range(top_n), importances[indices[:top_n]], color='steelblue')\n",
    "            ax.set_yticks(range(top_n))\n",
    "            ax.set_yticklabels([feature_names[i] for i in indices[:top_n]], fontsize=9)\n",
    "            ax.set_xlabel('Feature Importance', fontsize=10)\n",
    "            ax.set_title(f'{name}\\nTop {top_n} Features', fontsize=11, fontweight='bold')\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression coefficients\n",
    "if \"Logistic Regression\" in trained_models:\n",
    "    lr_model = trained_models[\"Logistic Regression\"]\n",
    "    coefficients = lr_model.coef_[0]\n",
    "    feature_names = X_train_scaled.columns\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "    top_n = min(15, len(feature_names))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in coefficients[indices[:top_n]]]\n",
    "    plt.barh(range(top_n), coefficients[indices[:top_n]], color=colors)\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices[:top_n]], fontsize=9)\n",
    "    plt.xlabel('Coefficient Value', fontsize=11)\n",
    "    plt.title(f'Logistic Regression\\nTop {top_n} Feature Coefficients', fontsize=12, fontweight='bold')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for more robust evaluation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Use appropriate features\n",
    "    if name == \"Logistic Regression\":\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores_accuracy = cross_val_score(model, X_cv, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_scores_balanced = cross_val_score(model, X_cv, y_train, cv=cv, scoring='balanced_accuracy')\n",
    "    cv_scores_f1 = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')\n",
    "    cv_scores_roc_auc = cross_val_score(model, X_cv, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        \"accuracy\": cv_scores_accuracy,\n",
    "        \"balanced_accuracy\": cv_scores_balanced,\n",
    "        \"f1\": cv_scores_f1,\n",
    "        \"roc_auc\": cv_scores_roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {cv_scores_accuracy.mean():.4f} (+/- {cv_scores_accuracy.std() * 2:.4f})\")\n",
    "    print(f\"  Balanced Accuracy: {cv_scores_balanced.mean():.4f} (+/- {cv_scores_balanced.std() * 2:.4f})\")\n",
    "    print(f\"  F1-Score: {cv_scores_f1.mean():.4f} (+/- {cv_scores_f1.std() * 2:.4f})\")\n",
    "    print(f\"  ROC-AUC: {cv_scores_roc_auc.mean():.4f} (+/- {cv_scores_roc_auc.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_cv = [\n",
    "    (\"Accuracy\", \"accuracy\"),\n",
    "    (\"Balanced Accuracy\", \"balanced_accuracy\"),\n",
    "    (\"F1-Score\", \"f1\"),\n",
    "    (\"ROC-AUC\", \"roc_auc\")\n",
    "]\n",
    "\n",
    "for idx, (title, metric_key) in enumerate(metrics_cv):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for box plot\n",
    "    data_to_plot = [cv_results[name][metric_key] for name in models.keys()]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=models.keys(), patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors_box = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Score\", fontsize=10)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Discussion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Class Distribution**: The dataset shows [X]% ischemic vs [Y]% non-ischemic cases, indicating [balanced/imbalanced] classes.\n",
    "\n",
    "2. **Model Performance**: \n",
    "   - Best performing model: [Model Name]\n",
    "   - Key metrics: [List key metrics]\n",
    "   \n",
    "3. **Preprocessing Steps Applied**:\n",
    "   - Categorical encoding (gender)\n",
    "   - Missing value imputation (median for numerical features)\n",
    "   - Feature standardization (for logistic regression)\n",
    "   - Correlation analysis and feature removal\n",
    "   - Class imbalance handling (class_weight='balanced')\n",
    "\n",
    "4. **Model Comparison**:\n",
    "   - [Discussion of which model performs best and why]\n",
    "   - [Trade-offs between interpretability and performance]\n",
    "   - [Overfitting analysis]\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   - Most important features: [List top features]\n",
    "   - Clinical relevance: [Discussion]\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "Based on the analysis, [Model Name] appears to be the most appropriate for this classification task because:\n",
    "- [Reason 1]\n",
    "- [Reason 2]\n",
    "- [Reason 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
