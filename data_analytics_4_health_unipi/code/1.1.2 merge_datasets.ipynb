{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics for Health - Task 1.1.2: Merge Datasets\n",
    "\n",
    "## Overview\n",
    "This notebook merges the four healthcare datasets using two strategies:\n",
    "1. **Option A**: Merge on `subject_id` only (patient-level)\n",
    "2. **Option B**: Merge on `(subject_id, hadm_id)` pair (admission-level)\n",
    "\n",
    "## Objectives\n",
    "- Clean problematic hadm_ids (those with multiple subject_ids)\n",
    "- Add subject_id to datasets that only have hadm_id\n",
    "- Aggregate datasets appropriately\n",
    "- Merge all datasets using both strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Data path: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up file paths\n",
    "notebook_dir = Path.cwd().resolve()\n",
    "data_path = (notebook_dir / '..' / 'Data').resolve()\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Heart Diagnoses: 4,864 rows × 25 columns\n",
      "Loaded Laboratory Events: 978,503 rows × 14 columns\n",
      "Loaded Microbiology Events: 15,587 rows × 14 columns\n",
      "Loaded Procedure Codes: 14,497 rows × 6 columns\n",
      "\n",
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load all four datasets\n",
    "df1 = pd.read_csv(data_path / 'heart_diagnoses_1.csv')  # Heart Diagnoses\n",
    "df2 = pd.read_csv(data_path / 'laboratory_events_codes_2.csv')  # Laboratory Events\n",
    "df3 = pd.read_csv(data_path / 'microbiology_events_codes_3.csv')  # Microbiology Events\n",
    "df4 = pd.read_csv(data_path / 'procedure_code_4.csv')  # Procedure Codes\n",
    "\n",
    "print(f\"Loaded Heart Diagnoses: {df1.shape[0]:,} rows × {df1.shape[1]} columns\")\n",
    "print(f\"Loaded Laboratory Events: {df2.shape[0]:,} rows × {df2.shape[1]} columns\")\n",
    "print(f\"Loaded Microbiology Events: {df3.shape[0]:,} rows × {df3.shape[1]} columns\")\n",
    "print(f\"Loaded Procedure Codes: {df4.shape[0]:,} rows × {df4.shape[1]} columns\")\n",
    "print(\"\\nAll datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for Problematic hadm_ids\n",
    "\n",
    "Some hadm_ids map to multiple subject_ids, which violates data integrity. We'll identify and handle these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Checking for problematic hadm_ids (multiple subject_ids per hadm_id)\n",
      "================================================================================\n",
      "\n",
      "Heart Diagnoses:\n",
      "  Total unique hadm_ids: 4,761\n",
      "  Problematic hadm_ids (multiple subject_ids): 101\n",
      "  Percentage: 2.12%\n",
      "    hadm_id 20200492: 2 subject_ids -> [19781816, 19998560]\n",
      "    hadm_id 20222315: 2 subject_ids -> [19032473, 19998539]\n",
      "    hadm_id 20343031: 2 subject_ids -> [17922874, 19998599]\n",
      "    hadm_id 20624985: 2 subject_ids -> [12483604, 19998509]\n",
      "    hadm_id 20706765: 2 subject_ids -> [12407830, 19998533]\n",
      "\n",
      "Microbiology Events:\n",
      "  Total unique hadm_ids: 2,454\n",
      "  Problematic hadm_ids (multiple subject_ids): 256\n",
      "  Percentage: 10.43%\n",
      "    hadm_id 20007905.0: 3 subject_ids -> [13709807, 19997460, 19997485]\n",
      "    hadm_id 20095782.0: 2 subject_ids -> [17443221, 19997450]\n",
      "    hadm_id 20097155.0: 2 subject_ids -> [18048134, 19997631]\n",
      "    hadm_id 20113266.0: 2 subject_ids -> [13933090, 19997491]\n",
      "    hadm_id 20205373.0: 2 subject_ids -> [17844820, 19997544]\n",
      "\n",
      "Procedure Codes:\n",
      "  Total unique hadm_ids: 3,459\n",
      "  Problematic hadm_ids (multiple subject_ids): 0\n",
      "  Percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to clean datasets: remove hadm_ids with multiple subject_ids\n",
    "def clean_df(df):\n",
    "    \"\"\"Remove rows where hadm_id maps to multiple subject_ids\"\"\"\n",
    "    # if 'hadm_id' in df.columns and 'subject_id' in df.columns:\n",
    "    #     # Count unique subject_ids per hadm_id\n",
    "    #     counts = df.groupby('hadm_id')['subject_id'].nunique()\n",
    "    #     # Keep only hadm_ids with exactly one subject_id\n",
    "    #     valid_hadm = counts[counts == 1].index\n",
    "    #     return df[df['hadm_id'].isin(valid_hadm)].copy()\n",
    "    return df.copy()\n",
    "\n",
    "# Check for problematic hadm_ids\n",
    "print(\"=\"*80)\n",
    "print(\"Checking for problematic hadm_ids (multiple subject_ids per hadm_id)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, df in [(\"Heart Diagnoses\", df1), (\"Microbiology Events\", df3), (\"Procedure Codes\", df4)]:\n",
    "    if 'hadm_id' in df.columns and 'subject_id' in df.columns:\n",
    "        total_hadm = df['hadm_id'].nunique()\n",
    "        problematic = df.groupby('hadm_id')['subject_id'].nunique()\n",
    "        problematic_count = (problematic > 1).sum()\n",
    "        pct = (problematic_count / total_hadm * 100) if total_hadm > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Total unique hadm_ids: {total_hadm:,}\")\n",
    "        print(f\"  Problematic hadm_ids (multiple subject_ids): {problematic_count}\")\n",
    "        print(f\"  Percentage: {pct:.2f}%\")\n",
    "        \n",
    "        if problematic_count > 0:\n",
    "            examples = problematic[problematic > 1].head(5)\n",
    "            for hadm, count in examples.items():\n",
    "                subject_ids = df[df['hadm_id'] == hadm]['subject_id'].unique()\n",
    "                print(f\"    hadm_id {hadm}: {count} subject_ids -> {list(subject_ids)}\")\n",
    "\n",
    "# # Clean all datasets\n",
    "df1_clean = clean_df(df1)\n",
    "df2_clean = df2.copy()  # df2 doesn't have subject_id yet\n",
    "df3_clean = clean_df(df3)\n",
    "df4_clean = clean_df(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Merge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def merge_option_a_subject_id(df1, df2, df3, df4):\n",
    "    \"\"\"\n",
    "    Option A: Merge on subject_id only (patient-level aggregation)\n",
    "    - Start with df1 (Heart Diagnoses) as base\n",
    "    - Add subject_id to df2 (Labs) using reference from df1/df3\n",
    "    - Aggregate all datasets by subject_id\n",
    "    - Merge all on subject_id\n",
    "    \"\"\"\n",
    "    # Create reference table for hadm_id -> subject_id mapping\n",
    "    ref_table = pd.concat([\n",
    "        df1_clean[['hadm_id', 'subject_id']].drop_duplicates(),\n",
    "        df3_clean[['hadm_id', 'subject_id']].drop_duplicates()\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    # Add subject_id to df2 (Labs) - it only has hadm_id\n",
    "    df2_with_subject = df2_clean.merge(ref_table, on='hadm_id', how='left')\n",
    "    print(f\"df2 (Labs) after adding subject_id: {df2_with_subject['subject_id'].notna().sum():,} / {len(df2_with_subject):,} rows have subject_id\")\n",
    "    \n",
    "    # Aggregate df2 by subject_id (numeric columns: mean and count)\n",
    "    numeric_cols_df2 = df2_with_subject.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df2 = [c for c in numeric_cols_df2 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df2 = {}\n",
    "    for col in numeric_cols_df2:\n",
    "        agg_dict_df2[col] = ['mean', 'count']\n",
    "    \n",
    "    df2_agg = df2_with_subject.groupby('subject_id').agg(agg_dict_df2).reset_index()\n",
    "    df2_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df2 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df3 by subject_id\n",
    "    numeric_cols_df3 = df3_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df3 = [c for c in numeric_cols_df3 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df3 = {}\n",
    "    for col in numeric_cols_df3:\n",
    "        agg_dict_df3[col] = ['mean', 'count']\n",
    "    \n",
    "    df3_agg = df3_clean.groupby('subject_id').agg(agg_dict_df3).reset_index()\n",
    "    df3_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df3 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df4 by subject_id\n",
    "    numeric_cols_df4 = df4_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df4 = [c for c in numeric_cols_df4 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df4 = {}\n",
    "    for col in numeric_cols_df4:\n",
    "        agg_dict_df4[col] = ['mean', 'count']\n",
    "    \n",
    "    df4_agg = df4_clean.groupby('subject_id').agg(agg_dict_df4).reset_index()\n",
    "    df4_agg.columns = ['subject_id'] + [f'{col}_{stat}' for col in numeric_cols_df4 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Start with df1 (keep all columns, but aggregate if multiple rows per subject_id)\n",
    "    # For df1, we'll take the first row per subject_id (or could aggregate)\n",
    "    df1_base = df1_clean.groupby('subject_id').first().reset_index()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged = df1_base.merge(df2_agg, on='subject_id', how='outer')\n",
    "    merged = merged.merge(df3_agg, on='subject_id', how='outer')\n",
    "    merged = merged.merge(df4_agg, on='subject_id', how='outer')\n",
    "    \n",
    "    return merged, df2_agg, df3_agg, df4_agg\n",
    "\n",
    "\n",
    "def merge_option_b_subject_hadm_id(df1, df2, df3, df4):\n",
    "    \"\"\"\n",
    "    Option B: Merge on (subject_id, hadm_id) pair (admission-level)\n",
    "    - Start with df1 (Heart Diagnoses) as base\n",
    "    - Add subject_id to df2 (Labs) using reference from df1/df3\n",
    "    - Aggregate all datasets by (subject_id, hadm_id)\n",
    "    - Merge all on (subject_id, hadm_id)\n",
    "    \"\"\"\n",
    "    # Create reference table for hadm_id -> subject_id mapping\n",
    "    ref_table = pd.concat([\n",
    "        df1_clean[['hadm_id', 'subject_id']].drop_duplicates(),\n",
    "        df3_clean[['hadm_id', 'subject_id']].drop_duplicates()\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    # Add subject_id to df2 (Labs)\n",
    "    df2_with_subject = df2_clean.merge(ref_table, on='hadm_id', how='left')\n",
    "    print(f\"df2 (Labs) after adding subject_id: {df2_with_subject['subject_id'].notna().sum():,} / {len(df2_with_subject):,} rows have subject_id\")\n",
    "    \n",
    "    # Aggregate df2 by (subject_id, hadm_id)\n",
    "    numeric_cols_df2 = df2_with_subject.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df2 = [c for c in numeric_cols_df2 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df2 = {}\n",
    "    for col in numeric_cols_df2:\n",
    "        agg_dict_df2[col] = ['mean', 'count']\n",
    "    \n",
    "    # Here we do the groupby\n",
    "    df2_agg = df2_with_subject.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df2).reset_index()\n",
    "    df2_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df2 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df3 by (subject_id, hadm_id)\n",
    "    numeric_cols_df3 = df3_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df3 = [c for c in numeric_cols_df3 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df3 = {}\n",
    "    for col in numeric_cols_df3:\n",
    "        agg_dict_df3[col] = ['mean', 'count']\n",
    "    \n",
    "    df3_agg = df3_clean.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df3).reset_index()\n",
    "    df3_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df3 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Aggregate df4 by (subject_id, hadm_id)\n",
    "    numeric_cols_df4 = df4_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols_df4 = [c for c in numeric_cols_df4 if c not in ['hadm_id', 'subject_id']]\n",
    "    \n",
    "    agg_dict_df4 = {}\n",
    "    for col in numeric_cols_df4:\n",
    "        agg_dict_df4[col] = ['mean', 'count']\n",
    "    \n",
    "    df4_agg = df4_clean.groupby(['subject_id', 'hadm_id']).agg(agg_dict_df4).reset_index()\n",
    "    df4_agg.columns = ['subject_id', 'hadm_id'] + [f'{col}_{stat}' for col in numeric_cols_df4 for stat in ['mean', 'count']]\n",
    "    \n",
    "    # Start with df1 - get unique (subject_id, hadm_id) pairs\n",
    "    df1_base = df1_clean[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged = df1_base.merge(df2_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    merged = merged.merge(df3_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    merged = merged.merge(df4_agg, on=['subject_id', 'hadm_id'], how='outer')\n",
    "    \n",
    "    return merged, df2_agg, df3_agg, df4_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Both Merge Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing both merge strategies...\n",
      "\n",
      "================================================================================\n",
      "OPTION A: Merging on subject_id only\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 (Labs) after adding subject_id: 1,166,572 / 1,166,572 rows have subject_id\n",
      "Base (df1): 4,392 unique subjects\n",
      "df2 aggregated: 4,687 unique subjects\n",
      "df3 aggregated: 2,616 unique subjects\n",
      "df4 aggregated: 3,229 unique subjects\n",
      "\n",
      "Final merged dataset: 4,694 unique subjects\n",
      "Shape: 4,694 rows × 35 columns\n",
      "\n",
      "================================================================================\n",
      "OPTION B: Merging on (subject_id, hadm_id) pair\n",
      "================================================================================\n",
      "df2 (Labs) after adding subject_id: 1,166,572 / 1,166,572 rows have subject_id\n",
      "Base (df1): 4,864 unique (subject_id, hadm_id) pairs\n",
      "Total (hadm_id, subject_id) pairs from df1 and df3: 5,166\n",
      "df2 aggregated: 5,157 unique (subject_id, hadm_id) pairs\n",
      "df3 aggregated: 2,756 unique (subject_id, hadm_id) pairs\n",
      "df4 aggregated: 3,459 unique (subject_id, hadm_id) pairs\n",
      "\n",
      "Final merged dataset: 5,166 unique (subject_id, hadm_id) pairs\n",
      "Shape: 5,166 rows × 12 columns\n"
     ]
    }
   ],
   "source": [
    "# Option B: Merge on (subject_id, hadm_id) pair\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTION B: Merging on (subject_id, hadm_id) pair\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "merged_option_b, df2_agg_b, df3_agg_b, df4_agg_b = merge_option_b_subject_hadm_id(df1_clean, df2_clean, df3_clean, df4_clean)\n",
    "\n",
    "print(f\"Base (df1): {df1_clean[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "\n",
    "# Get total unique pairs from df1 and df3\n",
    "total_pairs = pd.concat([\n",
    "    df1_clean[['subject_id', 'hadm_id']].drop_duplicates(),\n",
    "    df3_clean[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "]).drop_duplicates().shape[0]\n",
    "print(f\"Total (hadm_id, subject_id) pairs from df1 and df3: {total_pairs:,}\")\n",
    "\n",
    "print(f\"df2 aggregated: {df2_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"df3 aggregated: {df3_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"df4 aggregated: {df4_agg_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"\\nFinal merged dataset: {merged_option_b[['subject_id', 'hadm_id']].drop_duplicates().shape[0]:,} unique (subject_id, hadm_id) pairs\")\n",
    "print(f\"Shape: {merged_option_b.shape[0]:,} rows × {merged_option_b.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Sample of Option B (first 5 rows):\n",
      "================================================================================\n",
      "   subject_id   hadm_id  valuenum_mean  valuenum_count  ref_range_lower_mean  \\\n",
      "0    10000980  26913865      50.305427           164.0             31.772393   \n",
      "1    10000980  29654838      50.766780            59.0             28.163158   \n",
      "2    10002013  24760295      53.814800            50.0             30.610000   \n",
      "3    10002155  23822395      43.598140           387.0             33.267801   \n",
      "4    10004457  28723315      62.915600            25.0             31.069565   \n",
      "\n",
      "   ref_range_lower_count  ref_range_upper_mean  ref_range_upper_count  \\\n",
      "0                  163.0             58.486196                  163.0   \n",
      "1                   57.0             60.649649                   57.0   \n",
      "2                   50.0             52.862400                   50.0   \n",
      "3                  382.0             55.895340                  382.0   \n",
      "4                   23.0             71.017391                   23.0   \n",
      "\n",
      "   dilution_value_mean  dilution_value_count  seq_num_mean  seq_num_count  \n",
      "0                  NaN                   0.0           4.0            7.0  \n",
      "1                  NaN                   NaN           NaN            NaN  \n",
      "2                  NaN                   NaN           1.5            2.0  \n",
      "3                  NaN                   0.0           4.5            8.0  \n",
      "4                  NaN                   NaN           NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample of Option B (first 5 rows):\")\n",
    "print(\"=\"*80)\n",
    "print(merged_option_b.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Merged Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved Option A to: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data/1.1.2_merged_dataset_option_a_subject_id.csv\n",
      "✓ Saved Option B to: /Users/alexandermittet/Library/Mobile Documents/com~apple~CloudDocs/uni_life/UniPi DAD/data_analytics_4_health_unipi/Data/1.1.2_merged_dataset_option_b_subject_hadm_id.csv\n"
     ]
    }
   ],
   "source": [
    "# Save merged datasets with task prefix\n",
    "# Convert ID columns to integers before saving\n",
    "if 'subject_id' in merged_option_a.columns:\n",
    "    merged_option_a['subject_id'] = merged_option_a['subject_id'].astype('Int64')  # Nullable integer\n",
    "if 'subject_id' in merged_option_b.columns:\n",
    "    merged_option_b['subject_id'] = merged_option_b['subject_id'].astype('Int64')\n",
    "if 'hadm_id' in merged_option_b.columns:\n",
    "    merged_option_b['hadm_id'] = merged_option_b['hadm_id'].astype('Int64')\n",
    "\n",
    "option_a_file = data_path / '1.1.2_merged_dataset_option_a_subject_id.csv'\n",
    "option_b_file = data_path / '1.1.2_merged_dataset_option_b_subject_hadm_id.csv'\n",
    "\n",
    "merged_option_a.to_csv(option_a_file, index=False)\n",
    "print(f\"✓ Saved Option A to: {option_a_file}\")\n",
    "\n",
    "merged_option_b.to_csv(option_b_file, index=False)\n",
    "print(f\"✓ Saved Option B to: {option_b_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
