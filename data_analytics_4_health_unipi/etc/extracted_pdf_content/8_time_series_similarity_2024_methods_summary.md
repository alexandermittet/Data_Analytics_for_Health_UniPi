# Methods Summary: 8_time_series_similarity_2024.pdf

## Preprocessing

### scaling

```
‚Ä¢ Most common distortions:
‚Ä¢ Offset Translation
‚Ä¢ Amplitude Scaling
‚Ä¢ Linear Trend
‚Ä¢ Noise
```

```
--- Page 13 ---

Transformation II: Amplitude Scaling
0 100 200 300 400 500 600 700 800 900 1000 0 100 200 300 400 500 600 700 800 900 1000
Q = (Q - mean(Q)) / std(Q)
```

```
Removed linear trend,
offset translation,
amplitude scaling

--- Page 15 ---
```

### filtering

```
‚Ä¢ data compression
‚Ä¢ length
‚Ä¢ noise filtering
‚Ä¢ left_height
‚Ä¢ able to support some interesting non-Euclidean similarity measures (right_height can
```


## Approximation

### SAX

```
--- Page 71 ---

Symbolic Aggregate Approximation (SAX)
‚Ä¢ Convert the data into a discrete format, with a small alphabet size.
‚Ä¢ A time series T of length n is divided into w equal-sized segments;
```

```
--- Page 72 ---

Symbolic Aggregate Approximation (SAX)
‚Ä¢ Once the breakpoints are determined,
baabccbc
```

```
‚Ä¢ Dynamic Programming Algorithm Optimization for
Spoken Word Recognition. Hiroaki Sakode et al. 1978.
‚Ä¢ Experiencing SAX: a Novel Symbolic Representation of
Time Series. Jessica Line et al. 2009
‚Ä¢ Compression-based data mining of sequential data.
```

### Symbolic Aggregate Approximation

```
‚Ä¢ Adaptive Piecewise Constant
Approximation Euclidean CDM
‚Ä¢ Symbolic Aggregate Approximation

--- Page 62 ---
```

```
--- Page 71 ---

Symbolic Aggregate Approximation (SAX)
‚Ä¢ Convert the data into a discrete format, with a small alphabet size.
‚Ä¢ A time series T of length n is divided into w equal-sized segments;
```

```
--- Page 72 ---

Symbolic Aggregate Approximation (SAX)
‚Ä¢ Once the breakpoints are determined,
baabccbc
```

### PAA

```
--- Page 69 ---

Piecewise Aggregate Approximation (PAA)
‚Ä¢ Represent the time series as a sequence of box basis functions
with each box of the same size.
```

```
the values in each segment are then approximated and replaced by
a single coefficient, which is their average.
‚Ä¢ Aggregating these w coefficients form the PAA representation of T.
‚Ä¢ Next, we determine the breakpoints that divide the distribution
space into …ë equiprobable regions, where …ë is the alphabet size
```

```
baabccbc
each region is assigned a symbol.
‚Ä¢ The PAA coefficients can then be easily
mapped to the symbols corresponding to
the regions in which they reside.
```

### Piecewise Aggregate Approximation

```
--- Page 69 ---

Piecewise Aggregate Approximation (PAA)
‚Ä¢ Represent the time series as a sequence of box basis functions
with each box of the same size.
```

### DFT

```
--- Page 64 ---

Discrete Fourier Transform (DFT)
‚Ä¢ Apply a spectral decomposition of a signal
‚Ä¢ DTF is a method to decompose functions depending on time into functions
```

```
number of
complete cycles
‚Ä¢ DFT extracts different seasonality patterns from a single time series variable
‚Ä¢ Example: Given an hourly temperature data set, DFT can detect the presence
of day/night variations and summer/winter variations
```

```
complete cycles
‚Ä¢ DFT extracts different seasonality patterns from a single time series variable
‚Ä¢ Example: Given an hourly temperature data set, DFT can detect the presence
of day/night variations and summer/winter variations
‚Ä¢ it will tell you that those two seasonality (frequencies) are present in
```

### Discrete Fourier Transform

```
--- Page 64 ---

Discrete Fourier Transform (DFT)
‚Ä¢ Apply a spectral decomposition of a signal
‚Ä¢ DTF is a method to decompose functions depending on time into functions
```

```
--- Page 65 ---

Discrete Fourier Transform (DFT)
‚Ä¢ A peak value at 10 Hz with a
magnitude of one while all other
```

```
--- Page 66 ---

Discrete Fourier Transform (DFT)
‚Ä¢ Data comprises of 3 different
elementary components with 3
```

### approximation

```
--- Page 54 ---

Fast Approximations to DTW
‚Ä¢ Approximate the time series with some compressed or downsampled
representation, and do DTW on the new representation.
```

```
--- Page 55 ---

Fast Approximations to DTW
‚Ä¢ There is strong visual evidence to suggests it works well
‚Ä¢ In the literature there is good experimental evidence for the utility of
```

```
‚Ä¢ Time series can be compressed using
various transformations:
‚Ä¢ Piecewise Linear Approximation
‚Ä¢ Adaptive Piecewise Constant
Approximation Euclidean CDM
```

### compression

```
--- Page 61 ---

Compression Based Dissimilarity
‚Ä¢ Use as features whatever structure a
given compression algorithm finds.
```

```
Compression Based Dissimilarity
‚Ä¢ Use as features whatever structure a
given compression algorithm finds.
ùê∂(ùë•,ùë¶)
‚Ä¢ ùëë ùë•, ùë¶ = ùê∂ùê∑ùëÄ ùë•, ùë¶ =
```

```
‚Ä¢ Approximation is a special form of Dimensionality
Reduction specifically designed for TSs.
‚Ä¢ Approximation vs Compression:
‚Ä¢ the approximated space is always understandable
‚Ä¢ the compressed space is not necessarily understandable.
```


## Similarity

### DTW

```
--- Page 19 ---

How is DTW Calculated?
Q
‚Ä¢ We create a matrix with size of |Q| by
```

```
--- Page 20 ---

How is DTW Calculated?
‚Ä¢ The DTW distance can ‚Äúfreely‚Äù move
outside the diagonal of the matrix
```

```

How is DTW Calculated?
‚Ä¢ The DTW distance can ‚Äúfreely‚Äù move
outside the diagonal of the matrix
C
```

### Dynamic Time Warping

```
--- Page 17 ---

Dynamic Time Warping
‚Ä¢ Sometimes two time series that are
conceptually equivalent evolve at different
```

```
misalignments in data.
Euclidean.
Dynamic Time Warping.

--- Page 18 ---
```

```
--- Page 49 ---

Dynamic Time Warping ‚Äì A Real Example
‚Ä¢ A Real Example
‚Ä¢ This example shows 2 one-
```

### Euclidean

```
--- Page 10 ---

Euclidean Distance
‚Ä¢ Given two time series:
‚Ä¢ Q = q ‚Ä¶ q C
```

```
--- Page 11 ---

Problems with Euclidean Distance
‚Ä¢ Euclidean distance is very sensitive to ‚Äúdistortions‚Äù in the data.
‚Ä¢ These distortions are dangerous and should be removed.
```

```

Problems with Euclidean Distance
‚Ä¢ Euclidean distance is very sensitive to ‚Äúdistortions‚Äù in the data.
‚Ä¢ These distortions are dangerous and should be removed.
‚Ä¢ Most common distortions:
```

### similarity

```
--- Page 1 ---

Time Series - Similarity, Distances,
Transformations and Clustering

```

```
Problems in Working with Time Series
‚Ä¢ Large amount of data.
‚Ä¢ Similarity is not easy to estimate.
‚Ä¢ Differing data formats.
‚Ä¢ Differing sampling rates.
```

```
--- Page 7 ---

Similarity, Distances and
Transformations

```

### distance

```
--- Page 1 ---

Time Series - Similarity, Distances,
Transformations and Clustering

```

```
--- Page 7 ---

Similarity, Distances and
Transformations

```

```
--- Page 10 ---

Euclidean Distance
‚Ä¢ Given two time series:
‚Ä¢ Q = q ‚Ä¶ q C
```

### correlation

```
‚Ä¢ 1st derivative mean, 1st derivative variance, ‚Ä¶ Min Value 3 2 5
‚Ä¢ parameters of regression, forecasting, Markov model
Autocorrelation 0.2 0.3 0.5
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶

```

### alignment

```
Warped Time Axis. Nonlinear
aligned ‚Äúone to one‚Äù. Greatly suffers
alignments are possible. Can correct
from the misalignment in data.
misalignments in data.
```

```
aligned ‚Äúone to one‚Äù. Greatly suffers
alignments are possible. Can correct
from the misalignment in data.
misalignments in data.
Euclidean.
```

```
alignments are possible. Can correct
from the misalignment in data.
misalignments in data.
Euclidean.
Dynamic Time Warping.
```


## Feature Extraction

### mean

```
0 0
0 50 100 150 200 250 300 0 50 100 150 200 250 300
Q = Q - mean(Q)
C = C - mean(C)
D(Q,C)
```

```
0 50 100 150 200 250 300 0 50 100 150 200 250 300
Q = Q - mean(Q)
C = C - mean(C)
D(Q,C)
0 50 100 150 200 250 300
```

```
Transformation II: Amplitude Scaling
0 100 200 300 400 500 600 700 800 900 1000 0 100 200 300 400 500 600 700 800 900 1000
Q = (Q - mean(Q)) / std(Q)
C = (C - mean(C)) / std(C)
D(Q,C)
```

### variance

```
‚Ä¢ Example of features:
Mean 5.3 6.4 4.8
‚Ä¢ mean, variance, skewness, kurtosis,
‚Ä¢ 1st derivative mean, 1st derivative variance, ‚Ä¶ Min Value 3 2 5
‚Ä¢ parameters of regression, forecasting, Markov model
```

```
Mean 5.3 6.4 4.8
‚Ä¢ mean, variance, skewness, kurtosis,
‚Ä¢ 1st derivative mean, 1st derivative variance, ‚Ä¶ Min Value 3 2 5
‚Ä¢ parameters of regression, forecasting, Markov model
Autocorrelation 0.2 0.3 0.5
```

### std

```
Transformation II: Amplitude Scaling
0 100 200 300 400 500 600 700 800 900 1000 0 100 200 300 400 500 600 700 800 900 1000
Q = (Q - mean(Q)) / std(Q)
C = (C - mean(C)) / std(C)
D(Q,C)
```

```
0 100 200 300 400 500 600 700 800 900 1000 0 100 200 300 400 500 600 700 800 900 1000
Q = (Q - mean(Q)) / std(Q)
C = (C - mean(C)) / std(C)
D(Q,C)

```

### min

```
E Mountain Gorilla
S
https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html

--- Page 19 ---
```

```
ÔÅß(i,j) = cost of best path reaching cell (i,j)
(i-1,j-2) (i-1,j-1) (i-1,j)
= d(q ,c ) + min{ ÔÅß(i-1,j-1), ÔÅß(i-1,j ), ÔÅß(i,j-1) }
i j
(i-2,j-2) (i-2,j-1) (i-2,j)
```

```
--- Page 23 ---

Dynamic Programming Approach
C
Q
```

### max

```
Warping width that achieves
c 85
max Accuracy
a
r FACE 2%
```

```
Feature\Time Series A B C
2. create a feature vector, and
3. use it to measure similarity and/or classify Max Value 11 12 19
‚Ä¢ Example of features:
Mean 5.3 6.4 4.8
```

### trend

```

What We Can Do With Time Series?
‚Ä¢ Trends, Seasonality ‚Ä¢ Motif Discovery
10
ÔÉû
```

```
Time Series Components
‚Ä¢ A given TS consists of three systematic components including level,
trend, seasonality, and one non-systematic component called noise.
‚Ä¢ Level: The average value in the series.
‚Ä¢ Trend: The increasing or decreasing value in the series.
```

```
trend, seasonality, and one non-systematic component called noise.
‚Ä¢ Level: The average value in the series.
‚Ä¢ Trend: The increasing or decreasing value in the series.
‚Ä¢ Seasonality: The repeating short-term cycle in the series.
‚Ä¢ Noise: The random variation in the series.
```

### seasonality

```

What We Can Do With Time Series?
‚Ä¢ Trends, Seasonality ‚Ä¢ Motif Discovery
10
ÔÉû
```

```
Time Series Components
‚Ä¢ A given TS consists of three systematic components including level,
trend, seasonality, and one non-systematic component called noise.
‚Ä¢ Level: The average value in the series.
‚Ä¢ Trend: The increasing or decreasing value in the series.
```

```
‚Ä¢ Level: The average value in the series.
‚Ä¢ Trend: The increasing or decreasing value in the series.
‚Ä¢ Seasonality: The repeating short-term cycle in the series.
‚Ä¢ Noise: The random variation in the series.
‚Ä¢ A systematic component have consistency or recurrence and can be
```


## Key Concepts

- Time Series - Similarity, Distances,
- What is a Time Series? 25.1750
- ‚Ä¢ A time series is a collection of observations
- Time Series are Ubiquitous
- be seen as time series
- Problems in Working with Time Series
- What We Can Do With Time Series?
- Time Series Components
- ‚Ä¢ In time series analysis we recognize two
- ‚Ä¢ Given two time series:
- series, then subtract that line from the time series.
- ‚Ä¢ Sometimes two time series that are
- two similar time series
- time series.
- shifted points in the two time series
- time series, is a path through the matrix.
- Both time series move
- Only one time series moves
- Dynamic Programming Approach
- Dynamic Programming Approach
- Dynamic Programming Approach
- ‚Ä¢ Given the following input time series:
- power demand time series.
- ‚Ä¢ Approximate the time series with some compressed or downsampled
- the approach on clustering, classification, etc.
- ‚Ä¢ For long time series, shape-based similarity give
- 1. extract global features from the time series,
- Feature\Time Series A B C
- ‚Ä¢ Time series can be compressed using
- Time Series Approximation
- Time Series Approximation
- ‚Ä¢ DFT extracts different seasonality patterns from a single time series variable
- ‚Ä¢ Represent the time series as a sequence of straight lines.
- used to represent a particular time series.
- ‚Ä¢ Represent the time series as a sequence of box basis functions
- dividing the time series into M equi-sized ``frames‚Äô‚Äô.
- ‚Ä¢ A time series T of length n is divided into w equal-sized segments;
- Summary of Time Series Similarity
- ‚Ä¢ If you have short time series
- ‚Ä¢ If you have long time series
- Summary of Time Series Representation
- Clustering Time Series
- ‚Ä¢ It is based on the similarity between time series.
- ‚Ä¢ The two general methods of time series clustering are
- Types of Time Series Clustering
- objects. Given a set of individual time series data, the objective is to
- group similar time series into the same cluster.
- ‚Ä¢ Features-based clustering: extract features, or time series motifs (see
- next lectures) as the features and use them to cluster time series.
- ‚Ä¢ Compression-based clustering: compress time series and run

